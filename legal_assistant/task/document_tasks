# Create new file: legal_assistant/tasks/document_tasks.py
"""Background tasks for document processing"""
import asyncio
import logging
import time
from datetime import datetime
from typing import Dict, Any

from ..services.document_processor import SafeDocumentProcessor
from ..services.container_manager import get_container_manager
from ..storage.managers import uploaded_files, document_processing_status

logger = logging.getLogger(__name__)

async def process_document_background(
    file_id: str,
    file_content: bytes,
    file_ext: str,
    filename: str,
    user_id: str
):
    """Process document in background with progress updates and retry logic"""
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # Update status
            document_processing_status[file_id] = {
                'status': 'extracting',
                'progress': 10,
                'message': f'Extracting text from document... (attempt {retry_count + 1})',
                'started_at': datetime.utcnow().isoformat(),
                'retry_count': retry_count
            }
            
            # Process document
            start_time = time.time()
            content, pages_processed, warnings = SafeDocumentProcessor.process_document_from_bytes(
                file_content, filename, file_ext
            )
            
            if not content or len(content.strip()) < 50:
                raise ValueError("Document appears to be empty or could not be processed")
            
            # Update progress
            document_processing_status[file_id] = {
                'status': 'chunking',
                'progress': 40,
                'message': f'Creating searchable chunks from {pages_processed} pages...',
                'pages': pages_processed
            }
            
            # Prepare metadata
            metadata = {
                'source': filename,
                'file_id': file_id,
                'upload_date': datetime.utcnow().isoformat(),
                'user_id': user_id,
                'file_type': file_ext,
                'pages': pages_processed,
                'file_size': len(file_content),
                'content_length': len(content),
                'processing_warnings': warnings
            }
            
            # Add to container with progress tracking
            container_manager = get_container_manager()
            
            # Use async-friendly chunking
            chunks_created = await container_manager.add_document_to_container_async(
                user_id,
                content,
                metadata,
                file_id,
                progress_callback=lambda p: update_progress(file_id, p)
            )
            
            processing_time = time.time() - start_time
            
            # Update final status
            uploaded_files[file_id].update({
                'status': 'completed',
                'pages_processed': pages_processed,
                'chunks_created': chunks_created,
                'processing_time': processing_time,
                'content_length': len(content)
            })
            
            document_processing_status[file_id] = {
                'status': 'completed',
                'progress': 100,
                'message': f'Successfully processed {pages_processed} pages into {chunks_created} searchable chunks',
                'completed_at': datetime.utcnow().isoformat(),
                'processing_time': processing_time,
                'total_retries': retry_count
            }
            
            logger.info(f"Document {file_id} processed successfully: {pages_processed} pages, {chunks_created} chunks in {processing_time:.2f}s (retries: {retry_count})")
            
            # Success - break out of retry loop
            break
            
        except Exception as e:
            retry_count += 1
            logger.warning(f"Error processing document {file_id} (attempt {retry_count}/{max_retries}): {e}")
            
            if retry_count >= max_retries:
                # Permanent failure after all retries
                logger.error(f"Document {file_id} processing failed permanently after {max_retries} attempts: {e}", exc_info=True)
                
                # Update error status
                uploaded_files[file_id]['status'] = 'failed'
                document_processing_status[file_id] = {
                    'status': 'failed',
                    'progress': 0,
                    'message': f'Processing failed after {max_retries} attempts: {str(e)}',
                    'error': str(e),
                    'failed_at': datetime.utcnow().isoformat(),
                    'total_retries': retry_count - 1
                }
                raise  # Re-raise the exception
            else:
                # Wait before retrying with exponential backoff
                wait_time = 2 ** retry_count
                logger.info(f"Retrying document {file_id} processing in {wait_time} seconds...")
                
                # Update status to show retry
                document_processing_status[file_id] = {
                    'status': 'retrying',
                    'progress': 0,
                    'message': f'Attempt {retry_count} failed, retrying in {wait_time} seconds...',
                    'retry_count': retry_count,
                    'next_retry_in': wait_time
                }
                
                await asyncio.sleep(wait_time)

def update_progress(file_id: str, progress: int):
    """Update processing progress"""
    if file_id in document_processing_status:
        current_status = document_processing_status[file_id]
        current_status['progress'] = min(40 + int(progress * 0.5), 90)  # 40-90% for chunking
