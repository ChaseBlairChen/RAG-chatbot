=== legal_assistant/__init__.py ===
"""Legal Assistant API - Multi-User Platform with Enhanced RAG and Comprehensive Analysis"""

__version__ = "10.0.0-SmartRAG-ComprehensiveAnalysis"
__author__ = "Legal Assistant Team"
__description__ = "Multi-User Legal Assistant with Enhanced RAG, Comprehensive Analysis, and External Database Integration"



=== legal_assistant/api/__init__.py ===
"""API package"""



=== legal_assistant/api/routers/__init__.py ===
"""API routers package"""
from . import query, documents, analysis, admin, health

__all__ = ['query', 'documents', 'analysis', 'admin', 'health']



=== legal_assistant/api/routers/admin.py ===
"""Admin endpoints"""
import os
import logging
from datetime import datetime
from fastapi import APIRouter, Form, Depends

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from ...models import User
from ...config import USER_CONTAINERS_PATH
from ...core.security import get_current_user
from ...services.container_manager import get_container_manager
from ...storage.managers import uploaded_files
from ...utils.formatting import format_context_for_llm
from ...utils.text_processing import extract_bill_information, extract_universal_information

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/cleanup-containers")
async def cleanup_orphaned_containers():
    """Clean up orphaned files in containers that are no longer tracked"""
    cleanup_results = {
        "containers_checked": 0,
        "orphaned_documents_found": 0,
        "cleanup_performed": False,
        "errors": []
    }
    
    try:
        if not os.path.exists(USER_CONTAINERS_PATH):
            return cleanup_results
        
        container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                         if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
        
        cleanup_results["containers_checked"] = len(container_dirs)
        tracked_file_ids = set(uploaded_files.keys())
        
        logger.info(f"Checking {len(container_dirs)} containers against {len(tracked_file_ids)} tracked files")
        
        for container_dir in container_dirs:
            try:
                container_path = os.path.join(USER_CONTAINERS_PATH, container_dir)
                
                try:
                    embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
                    db = Chroma(
                        collection_name=f"user_{container_dir}",
                        embedding_function=embedding_function,
                        persist_directory=container_path
                    )
                    
                    logger.info(f"Container {container_dir} loaded successfully")
                    
                except Exception as e:
                    logger.warning(f"Could not load container {container_dir}: {e}")
                    cleanup_results["errors"].append(f"Container {container_dir}: {str(e)}")
                    continue
                    
            except Exception as e:
                logger.error(f"Error processing container {container_dir}: {e}")
                cleanup_results["errors"].append(f"Container {container_dir}: {str(e)}")
        
        return cleanup_results
        
    except Exception as e:
        logger.error(f"Error during container cleanup: {e}")
        cleanup_results["errors"].append(str(e))
        return cleanup_results

@router.post("/sync-document-tracking")
async def sync_document_tracking():
    """Sync the uploaded_files tracking with what's actually in the containers"""
    sync_results = {
        "tracked_files": len(uploaded_files),
        "containers_found": 0,
        "sync_performed": False,
        "recovered_files": 0,
        "errors": []
    }
    
    try:
        if not os.path.exists(USER_CONTAINERS_PATH):
            return sync_results
        
        container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                         if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
        
        sync_results["containers_found"] = len(container_dirs)
        
        logger.info(f"Syncing document tracking: {len(uploaded_files)} tracked files, {len(container_dirs)} containers")
        
        return sync_results
        
    except Exception as e:
        logger.error(f"Error during document tracking sync: {e}")
        sync_results["errors"].append(str(e))
        return sync_results

@router.get("/document-health")
async def check_document_health():
    """Check the health of document tracking and containers"""
    health_info = {
        "timestamp": datetime.utcnow().isoformat(),
        "uploaded_files_count": len(uploaded_files),
        "container_directories": 0,
        "users_with_containers": 0,
        "orphaned_files": [],
        "container_errors": [],
        "recommendations": []
    }
    
    try:
        # Check container directories
        if os.path.exists(USER_CONTAINERS_PATH):
            container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                             if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
            health_info["container_directories"] = len(container_dirs)
            
            # Check which users have containers
            user_ids_with_files = set()
            for file_data in uploaded_files.values():
                if 'user_id' in file_data:
                    user_ids_with_files.add(file_data['user_id'])
            
            health_info["users_with_containers"] = len(user_ids_with_files)
            
            # Check for potential issues
            if len(container_dirs) > len(user_ids_with_files):
                health_info["recommendations"].append("Some containers may be orphaned - consider running cleanup")
            
            if len(uploaded_files) == 0 and len(container_dirs) > 0:
                health_info["recommendations"].append("Containers exist but no files are tracked - may need sync")
        
        # Check for files with missing metadata
        for file_id, file_data in uploaded_files.items():
            if not file_data.get('user_id'):
                health_info["orphaned_files"].append(file_id)
        
        if health_info["orphaned_files"]:
            health_info["recommendations"].append(f"{len(health_info['orphaned_files'])} files have missing user_id")
        
        logger.info(f"Document health check: {health_info['uploaded_files_count']} files, {health_info['container_directories']} containers")
        
        return health_info
        
    except Exception as e:
        logger.error(f"Error during document health check: {e}")
        health_info["container_errors"].append(str(e))
        return health_info

@router.post("/emergency-clear-tracking")
async def emergency_clear_document_tracking():
    """EMERGENCY: Clear all document tracking"""
    try:
        global uploaded_files
        backup_count = len(uploaded_files)
        uploaded_files.clear()
        
        logger.warning(f"EMERGENCY: Cleared tracking for {backup_count} files")
        
        return {
            "status": "completed",
            "cleared_files": backup_count,
            "warning": "All document tracking has been cleared. Users will need to re-upload documents.",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error during emergency clear: {e}")
        return {
            "status": "failed",
            "error": str(e)
        }

@router.get("/debug/test-bill-search")
async def debug_bill_search_get(
    bill_number: str,
    user_id: str
):
    """Debug bill-specific search functionality (GET version for browser testing)"""
    
    try:
        container_manager = get_container_manager()
        # Get user database
        user_db = container_manager.get_user_database_safe(user_id)
        if not user_db:
            return {"error": "No user database found"}
        
        # Get all documents and check metadata
        all_docs = user_db.get()
        found_chunks = []
        
        logger.info(f"Debugging search for bill: {bill_number}")
        logger.info(f"Total documents in database: {len(all_docs.get('ids', []))}")
        
        for i, (doc_id, metadata, content) in enumerate(zip(
            all_docs.get('ids', []), 
            all_docs.get('metadatas', []), 
            all_docs.get('documents', [])
        )):
            if metadata:
                chunk_index = metadata.get('chunk_index', 'unknown')
                contains_bills = metadata.get('contains_bills', '')
                
                if bill_number in contains_bills:
                    found_chunks.append({
                        'chunk_index': chunk_index,
                        'contains_bills': contains_bills,
                        'content_preview': content[:200] + "..." if len(content) > 200 else content
                    })
                    logger.info(f"Found {bill_number} in chunk {chunk_index}")
        
        # Also test direct text search
        direct_search = [content for content in all_docs.get('documents', []) if bill_number in content]
        
        return {
            "bill_number": bill_number,
            "user_id": user_id,
            "total_chunks": len(all_docs.get('ids', [])),
            "chunks_with_bill_metadata": found_chunks,
            "chunks_with_bill_in_text": len(direct_search),
            "text_search_preview": direct_search[0][:300] + "..." if direct_search else "Not found in text",
            "sample_metadata": all_docs.get('metadatas', [])[:2] if all_docs.get('metadatas') else []
        }
        
    except Exception as e:
        logger.error(f"Debug bill search failed: {e}")
        return {"error": str(e)}

@router.post("/debug/test-bill-search")
async def debug_bill_search(
    bill_number: str = Form(...),
    user_id: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Debug bill-specific search functionality"""
    
    try:
        container_manager = get_container_manager()
        # Get user database
        user_db = container_manager.get_user_database_safe(user_id)
        if not user_db:
            return {"error": "No user database found"}
        
        # Get all documents and check metadata
        all_docs = user_db.get()
        found_chunks = []
        
        logger.info(f"Debugging search for bill: {bill_number}")
        logger.info(f"Total documents in database: {len(all_docs.get('ids', []))}")
        
        for i, (doc_id, metadata, content) in enumerate(zip(
            all_docs.get('ids', []), 
            all_docs.get('metadatas', []), 
            all_docs.get('documents', [])
        )):
            if metadata:
                chunk_index = metadata.get('chunk_index', 'unknown')
                contains_bills = metadata.get('contains_bills', '')
                
                if bill_number in contains_bills:
                    found_chunks.append({
                        'chunk_index': chunk_index,
                        'contains_bills': contains_bills,
                        'content_preview': content[:200] + "..." if len(content) > 200 else content
                    })
                    logger.info(f"Found {bill_number} in chunk {chunk_index}")
        
        # Also test direct text search
        direct_search = [content for content in all_docs.get('documents', []) if bill_number in content]
        
        return {
            "bill_number": bill_number,
            "total_chunks": len(all_docs.get('ids', [])),
            "chunks_with_bill_metadata": found_chunks,
            "chunks_with_bill_in_text": len(direct_search),
            "text_search_preview": direct_search[0][:300] + "..." if direct_search else "Not found in text"
        }
        
    except Exception as e:
        logger.error(f"Debug bill search failed: {e}")
        return {"error": str(e)}

@router.post("/debug/test-extraction")
async def debug_test_extraction(
    question: str = Form(...),
    user_id: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Test information extraction for any question"""
    
    try:
        container_manager = get_container_manager()
        # Search user's documents
        user_results = container_manager.enhanced_search_user_container(user_id, question, "", k=5)
        
        if user_results:
            # Get context
            context_text, source_info = format_context_for_llm(user_results, max_length=3000)
            
            # Test extraction
            import re
            bill_match = re.search(r"(HB|SB|SSB|ESSB|SHB|ESHB)\s*(\d+)", question, re.IGNORECASE)
            if bill_match:
                bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
                extracted_info = extract_bill_information(context_text, bill_number)
            else:
                extracted_info = extract_universal_information(context_text, question)
            
            return {
                "question": question,
                "context_preview": context_text[:500] + "...",
                "extracted_info": extracted_info,
                "sources_found": len(user_results)
            }
        else:
            return {
                "question": question,
                "error": "No relevant documents found"
            }
            
    except Exception as e:
        return {"error": str(e)}



=== legal_assistant/api/routers/analysis.py ===
"""Analysis endpoints"""
import logging
from fastapi import APIRouter, Depends, HTTPException

from ...models import User, ComprehensiveAnalysisRequest, StructuredAnalysisResponse, AnalysisType
from ...core.security import get_current_user
from ...services.analysis_service import ComprehensiveAnalysisProcessor

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/comprehensive-analysis", response_model=StructuredAnalysisResponse)
async def comprehensive_document_analysis(
    request: ComprehensiveAnalysisRequest,
    current_user: User = Depends(get_current_user)
):
    """Comprehensive document analysis endpoint"""
    logger.info(f"Comprehensive analysis request: user={request.user_id}, doc={request.document_id}, types={request.analysis_types}")
    
    try:
        if request.user_id != current_user.user_id:
            raise HTTPException(status_code=403, detail="Cannot analyze documents for different user")
        
        processor = ComprehensiveAnalysisProcessor()
        result = processor.process_comprehensive_analysis(request)
        
        logger.info(f"Comprehensive analysis completed: confidence={result.overall_confidence:.2f}, time={result.processing_time:.2f}s")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Comprehensive analysis endpoint failed: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@router.post("/quick-analysis/{document_id}")
async def quick_document_analysis(
    document_id: str,
    analysis_type: AnalysisType = AnalysisType.COMPREHENSIVE,
    current_user: User = Depends(get_current_user)
):
    """Quick analysis endpoint for single documents"""
    try:
        request = ComprehensiveAnalysisRequest(
            document_id=document_id,
            analysis_types=[analysis_type],
            user_id=current_user.user_id,
            response_style="detailed"
        )
        
        processor = ComprehensiveAnalysisProcessor()
        result = processor.process_comprehensive_analysis(request)
        
        return {
            "success": True,
            "analysis": result,
            "message": f"Analysis completed with {result.overall_confidence:.1%} confidence"
        }
        
    except Exception as e:
        logger.error(f"Quick analysis failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "Analysis failed"
        }



=== legal_assistant/api/routers/documents.py ===
# 1. Update api/routers/documents.py for async processing
"""Document management endpoints with async processing"""
import os
import uuid
import logging
import traceback
from datetime import datetime
from fastapi import APIRouter, File, UploadFile, Depends, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse

from ...models import User, DocumentUploadResponse
from ...config import MAX_FILE_SIZE, LEGAL_EXTENSIONS
from ...core.security import get_current_user
from ...services.document_processor import SafeDocumentProcessor
from ...services.container_manager import get_container_manager
from ...storage.managers import uploaded_files, document_processing_status
from ...tasks.document_tasks import process_document_background

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/user/upload", response_model=DocumentUploadResponse)
async def upload_user_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user)
):
    """Enhanced upload endpoint with background processing"""
    start_time = datetime.utcnow()
    
    try:
        # Check file size first
        file.file.seek(0, 2)
        file_size = file.file.tell()
        file.file.seek(0)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400, 
                detail=f"File too large. Maximum size is {MAX_FILE_SIZE//1024//1024}MB"
            )
        
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in LEGAL_EXTENSIONS:
            raise HTTPException(status_code=400, detail=f"Unsupported file type")
        
        # Generate file_id immediately
        file_id = str(uuid.uuid4())
        session_id = str(uuid.uuid4())
        
        # Read file content
        file_content = file.file.read()
        
        # Quick validation - just check if we can extract text
        try:
            content_preview, _, _ = SafeDocumentProcessor.quick_validate(file_content, file_ext)
            if len(content_preview) < 50:
                raise HTTPException(status_code=422, detail="Document appears to be empty")
        except Exception as e:
            logger.error(f"Document validation failed: {e}")
            raise HTTPException(status_code=422, detail=f"Invalid document: {str(e)}")
        
        # Store initial metadata
        uploaded_files[file_id] = {
            'filename': file.filename,
            'user_id': current_user.user_id,
            'container_id': current_user.container_id,
            'uploaded_at': datetime.utcnow(),
            'session_id': session_id,
            'file_size': file_size,
            'status': 'processing',
            'progress': 0
        }
        
        # Initialize processing status
        document_processing_status[file_id] = {
            'status': 'queued',
            'progress': 0,
            'message': 'Document queued for processing',
            'started_at': datetime.utcnow().isoformat()
        }
        
        # Process document in background
        background_tasks.add_task(
            process_document_background,
            file_id=file_id,
            file_content=file_content,
            file_ext=file_ext,
            filename=file.filename,
            user_id=current_user.user_id
        )
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        logger.info(f"Document {file.filename} queued for processing in {processing_time:.2f}s")
        
        return DocumentUploadResponse(
            message=f"Document {file.filename} is being processed",
            file_id=file_id,
            pages_processed=0,  # Will be updated async
            processing_time=processing_time,
            warnings=[],
            session_id=session_id,
            user_id=current_user.user_id,
            container_id=current_user.container_id or "",
            status="processing"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error uploading document: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@router.get("/user/documents/{file_id}/status")
async def get_document_status(
    file_id: str,
    current_user: User = Depends(get_current_user)
):
    """Get document processing status"""
    if file_id not in uploaded_files:
        raise HTTPException(status_code=404, detail="Document not found")
    
    file_data = uploaded_files[file_id]
    if file_data.get('user_id') != current_user.user_id:
        raise HTTPException(status_code=403, detail="Unauthorized")
    
    status = document_processing_status.get(file_id, {
        'status': 'unknown',
        'progress': 0,
        'message': 'Status unavailable'
    })
    
    return {
        'file_id': file_id,
        'filename': file_data['filename'],
        'status': status['status'],
        'progress': status['progress'],
        'message': status.get('message', ''),
        'pages_processed': file_data.get('pages_processed', 0),
        'chunks_created': file_data.get('chunks_created', 0),
        'processing_time': file_data.get('processing_time', 0),
        'errors': status.get('errors', [])
    }



=== legal_assistant/api/routers/external.py ===
"""External database endpoints"""
import logging
from typing import List
from fastapi import APIRouter, Form, Depends, HTTPException

from ...models import User
from ...core.security import get_current_user
from ...services.external_db_service import search_external_databases

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/external/search")
async def search_external_databases_endpoint(
    query: str = Form(...),
    databases: List[str] = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Search external legal databases (requires premium subscription)"""
    if current_user.subscription_tier not in ["premium", "enterprise"]:
        raise HTTPException(
            status_code=403, 
            detail="External database access requires premium subscription"
        )
    
    results = search_external_databases(query, databases, current_user)
    
    return {
        "query": query,
        "databases_searched": databases,
        "results": results,
        "total_results": len(results)
    }



=== legal_assistant/api/routers/health.py ===
"""Health check endpoints"""
import os
from datetime import datetime
from fastapi import APIRouter
from fastapi.responses import HTMLResponse

from ...config import (
    DEFAULT_CHROMA_PATH, USER_CONTAINERS_PATH, OPENROUTER_API_KEY, FeatureFlags
)
from ...models import ConversationHistory
from ...storage.managers import conversations
from ...core.dependencies import get_nlp, get_sentence_model, get_embeddings, get_sentence_model_name

router = APIRouter()

@router.get("/health")
def health_check():
    """Enhanced system health check with comprehensive analysis capabilities"""
    db_exists = os.path.exists(DEFAULT_CHROMA_PATH)
    
    nlp = get_nlp()
    sentence_model = get_sentence_model()
    embeddings = get_embeddings()
    sentence_model_name = get_sentence_model_name()
    
    return {
        "status": "healthy",
        "version": "10.0.0-SmartRAG-ComprehensiveAnalysis",
        "timestamp": datetime.utcnow().isoformat(),
        "ai_enabled": FeatureFlags.AI_ENABLED,
        "openrouter_api_configured": bool(OPENROUTER_API_KEY),
        "components": {
            "default_database": {
                "exists": db_exists,
                "path": DEFAULT_CHROMA_PATH
            },
            "user_containers": {
                "enabled": True,
                "base_path": USER_CONTAINERS_PATH,
                "active_containers": len(os.listdir(USER_CONTAINERS_PATH)) if os.path.exists(USER_CONTAINERS_PATH) else 0,
                "document_specific_retrieval": True,
                "file_id_tracking": True
            },
            "external_databases": {
                "lexisnexis": {
                    "configured": bool(os.environ.get("LEXISNEXIS_API_KEY")),
                    "status": "ready" if bool(os.environ.get("LEXISNEXIS_API_KEY")) else "not_configured"
                },
                "westlaw": {
                    "configured": bool(os.environ.get("WESTLAW_API_KEY")),
                    "status": "ready" if bool(os.environ.get("WESTLAW_API_KEY")) else "not_configured"
                }
            },
            "comprehensive_analysis": {
                "enabled": True,
                "analysis_types": [
                    "comprehensive",
                    "document_summary", 
                    "key_clauses",
                    "risk_assessment",
                    "timeline_deadlines", 
                    "party_obligations",
                    "missing_clauses"
                ],
                "structured_output": True,
                "document_specific": True,
                "confidence_scoring": True,
                "single_api_call": True
            },
            "enhanced_rag": {
                "enabled": True,
                "features": [
                    "multi_query_strategies",
                    "query_expansion",
                    "entity_extraction",
                    "sub_query_decomposition",
                    "confidence_scoring",
                    "duplicate_removal",
                    "document_specific_filtering"
                ],
                "nlp_model": nlp is not None,
                "sentence_model": sentence_model is not None,
                "sentence_model_name": sentence_model_name if sentence_model else "none",
                "embedding_model": getattr(embeddings, 'model_name', 'unknown') if embeddings else "none"
            },
            "document_processing": {
                "pdf_support": FeatureFlags.PYMUPDF_AVAILABLE or FeatureFlags.PDFPLUMBER_AVAILABLE,
                "pymupdf_available": FeatureFlags.PYMUPDF_AVAILABLE,
                "pdfplumber_available": FeatureFlags.PDFPLUMBER_AVAILABLE,
                "unstructured_available": FeatureFlags.UNSTRUCTURED_AVAILABLE,
                "docx_support": True,
                "txt_support": True,
                "safe_document_processor": True,
                "enhanced_page_estimation": True,
                "bert_semantic_chunking": sentence_model is not None,
                "advanced_legal_chunking": True,
                "embedding_model": sentence_model_name if sentence_model else "none"
            }
        },
        "new_endpoints": [
            "POST /comprehensive-analysis - Full structured analysis",
            "POST /quick-analysis/{document_id} - Quick single document analysis", 
            "Enhanced /ask - Detects comprehensive analysis requests",
            "Enhanced /user/upload - Stores file_id for targeting",
            "GET /admin/document-health - Check system health",
            "POST /admin/cleanup-containers - Clean orphaned containers",
            "POST /admin/emergency-clear-tracking - Reset document tracking"
        ],
        "features": [
            "‚úÖ User-specific document containers",
            "‚úÖ Enhanced RAG with multi-query strategies",
            "‚úÖ Combined search across all sources",
            "‚úÖ External legal database integration (ready)",
            "‚úÖ Subscription tier management",
            "‚úÖ Document access control",
            "‚úÖ Source attribution (default/user/external)",
            "‚úÖ Dynamic confidence scoring",
            "‚úÖ Query expansion and decomposition",
            "‚úÖ SafeDocumentProcessor for file handling",
            "üîß Optional authentication for debugging",
            "üÜï Comprehensive multi-analysis in single API call",
            "üÜï Document-specific analysis targeting",
            "üÜï Structured analysis responses with sections",
            "üÜï Enhanced confidence scoring per section",
            "üÜï File ID tracking for precise document retrieval",
            "üÜï Automatic comprehensive analysis detection",
            "üÜï Container cleanup and health monitoring",
            "üÜï Enhanced error handling and recovery",
            "üÜï Fixed page estimation with content analysis",
            "üÜï Unstructured.io integration for advanced processing",
            "üÜï BERT-based semantic chunking for better retrieval",
            "üÜï Enhanced information extraction (bills, sponsors, etc.)",
            "üÜï Legal-specific BERT models (InCaseLawBERT, legal-bert-base-uncased)",
            "üÜï Advanced semantic similarity for intelligent chunking",
            "üÜï Legal document pattern recognition for better segmentation"
        ],
        # Frontend compatibility fields
        "unified_mode": True,
        "enhanced_rag": True,
        "database_exists": db_exists,
        "database_path": DEFAULT_CHROMA_PATH,
        "api_key_configured": bool(OPENROUTER_API_KEY),
        "active_conversations": len(conversations)
    }

@router.get("/conversation/{session_id}", response_model=ConversationHistory)
async def get_conversation(session_id: str):
    """Get the conversation history for a session"""
    from fastapi import HTTPException
    
    if session_id not in conversations:
        raise HTTPException(status_code=404, detail="Session not found")
    
    return ConversationHistory(
        session_id=session_id,
        messages=conversations[session_id]['messages']
    )

@router.get("/subscription/status")
async def get_subscription_status():
    """Get user's subscription status and available features"""
    from ...core.security import get_current_user
    from fastapi import Depends
    
    current_user = await get_current_user()
    
    features = {
        "free": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": 10,
            "external_databases": [],
            "ai_analysis": True,
            "api_calls_per_month": 100,
            "enhanced_rag": True,
            "comprehensive_analysis": True
        },
        "premium": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": 100,
            "external_databases": ["lexisnexis", "westlaw"],
            "ai_analysis": True,
            "api_calls_per_month": 1000,
            "priority_support": True,
            "enhanced_rag": True,
            "comprehensive_analysis": True,
            "document_specific_analysis": True
        },
        "enterprise": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": "unlimited",
            "external_databases": ["lexisnexis", "westlaw", "bloomberg_law"],
            "ai_analysis": True,
            "api_calls_per_month": "unlimited",
            "priority_support": True,
            "custom_integrations": True,
            "enhanced_rag": True,
            "comprehensive_analysis": True,
            "document_specific_analysis": True,
            "bulk_analysis": True
        }
    }
    
    return {
        "user_id": current_user.user_id,
        "subscription_tier": current_user.subscription_tier,
        "features": features.get(current_user.subscription_tier, features["free"]),
        "external_db_access": current_user.external_db_access
    }

@router.get("/", response_class=HTMLResponse)
def get_interface():
    """Web interface with updated documentation for comprehensive analysis"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Legal Assistant - Complete Multi-Analysis Edition [MODULAR]</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background: #f8f9fa; }
            .container { max-width: 1200px; margin: 0 auto; }
            h1 { color: #2c3e50; }
            .feature-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 30px 0; }
            .feature-card { background: #fff; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px; }
            .endpoint { background: #f1f3f4; padding: 10px; margin: 10px 0; border-radius: 5px; font-family: monospace; }
            .status { padding: 5px 10px; border-radius: 15px; font-size: 12px; }
            .status-active { background: #d4edda; color: #155724; }
            .status-ready { background: #cce5ff; color: #004085; }
            .status-modular { background: #28a745; color: white; }
            .badge-modular { background: #17a2b8; color: white; padding: 2px 8px; border-radius: 10px; font-size: 11px; margin-left: 5px; }
            .code-example { background: #f8f9fa; border: 1px solid #e9ecef; border-radius: 5px; padding: 15px; margin: 10px 0; font-family: monospace; font-size: 12px; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>‚öñÔ∏è Legal Assistant API v10.0 <span class="badge-modular">MODULAR ARCHITECTURE</span></h1>
            <p>Complete Multi-User Platform with Enhanced RAG, Comprehensive Analysis, and Container Management</p>
            <div class="status status-modular">üéØ Clean Modular Structure - Easy to Maintain and Extend!</div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h3>üìÅ Modular Architecture</h3>
                    <p>Clean separation of concerns</p>
                    <ul>
                        <li>‚úÖ Services for business logic</li>
                        <li>‚úÖ API routers for endpoints</li>
                        <li>‚úÖ Models for data structures</li>
                        <li>‚úÖ Utils for helper functions</li>
                    </ul>
                </div>
                
                <div class="feature-card">
                    <h3>üöÄ Comprehensive Analysis</h3>
                    <p>All analysis types in a single efficient API call</p>
                    <ul>
                        <li>‚úÖ Document summary</li>
                        <li>‚úÖ Key clauses extraction</li>
                        <li>‚úÖ Risk assessment</li>
                        <li>‚úÖ Timeline & deadlines</li>
                        <li>‚úÖ Party obligations</li>
                        <li>‚úÖ Missing clauses detection</li>
                    </ul>
                </div>
                
                <div class="feature-card">
                    <h3>üõ†Ô∏è Enhanced Error Handling</h3>
                    <p>Robust container management with auto-recovery</p>
                    <ul>
                        <li>‚úÖ Timeout protection</li>
                        <li>‚úÖ Container auto-recovery</li>
                        <li>‚úÖ Graceful degradation</li>
                        <li>‚úÖ Health monitoring</li>
                    </ul>
                </div>
            </div>
            
            <h2>üì° API Reference</h2>
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>Core Endpoints</h4>
                    <div class="endpoint">POST /api/ask - Enhanced chat</div>
                    <div class="endpoint">POST /api/user/upload - Document upload</div>
                    <div class="endpoint">GET /api/user/documents - List documents</div>
                </div>
                
                <div class="feature-card">
                    <h4>Analysis Endpoints</h4>
                    <div class="endpoint">POST /api/comprehensive-analysis</div>
                    <div class="endpoint">POST /api/quick-analysis/{id}</div>
                </div>
                
                <div class="feature-card">
                    <h4>Admin Endpoints</h4>
                    <div class="endpoint">GET /api/admin/document-health</div>
                    <div class="endpoint">POST /api/admin/cleanup-containers</div>
                </div>
            </div>
            
            <p style="text-align: center; color: #7f8c8d; margin-top: 30px;">
                üéâ Modular Legal Assistant Backend - Clean Architecture üéâ
                <br>Version 10.0.0-SmartRAG-ComprehensiveAnalysis
                <br>Fully modularized for easy maintenance and extensibility!
            </p>
        </div>
    </body>
    </html>
    """



=== legal_assistant/api/routers/query.py ===
"""Query endpoints"""
import uuid
import logging
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException

from ...models import Query, QueryResponse, User
from ...core.security import get_current_user
from ...storage.managers import conversations, cleanup_expired_conversations
from ...processors.query_processor import process_query

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/ask", response_model=QueryResponse)
async def ask_question(query: Query, current_user: User = Depends(get_current_user)):
    """Enhanced ask endpoint with comprehensive analysis detection"""
    logger.info(f"Received ask request: {query}")
    
    cleanup_expired_conversations()
    
    session_id = query.session_id or str(uuid.uuid4())
    user_id = query.user_id or current_user.user_id
    
    if session_id not in conversations:
        conversations[session_id] = {
            "messages": [],
            "created_at": datetime.utcnow(),
            "last_accessed": datetime.utcnow()
        }
    else:
        conversations[session_id]["last_accessed"] = datetime.utcnow()
    
    user_question = query.question.strip()
    if not user_question:
        return QueryResponse(
            response=None,
            error="Question cannot be empty.",
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[]
        )
    
    response = process_query(
        user_question, 
        session_id, 
        user_id,
        query.search_scope or "all",
        query.response_style or "balanced",
        query.use_enhanced_rag if query.use_enhanced_rag is not None else True,
        query.document_id
    )
    return response

@router.post("/ask-debug", response_model=QueryResponse)
async def ask_question_debug(query: Query):
    """Debug version of ask endpoint without authentication"""
    logger.info(f"Debug ask request received: {query}")
    
    cleanup_expired_conversations()
    
    session_id = query.session_id or str(uuid.uuid4())
    user_id = query.user_id or "debug_user"
    
    if session_id not in conversations:
        conversations[session_id] = {
            "messages": [],
            "created_at": datetime.utcnow(),
            "last_accessed": datetime.utcnow()
        }
    else:
        conversations[session_id]["last_accessed"] = datetime.utcnow()
    
    user_question = query.question.strip()
    if not user_question:
        return QueryResponse(
            response=None,
            error="Question cannot be empty.",
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[]
        )
    
    response = process_query(
        user_question, 
        session_id, 
        user_id,
        query.search_scope or "all",
        query.response_style or "balanced",
        query.use_enhanced_rag if query.use_enhanced_rag is not None else True,
        query.document_id
    )
    return response



=== legal_assistant/config.py ===
"""Configuration and environment variables"""
import os
from typing import List

# API Configuration
OPENROUTER_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_API_BASE = os.environ.get("OPENAI_API_BASE", "https://openrouter.ai/api/v1")

# Database Paths
DEFAULT_CHROMA_PATH = os.path.abspath(os.path.join(os.getcwd(), "chromadb-database"))
USER_CONTAINERS_PATH = os.path.abspath(os.path.join(os.getcwd(), "user-containers"))

# File Processing
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
LEGAL_EXTENSIONS = {'.pdf', '.txt', '.docx', '.rtf'}

# External Database Configuration
LEXISNEXIS_API_KEY = os.environ.get("LEXISNEXIS_API_KEY")
LEXISNEXIS_API_ENDPOINT = os.environ.get("LEXISNEXIS_API_ENDPOINT")
WESTLAW_API_KEY = os.environ.get("WESTLAW_API_KEY")
WESTLAW_API_ENDPOINT = os.environ.get("WESTLAW_API_ENDPOINT")

# Model Names
EMBEDDING_MODELS = [
    "nlpaueb/legal-bert-base-uncased",
    "law-ai/InCaseLawBERT", 
    "sentence-transformers/all-mpnet-base-v2",
    "sentence-transformers/all-roberta-large-v1",
    "microsoft/DialoGPT-medium",
    "sentence-transformers/all-MiniLM-L12-v2",
    "all-MiniLM-L6-v2"
]

FAST_EMBEDDING_MODELS = [
    "all-MiniLM-L6-v2",
    "all-MiniLM-L12-v2",
]

# AI Models
AI_MODELS = [
    "moonshotai/kimi-k2:free",
    "deepseek/deepseek-chat",
    "microsoft/phi-3-mini-128k-instruct:free",
    "meta-llama/llama-3.2-3b-instruct:free",
    "google/gemma-2-9b-it:free",
    "mistralai/mistral-7b-instruct:free",
    "openchat/openchat-7b:free"
]

# Chunk Sizes - OPTIMIZED
DEFAULT_CHUNK_SIZE = 1000     # Reduced from 1500
LEGISLATIVE_CHUNK_SIZE = 1500 # Reduced from 2000
DEFAULT_CHUNK_OVERLAP = 200   # Reduced from 300
LEGISLATIVE_CHUNK_OVERLAP = 300  # Reduced from 500

# Search Settings - IMPROVED
DEFAULT_SEARCH_K = 15  # Increased from 10
ENHANCED_SEARCH_K = 20  # Increased from 12
COMPREHENSIVE_SEARCH_K = 30  # Increased from 20
MIN_RELEVANCE_SCORE = 0.3  # Increased from 0.15 - much stricter!

# Confidence Score Weights - REBALANCED
CONFIDENCE_WEIGHTS = {
    "relevance": 0.5,     # Increased weight on relevance
    "document_count": 0.2, # Decreased weight on count
    "consistency": 0.2,
    "completeness": 0.1
}

# Add new search configuration
SEARCH_CONFIG = {
    "rerank_enabled": True,
    "hybrid_search_enabled": True,
    "keyword_weight": 0.3,
    "semantic_weight": 0.7,
    "max_results_to_rerank": 50,
    "query_expansion_enabled": True,
    "min_score_threshold": 0.3,
    "boost_exact_matches": True,
    "boost_factor": 1.5
}

# Feature Flags
class FeatureFlags:
    AI_ENABLED: bool = bool(OPENROUTER_API_KEY)
    AIOHTTP_AVAILABLE: bool = False
    OPEN_SOURCE_NLP_AVAILABLE: bool = False
    PYMUPDF_AVAILABLE: bool = False
    PDFPLUMBER_AVAILABLE: bool = False
    UNSTRUCTURED_AVAILABLE: bool = False
    OCR_AVAILABLE: bool = False
    HYBRID_SEARCH_AVAILABLE: bool = False

def initialize_feature_flags():
    """Initialize feature flags by checking for available dependencies"""
    
    # Check OCR
    try:
        import pytesseract
        from pdf2image import convert_from_bytes
        FeatureFlags.OCR_AVAILABLE = True
        print("‚úÖ OCR support available - can process scanned PDFs")
    except ImportError:
        FeatureFlags.OCR_AVAILABLE = False
        print("‚ö†Ô∏è OCR not available - install pytesseract and pdf2image")

    # Check hybrid search
    try:
        import rank_bm25
        FeatureFlags.HYBRID_SEARCH_AVAILABLE = True
        print("‚úÖ Hybrid search available - better retrieval accuracy")
    except ImportError:
        FeatureFlags.HYBRID_SEARCH_AVAILABLE = False
        print("‚ö†Ô∏è Hybrid search not available - install rank-bm25")
    
    # Check existing features
    try:
        import aiohttp
        FeatureFlags.AIOHTTP_AVAILABLE = True
        print("‚úÖ Async HTTP support available")
    except ImportError:
        FeatureFlags.AIOHTTP_AVAILABLE = False
        print("‚ö†Ô∏è Async HTTP not available - install aiohttp")
    
    try:
        import spacy
        import nltk
        FeatureFlags.OPEN_SOURCE_NLP_AVAILABLE = True
        print("‚úÖ Open source NLP available")
    except ImportError:
        FeatureFlags.OPEN_SOURCE_NLP_AVAILABLE = False
        print("‚ö†Ô∏è Open source NLP not available - install spacy and nltk")
    
    try:
        import fitz  # PyMuPDF
        FeatureFlags.PYMUPDF_AVAILABLE = True
        print("‚úÖ PyMuPDF available for PDF processing")
    except ImportError:
        FeatureFlags.PYMUPDF_AVAILABLE = False
        print("‚ö†Ô∏è PyMuPDF not available - install PyMuPDF")
    
    try:
        import pdfplumber
        FeatureFlags.PDFPLUMBER_AVAILABLE = True
        print("‚úÖ PDFPlumber available for advanced PDF parsing")
    except ImportError:
        FeatureFlags.PDFPLUMBER_AVAILABLE = False
        print("‚ö†Ô∏è PDFPlumber not available - install pdfplumber")
    
    try:
        import unstructured
        FeatureFlags.UNSTRUCTURED_AVAILABLE = True
        print("‚úÖ Unstructured available for document parsing")
    except ImportError:
        FeatureFlags.UNSTRUCTURED_AVAILABLE = False
        print("‚ö†Ô∏è Unstructured not available - install unstructured")

# Initialize feature flags when module is imported
initialize_feature_flags()



=== legal_assistant/core/__init__.py ===
"""Core functionality package"""
from .dependencies import (
    initialize_nlp_models,
    initialize_feature_flags,
    get_nlp,
    get_sentence_model,
    get_embeddings
)
from .security import get_current_user, security
from .exceptions import (
    LegalAssistantException,
    DocumentProcessingError,
    ContainerError,
    RetrievalError,
    AnalysisError,
    AuthenticationError
)

__all__ = [
    'initialize_nlp_models',
    'initialize_feature_flags',
    'get_nlp',
    'get_sentence_model',
    'get_embeddings',
    'get_current_user',
    'security',
    'LegalAssistantException',
    'DocumentProcessingError',
    'ContainerError',
    'RetrievalError',
    'AnalysisError',
    'AuthenticationError'
]



=== legal_assistant/core/dependencies.py ===
"""Dependency injection and initialization"""
import logging
from typing import Optional
import spacy
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from ..config import EMBEDDING_MODELS, FAST_EMBEDDING_MODELS, FeatureFlags

logger = logging.getLogger(__name__)

# Global instances
nlp: Optional[spacy.Language] = None
sentence_model: Optional[SentenceTransformer] = None
embeddings: Optional[HuggingFaceEmbeddings] = None
sentence_model_name: Optional[str] = None

def initialize_nlp_models():
    """Initialize NLP models"""
    global nlp, sentence_model, embeddings, sentence_model_name
    
    # Load spaCy
    try:
        nlp = spacy.load("en_core_web_sm")
        logger.info("‚úÖ spaCy model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}")
        nlp = None
    
    # Load sentence transformer
    for model_name in EMBEDDING_MODELS:
        try:
            sentence_model = SentenceTransformer(model_name)
            sentence_model_name = model_name
            logger.info(f"‚úÖ Loaded powerful sentence model: {model_name}")
            break
        except Exception as e:
            logger.warning(f"Failed to load {model_name}: {e}")
            continue
    
    if sentence_model is None:
        logger.error("‚ùå Failed to load any sentence transformer model")
        sentence_model_name = "none"
    
    # Load embeddings
    try:
        if sentence_model_name and sentence_model_name != "none":
            embeddings = HuggingFaceEmbeddings(model_name=sentence_model_name)
            logger.info(f"‚úÖ Loaded embeddings with: {sentence_model_name}")
        else:
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            logger.info("‚ö†Ô∏è Using fallback embeddings: all-MiniLM-L6-v2")
    except Exception as e:
        logger.error(f"Failed to load embeddings: {e}")
        embeddings = None

def initialize_feature_flags():
    """Initialize feature availability flags"""
    # Check aiohttp
    try:
        import aiohttp
        FeatureFlags.AIOHTTP_AVAILABLE = True
    except ImportError:
        FeatureFlags.AIOHTTP_AVAILABLE = False
        print("‚ö†Ô∏è aiohttp not available - AI features disabled. Install with: pip install aiohttp")
    
    # Check open source NLP
    try:
        import torch
        from transformers import pipeline
        FeatureFlags.OPEN_SOURCE_NLP_AVAILABLE = True
        logger.info("‚úÖ Open-source NLP models available")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Open-source NLP models not available: {e}")
        print("Install with: pip install transformers torch")
    
    # Check PDF processors
    try:
        import fitz
        FeatureFlags.PYMUPDF_AVAILABLE = True
        print("‚úÖ PyMuPDF available - using enhanced PDF extraction")
    except ImportError as e:
        print(f"‚ö†Ô∏è PyMuPDF not available: {e}")
        print("Install with: pip install PyMuPDF")
    
    try:
        import pdfplumber
        FeatureFlags.PDFPLUMBER_AVAILABLE = True
        print("‚úÖ pdfplumber available - using enhanced PDF extraction")
    except ImportError as e:
        print(f"‚ö†Ô∏è pdfplumber not available: {e}")
        print("Install with: pip install pdfplumber")
    
    try:
        from unstructured.partition.auto import partition
        FeatureFlags.UNSTRUCTURED_AVAILABLE = True
        print("‚úÖ Unstructured.io available - using advanced document processing")
    except ImportError as e:
        print(f"‚ö†Ô∏è Unstructured.io not available: {e}")
        print("Install with: pip install unstructured[all-docs]")
    
    # Update AI enabled flag
    FeatureFlags.AI_ENABLED = bool(FeatureFlags.AIOHTTP_AVAILABLE and FeatureFlags.AI_ENABLED)
    
    print(f"Document processing status: PyMuPDF={FeatureFlags.PYMUPDF_AVAILABLE}, pdfplumber={FeatureFlags.PDFPLUMBER_AVAILABLE}, Unstructured={FeatureFlags.UNSTRUCTURED_AVAILABLE}")

def get_nlp():
    """Get NLP instance"""
    return nlp

def get_sentence_model():
    """Get sentence model instance"""
    return sentence_model

def get_embeddings():
    """Get embeddings instance"""
    return embeddings

def get_sentence_model_name():
    """Get sentence model name"""
    return sentence_model_name



=== legal_assistant/core/exceptions.py ===
"""Custom exceptions for the application"""

class LegalAssistantException(Exception):
    """Base exception for all custom exceptions"""
    pass

class DocumentProcessingError(LegalAssistantException):
    """Raised when document processing fails"""
    pass

class ContainerError(LegalAssistantException):
    """Raised when container operations fail"""
    pass

class RetrievalError(LegalAssistantException):
    """Raised when document retrieval fails"""
    pass

class AnalysisError(LegalAssistantException):
    """Raised when analysis operations fail"""
    pass

class AuthenticationError(LegalAssistantException):
    """Raised when authentication fails"""
    pass



=== legal_assistant/core/security.py ===
"""Authentication and authorization"""
from typing import Optional
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from ..models import User
from ..storage.managers import user_sessions

security = HTTPBearer(auto_error=False)

def get_current_user(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> User:
    """Get current user from credentials"""
    if credentials is None:
        default_user_id = "debug_user"
        if default_user_id not in user_sessions:
            from ..services.container_manager import get_container_manager
            container_manager = get_container_manager()
            user_sessions[default_user_id] = User(
                user_id=default_user_id,
                container_id=container_manager.get_container_id(default_user_id),
                subscription_tier="free"
            )
        return user_sessions[default_user_id]
    
    token = credentials.credentials
    user_id = f"user_{token[:8]}"
    
    if user_id not in user_sessions:
        from ..services.container_manager import get_container_manager
        container_manager = get_container_manager()
        user_sessions[user_id] = User(
            user_id=user_id,
            container_id=container_manager.get_container_id(user_id),
            subscription_tier="free"
        )
    
    return user_sessions[user_id]



=== legal_assistant/main.py ===
"""Main FastAPI application entry point"""
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from .config import DEFAULT_CHROMA_PATH, USER_CONTAINERS_PATH, FeatureFlags
from .core import initialize_nlp_models, initialize_feature_flags
from .services import initialize_container_manager
from .api.routers import query, documents, analysis, admin, health

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Create directories
os.makedirs(USER_CONTAINERS_PATH, exist_ok=True)

# Initialize everything
logger.info(f"Using DEFAULT_CHROMA_PATH: {DEFAULT_CHROMA_PATH}")
logger.info(f"Using USER_CONTAINERS_PATH: {USER_CONTAINERS_PATH}")

initialize_feature_flags()
initialize_nlp_models()
initialize_container_manager()

# Create FastAPI app
app = FastAPI(
    title="Unified Legal Assistant API",
    description="Multi-User Legal Assistant with Enhanced RAG, Comprehensive Analysis, and External Database Integration",
    version="10.0.0-SmartRAG-ComprehensiveAnalysis"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(query.router, tags=["queries"])
app.include_router(documents.router, tags=["documents"])
app.include_router(analysis.router, tags=["analysis"])
app.include_router(admin.router, prefix="/admin", tags=["admin"])
app.include_router(health.router, tags=["health"])

# Mount the home page from health router
app.mount("/", health.router)

def create_app():
    """Application factory"""
    return app

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"üöÄ Starting Modular Legal Assistant on port {port}")
    logger.info(f"ChromaDB Path: {DEFAULT_CHROMA_PATH}")
    logger.info(f"User Containers Path: {USER_CONTAINERS_PATH}")
    logger.info(f"AI Status: {'ENABLED' if FeatureFlags.AI_ENABLED else 'DISABLED - Set OPENAI_API_KEY to enable'}")
    logger.info(f"PDF processing: PyMuPDF={FeatureFlags.PYMUPDF_AVAILABLE}, pdfplumber={FeatureFlags.PDFPLUMBER_AVAILABLE}")
    logger.info("Features: Comprehensive analysis, document-specific targeting, container cleanup, enhanced error handling")
    logger.info("Version: 10.0.0-SmartRAG-ComprehensiveAnalysis")
    logger.info("üìÅ MODULAR ARCHITECTURE - Clean separation of concerns!")
    uvicorn.run("legal_assistant.main:app", host="0.0.0.0", port=port, reload=True)



=== legal_assistant/models/__init__.py ===
"""Models package"""
from .api_models import (
    User, Query, QueryResponse, ComprehensiveAnalysisRequest,
    StructuredAnalysisResponse, UserDocumentUpload, DocumentUploadResponse,
    ConversationHistory
)
from .enums import AnalysisType

__all__ = [
    'User', 'Query', 'QueryResponse', 'ComprehensiveAnalysisRequest',
    'StructuredAnalysisResponse', 'UserDocumentUpload', 'DocumentUploadResponse',
    'ConversationHistory', 'AnalysisType'
]



=== legal_assistant/models/api_models.py ===
"""Pydantic models for API requests and responses"""
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from datetime import datetime
from .enums import AnalysisType

class User(BaseModel):
    user_id: str
    email: Optional[str] = None
    container_id: Optional[str] = None
    subscription_tier: str = "free"
    external_db_access: List[str] = []

class Query(BaseModel):
    question: str
    session_id: Optional[str] = None
    response_style: Optional[str] = "balanced"
    user_id: Optional[str] = None
    search_scope: Optional[str] = "all"
    external_databases: Optional[List[str]] = []
    use_enhanced_rag: Optional[bool] = True
    document_id: Optional[str] = None

class QueryResponse(BaseModel):
    response: Optional[str] = None
    error: Optional[str] = None
    context_found: bool = False
    sources: Optional[list] = None
    session_id: str
    confidence_score: float = 0.0
    expand_available: bool = False
    sources_searched: List[str] = []
    retrieval_method: Optional[str] = None

class ComprehensiveAnalysisRequest(BaseModel):
    document_id: Optional[str] = None
    analysis_types: List[AnalysisType] = [AnalysisType.COMPREHENSIVE]
    user_id: str
    session_id: Optional[str] = None
    response_style: str = "detailed"

class StructuredAnalysisResponse(BaseModel):
    document_summary: Optional[str] = None
    key_clauses: Optional[str] = None
    risk_assessment: Optional[str] = None
    timeline_deadlines: Optional[str] = None
    party_obligations: Optional[str] = None
    missing_clauses: Optional[str] = None
    confidence_scores: Dict[str, float] = {}
    sources_by_section: Dict[str, List[Dict]] = {}
    overall_confidence: float = 0.0
    processing_time: float = 0.0
    warnings: List[str] = []
    retrieval_method: str = "comprehensive_analysis"

class UserDocumentUpload(BaseModel):
    user_id: str
    file_id: str
    filename: str
    upload_timestamp: str
    pages_processed: int
    metadata: Dict[str, Any]

class DocumentUploadResponse(BaseModel):
    message: str
    file_id: str
    pages_processed: int
    processing_time: float
    warnings: List[str]
    session_id: str
    user_id: str
    container_id: str
    status: str = "completed"  # NEW: Add status field

class ConversationHistory(BaseModel):
    session_id: str
    messages: List[Dict[str, Any]]



=== legal_assistant/models/enums.py ===
"""Enumeration types for the legal assistant application"""
from enum import Enum

class AnalysisType(str, Enum):
    """Types of analysis that can be performed on legal documents"""
    COMPREHENSIVE = "comprehensive"
    SUMMARY = "document_summary"
    CLAUSES = "key_clauses"
    RISKS = "risk_assessment"
    TIMELINE = "timeline_deadlines"
    OBLIGATIONS = "party_obligations"
    MISSING_CLAUSES = "missing_clauses"



=== legal_assistant/processors/__init__.py ===
"""Processors package"""
from .query_processor import process_query

__all__ = ['process_query']



=== legal_assistant/processors/query_processor.py ===
"""Query processing logic - Enhanced for better statutory analysis"""
import re
import logging
import traceback
from typing import Optional

from ..models import QueryResponse, ComprehensiveAnalysisRequest, AnalysisType
from ..config import FeatureFlags, OPENROUTER_API_KEY, MIN_RELEVANCE_SCORE
from ..services import (
    ComprehensiveAnalysisProcessor,
    combined_search,
    calculate_confidence_score,
    call_openrouter_api
)
from ..storage.managers import add_to_conversation, get_conversation_context
from ..utils import (
    parse_multiple_questions,
    extract_bill_information,
    extract_universal_information,
    format_context_for_llm
)

logger = logging.getLogger(__name__)

def detect_statutory_question(question: str) -> bool:
    """Detect if this is a statutory/regulatory question requiring detailed extraction"""
    statutory_indicators = [
        # Federal Laws and Regulations
        r'\bUSC\s+\d+',                    # US Code
        r'\bU\.S\.C\.\s*¬ß?\s*\d+',         # U.S.C. ¬ß 123
        r'\bCFR\s+\d+',                    # Code of Federal Regulations
        r'\bC\.F\.R\.\s*¬ß?\s*\d+',         # C.F.R. ¬ß 123
        r'\bFed\.\s*R\.\s*Civ\.\s*P\.',    # Federal Rules of Civil Procedure
        r'\bFed\.\s*R\.\s*Crim\.\s*P\.',   # Federal Rules of Criminal Procedure
        r'\bFed\.\s*R\.\s*Evid\.',         # Federal Rules of Evidence
        
        # Washington State
        r'\bRCW\s+\d+\.\d+\.\d+',          # Washington Revised Code
        r'\bWAC\s+\d+',                    # Washington Administrative Code
        r'\bWash\.\s*Rev\.\s*Code\s*¬ß?\s*\d+', # Washington Revised Code alternate format
        
        # California
        r'\bCal\.\s*(?:Bus\.\s*&\s*Prof\.|Civ\.|Comm\.|Corp\.|Educ\.|Fam\.|Gov\.|Health\s*&\s*Safety|Ins\.|Lab\.|Penal|Prob\.|Pub\.\s*Util\.|Rev\.\s*&\s*Tax\.|Veh\.|Welf\.\s*&\s*Inst\.)\s*Code\s*¬ß?\s*\d+',
        r'\bCalifornia\s+(?:Business|Civil|Commercial|Corporation|Education|Family|Government|Health|Insurance|Labor|Penal|Probate|Public\s+Utilities|Revenue|Vehicle|Welfare)\s+Code\s*¬ß?\s*\d+',
        r'\bCal\.\s*Code\s*Regs\.\s*tit\.\s*\d+',  # California Code of Regulations
        
        # Texas
        r'\bTex\.\s*(?:Agric\.|Alco\.|Bus\.\s*&\s*Com\.|Civ\.\s*Prac\.\s*&\s*Rem\.|Code\s*Crim\.\s*Proc\.|Educ\.|Elec\.|Fam\.|Gov\.|Health\s*&\s*Safety|Hum\.\s*Res\.|Ins\.|Lab\.|Loc\.\s*Gov\.|Nat\.\s*Res\.|Occ\.|Parks\s*&\s*Wild\.|Penal|Prop\.|Tax|Transp\.|Util\.|Water)\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+',
        r'\bTexas\s+(?:Agriculture|Alcoholic\s+Beverage|Business|Civil\s+Practice|Criminal\s+Procedure|Education|Election|Family|Government|Health|Human\s+Resources|Insurance|Labor|Local\s+Government|Natural\s+Resources|Occupations|Parks|Penal|Property|Tax|Transportation|Utilities|Water)\s+Code\s*¬ß?\s*\d+',
        r'\bTex\.\s*Admin\.\s*Code\s*tit\.\s*\d+',  # Texas Administrative Code
        
        # New York
        r'\bN\.Y\.\s*(?:Agric\.\s*&\s*Mkts\.|Arts\s*&\s*Cult\.\s*Aff\.|Bank\.|Bus\.\s*Corp\.|Civ\.\s*Prac\.\s*L\.\s*&\s*R\.|Civ\.\s*Rights|Civ\.\s*Serv\.|Com\.|Correct\.|County|Crim\.\s*Proc\.|Dom\.\s*Rel\.|Econ\.\s*Dev\.|Educ\.|Elec\.|Empl\.|Energy|Envtl\.\s*Conserv\.|Est\.\s*Powers\s*&\s*Trusts|Exec\.|Fam\.\s*Ct\.\s*Act|Gen\.\s*Bus\.|Gen\.\s*City|Gen\.\s*Constr\.|Gen\.\s*Mun\.|Gen\.\s*Oblig\.|High\.|Indian|Ins\.|Jud\.|Lab\.|Lien|Local\s*Fin\.|Ment\.\s*Hyg\.|Mil\.|Multi-Dwell\.|Multi-Mun\.|Nav\.|Not-for-Profit\s*Corp\.|Parks\s*Rec\.\s*&\s*Hist\.\s*Preserv\.|Penal|Pers\.\s*Prop\.|Priv\.\s*Hous\.\s*Fin\.|Pub\.\s*Auth\.|Pub\.\s*Health|Pub\.\s*Hous\.|Pub\.\s*Off\.|Pub\.\s*Serv\.|Racing\s*Pari-Mut\.\s*Wag\.\s*&\s*Breed\.|Real\s*Prop\.|Real\s*Prop\.\s*Actions\s*&\s*Proc\.|Real\s*Prop\.\s*Tax|Relig\.\s*Corp\.|Retire\.\s*&\s*Soc\.\s*Sec\.|Rural\s*Elec\.\s*Coop\.|Second\s*Class\s*Cities|Soc\.\s*Serv\.|State|State\s*Fin\.|Surr\.\s*Ct\.\s*Proc\.\s*Act|Tax|Town|Transp\.|Transp\.\s*Corp\.|U\.C\.C\.|Unconsol\.|Veh\.\s*&\s*Traf\.|Vill\.|Vol\.\s*Fire\s*Benefit|Workers\'\s*Comp\.)\s*(?:Law)?\s*¬ß?\s*\d+',
        r'\bNew\s+York\s+(?:Criminal\s+Procedure|Penal|Civil\s+Practice|Family\s+Court\s+Act|General\s+Business|Vehicle\s+and\s+Traffic)\s+Law\s*¬ß?\s*\d+',
        r'\bN\.Y\.C\.R\.R\.\s*tit\.\s*\d+',         # New York Codes, Rules and Regulations
        
        # Florida
        r'\bFla\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Florida Statutes
        r'\bFlorida\s+Statutes\s*¬ß?\s*\d+',
        r'\bFla\.\s*Admin\.\s*Code\s*Ann\.\s*r\.\s*\d+', # Florida Administrative Code
        
        # Illinois
        r'\b\d+\s*ILCS\s*\d+', # Illinois Compiled Statutes (e.g., 720 ILCS 5)
        r'\bIll\.\s*Comp\.\s*Stat\.\s*ch\.\s*\d+', # Illinois Compiled Statutes alternate
        r'\bIll\.\s*Admin\.\s*Code\s*tit\.\s*\d+', # Illinois Administrative Code
        
        # Pennsylvania
        r'\b\d+\s*Pa\.\s*(?:C\.S\.|Cons\.\s*Stat\.)\s*¬ß?\s*\d+', # Pennsylvania Consolidated Statutes
        r'\bPa\.\s*Code\s*¬ß?\s*\d+',               # Pennsylvania Code
        
        # Ohio
        r'\bOhio\s*Rev\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Ohio Revised Code
        r'\bOhio\s*Admin\.\s*Code\s*\d+',          # Ohio Administrative Code
        
        # Georgia
        r'\bO\.C\.G\.A\.\s*¬ß?\s*\d+',              # Official Code of Georgia Annotated
        r'\bGa\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+',   # Georgia Code
        r'\bGa\.\s*Comp\.\s*R\.\s*&\s*Regs\.\s*r\.\s*\d+', # Georgia Rules and Regulations
        
        # North Carolina
        r'\bN\.C\.\s*Gen\.\s*Stat\.\s*¬ß?\s*\d+',   # North Carolina General Statutes
        r'\bN\.C\.\s*Admin\.\s*Code\s*tit\.\s*\d+', # North Carolina Administrative Code
        
        # Michigan
        r'\bMich\.\s*Comp\.\s*Laws\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Michigan Compiled Laws
        r'\bMich\.\s*Admin\.\s*Code\s*r\.\s*\d+',   # Michigan Administrative Code
        
        # New Jersey
        r'\bN\.J\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # New Jersey Statutes
        r'\bN\.J\.\s*Admin\.\s*Code\s*¬ß?\s*\d+',    # New Jersey Administrative Code
        
        # Virginia
        r'\bVa\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+',   # Virginia Code
        r'\bVirginia\s+Code\s*¬ß?\s*\d+',
        r'\bVa\.\s*Admin\.\s*Code\s*¬ß?\s*\d+',      # Virginia Administrative Code
        
        # Massachusetts
        r'\bMass\.\s*Gen\.\s*Laws\s*(?:Ann\.)?\s*ch\.\s*\d+', # Massachusetts General Laws
        r'\bM\.G\.L\.\s*ch?\.\s*\d+',               # Massachusetts General Laws (abbreviated)
        r'\bMass\.\s*Regs\.\s*Code\s*tit\.\s*\d+', # Massachusetts Regulations
        
        # Maryland
        r'\bMd\.\s*Code\s*(?:Ann\.)?(?:\s*,\s*\w+)?\s*¬ß?\s*\d+', # Maryland Code (with possible subject area)
        r'\bCOMR\s*\d+',                           # Code of Maryland Regulations
        
        # Wisconsin
        r'\bWis\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Wisconsin Statutes
        r'\bWis\.\s*Admin\.\s*Code\s*¬ß?\s*\d+',     # Wisconsin Administrative Code
        
        # Minnesota
        r'\bMinn\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Minnesota Statutes
        r'\bMinn\.\s*R\.\s*\d+',                   # Minnesota Rules
        
        # Colorado
        r'\bColo\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Colorado Revised Statutes
        r'\bC\.R\.S\.\s*¬ß?\s*\d+',                 # Colorado Revised Statutes (abbreviated)
        r'\bColo\.\s*Code\s*Regs\.\s*¬ß?\s*\d+',    # Colorado Code of Regulations
        
        # Arizona
        r'\bAriz\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Arizona Revised Statutes
        r'\bA\.R\.S\.\s*¬ß?\s*\d+',                 # Arizona Revised Statutes (abbreviated)
        r'\bAriz\.\s*Admin\.\s*Code\s*¬ß?\s*\d+',   # Arizona Administrative Code
        
        # Tennessee
        r'\bTenn\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Tennessee Code
        r'\bT\.C\.A\.\s*¬ß?\s*\d+',                 # Tennessee Code (abbreviated)
        r'\bTenn\.\s*Comp\.\s*R\.\s*&\s*Regs\.\s*\d+', # Tennessee Rules and Regulations
        
        # Missouri
        r'\bMo\.\s*(?:Ann\.\s*)?Stat\.\s*¬ß?\s*\d+', # Missouri Statutes
        r'\bR\.S\.Mo\.\s*¬ß?\s*\d+',                # Revised Statutes of Missouri
        r'\bMo\.\s*Code\s*Regs\.\s*Ann\.\s*tit\.\s*\d+', # Missouri Code of Regulations
        
        # Indiana
        r'\bInd\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+',  # Indiana Code
        r'\bI\.C\.\s*¬ß?\s*\d+',                    # Indiana Code (abbreviated)
        r'\bInd\.\s*Admin\.\s*Code\s*tit\.\s*\d+', # Indiana Administrative Code
        
        # Louisiana
        r'\bLa\.\s*(?:Civ\.|Rev\.\s*Stat\.\s*Ann\.|Code\s*Civ\.\s*Proc\.\s*Ann\.|Code\s*Crim\.\s*Proc\.\s*Ann\.|Code\s*Evid\.\s*Ann\.|Const\.|R\.S\.)\s*(?:art\.)?\s*¬ß?\s*\d+', # Louisiana various codes
        r'\bLouisiana\s+(?:Civil|Revised\s+Statutes|Criminal|Evidence)\s+Code\s*(?:art\.)?\s*¬ß?\s*\d+',
        r'\bLa\.\s*Admin\.\s*Code\s*tit\.\s*\d+',  # Louisiana Administrative Code
        
        # Alabama
        r'\bAla\.\s*Code\s*¬ß?\s*\d+',              # Alabama Code
        r'\bAlabama\s+Code\s*¬ß?\s*\d+',
        r'\bAla\.\s*Admin\.\s*Code\s*r\.\s*\d+',   # Alabama Administrative Code
        
        # South Carolina
        r'\bS\.C\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+', # South Carolina Code
        r'\bS\.C\.\s*Code\s*Regs\.\s*\d+',         # South Carolina Code of Regulations
        
        # Kentucky
        r'\bKy\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Kentucky Revised Statutes
        r'\bK\.R\.S\.\s*¬ß?\s*\d+',                 # Kentucky Revised Statutes (abbreviated)
        r'\bKy\.\s*Admin\.\s*Regs\.\s*tit\.\s*\d+', # Kentucky Administrative Regulations
        
        # Oregon
        r'\bOr\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Oregon Revised Statutes
        r'\bO\.R\.S\.\s*¬ß?\s*\d+',                 # Oregon Revised Statutes (abbreviated)
        r'\bOr\.\s*Admin\.\s*R\.\s*\d+',           # Oregon Administrative Rules
        
        # Oklahoma
        r'\bOkla\.\s*Stat\.\s*(?:Ann\.)?\s*tit\.\s*\d+', # Oklahoma Statutes
        r'\bOklahoma\s+Statutes\s+tit\.\s*\d+',
        r'\bOkla\.\s*Admin\.\s*Code\s*¬ß?\s*\d+',   # Oklahoma Administrative Code
        
        # Connecticut
        r'\bConn\.\s*Gen\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Connecticut General Statutes
        r'\bC\.G\.S\.\s*¬ß?\s*\d+',                 # Connecticut General Statutes (abbreviated)
        r'\bConn\.\s*Agencies\s*Regs\.\s*¬ß?\s*\d+', # Connecticut Agencies Regulations
        
        # Iowa
        r'\bIowa\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+',   # Iowa Code
        r'\bI\.C\.\s*¬ß?\s*\d+',                    # Iowa Code (abbreviated) - Note: conflicts with Indiana
        r'\bIowa\s*Admin\.\s*Code\s*r\.\s*\d+',    # Iowa Administrative Code
        
        # Arkansas
        r'\bArk\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+',  # Arkansas Code
        r'\bA\.C\.A\.\s*¬ß?\s*\d+',                 # Arkansas Code (abbreviated)
        r'\bArk\.\s*Code\s*R\.\s*\d+',             # Arkansas Code of Rules
        
        # Mississippi
        r'\bMiss\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Mississippi Code
        r'\bMississippi\s+Code\s*¬ß?\s*\d+',
        
        # Kansas
        r'\bKan\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Kansas Statutes
        r'\bK\.S\.A\.\s*¬ß?\s*\d+',                 # Kansas Statutes (abbreviated)
        r'\bKan\.\s*Admin\.\s*Regs\.\s*¬ß?\s*\d+',  # Kansas Administrative Regulations
        
        # Utah
        r'\bUtah\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+',   # Utah Code
        r'\bU\.C\.A\.\s*¬ß?\s*\d+',                 # Utah Code (abbreviated)
        r'\bUtah\s*Admin\.\s*Code\s*r\.\s*\d+',    # Utah Administrative Code
        
        # Nevada
        r'\bNev\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Nevada Revised Statutes
        r'\bN\.R\.S\.\s*¬ß?\s*\d+',                 # Nevada Revised Statutes (abbreviated)
        r'\bNev\.\s*Admin\.\s*Code\s*¬ß?\s*\d+',    # Nevada Administrative Code
        
        # New Mexico
        r'\bN\.M\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # New Mexico Statutes
        r'\bNMSA\s*¬ß?\s*\d+',                      # New Mexico Statutes (abbreviated)
        r'\bN\.M\.\s*Code\s*R\.\s*¬ß?\s*\d+',       # New Mexico Code of Rules
        
        # West Virginia
        r'\bW\.\s*Va\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+', # West Virginia Code
        r'\bW\.Va\.\s*Code\s*R\.\s*¬ß?\s*\d+',      # West Virginia Code of Rules
        
        # Nebraska
        r'\bNeb\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Nebraska Revised Statutes
        r'\bR\.R\.S\.\s*Neb\.\s*¬ß?\s*\d+',         # Revised Revised Statutes Nebraska
        r'\bNeb\.\s*Admin\.\s*R\.\s*&\s*Regs\.\s*¬ß?\s*\d+', # Nebraska Administrative Rules
        
        # Idaho
        r'\bIdaho\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+',  # Idaho Code
        r'\bI\.C\.\s*¬ß?\s*\d+',                    # Idaho Code (abbreviated) - Note: conflicts with others
        r'\bIDAPA\s*\d+',                          # Idaho Administrative Procedures Act
        
        # Hawaii
        r'\bHaw\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Hawaii Revised Statutes
        r'\bH\.R\.S\.\s*¬ß?\s*\d+',                 # Hawaii Revised Statutes (abbreviated)
        r'\bHaw\.\s*Code\s*R\.\s*¬ß?\s*\d+',        # Hawaii Code of Rules
        
        # New Hampshire
        r'\bN\.H\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # New Hampshire Revised Statutes
        r'\bR\.S\.A\.\s*¬ß?\s*\d+',                 # Revised Statutes Annotated
        r'\bN\.H\.\s*Code\s*Admin\.\s*R\.\s*¬ß?\s*\d+', # New Hampshire Code of Administrative Rules
        
        # Maine
        r'\bMe\.\s*Rev\.\s*Stat\.\s*(?:Ann\.)?\s*tit\.\s*\d+', # Maine Revised Statutes
        r'\bM\.R\.S\.\s*tit\.\s*\d+',              # Maine Revised Statutes (abbreviated)
        r'\bMe\.\s*Code\s*R\.\s*¬ß?\s*\d+',         # Maine Code of Rules
        
        # Rhode Island
        r'\bR\.I\.\s*Gen\.\s*Laws\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Rhode Island General Laws
        r'\bR\.I\.\s*Code\s*R\.\s*¬ß?\s*\d+',       # Rhode Island Code of Rules
        
        # Montana
        r'\bMont\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Montana Code
        r'\bM\.C\.A\.\s*¬ß?\s*\d+',                 # Montana Code (abbreviated)
        r'\bMont\.\s*Admin\.\s*R\.\s*¬ß?\s*\d+',    # Montana Administrative Rules
        
        # Delaware
        r'\bDel\.\s*Code\s*(?:Ann\.)?\s*tit\.\s*\d+', # Delaware Code
        r'\bDel\.\s*Admin\.\s*Code\s*tit\.\s*\d+', # Delaware Administrative Code
        
        # South Dakota
        r'\bS\.D\.\s*Codified\s*Laws\s*(?:Ann\.)?\s*¬ß?\s*\d+', # South Dakota Codified Laws
        r'\bSDCL\s*¬ß?\s*\d+',                      # South Dakota Codified Laws (abbreviated)
        r'\bS\.D\.\s*Admin\.\s*R\.\s*¬ß?\s*\d+',    # South Dakota Administrative Rules
        
        # North Dakota
        r'\bN\.D\.\s*Cent\.\s*Code\s*(?:Ann\.)?\s*¬ß?\s*\d+', # North Dakota Century Code
        r'\bN\.D\.C\.C\.\s*¬ß?\s*\d+',              # North Dakota Century Code (abbreviated)
        r'\bN\.D\.\s*Admin\.\s*Code\s*¬ß?\s*\d+',   # North Dakota Administrative Code
        
        # Alaska
        r'\bAlaska\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Alaska Statutes
        r'\bA\.S\.\s*¬ß?\s*\d+',                    # Alaska Statutes (abbreviated)
        r'\bAlaska\s*Admin\.\s*Code\s*tit\.\s*\d+', # Alaska Administrative Code
        
        # Vermont
        r'\bVt\.\s*Stat\.\s*(?:Ann\.)?\s*tit\.\s*\d+', # Vermont Statutes
        r'\bV\.S\.A\.\s*tit\.\s*\d+',              # Vermont Statutes Annotated (abbreviated)
        r'\bVt\.\s*Code\s*R\.\s*¬ß?\s*\d+',         # Vermont Code of Rules
        
        # Wyoming
        r'\bWyo\.\s*Stat\.\s*(?:Ann\.)?\s*¬ß?\s*\d+', # Wyoming Statutes
        r'\bW\.S\.\s*¬ß?\s*\d+',                    # Wyoming Statutes (abbreviated)
        r'\bWyo\.\s*Code\s*R\.\s*¬ß?\s*\d+',        # Wyoming Code of Rules
        
        # District of Columbia
        r'\bD\.C\.\s*(?:Code|Official\s*Code)\s*(?:Ann\.)?\s*¬ß?\s*\d+', # DC Code
        r'\bD\.C\.M\.R\.\s*¬ß?\s*\d+',              # DC Municipal Regulations
        
        # Generic statutory terms (these should come after specific state patterns)
        r'\bstatute[s]?', r'\bregulation[s]?', r'\bcode\s+section[s]?',
        r'\bminimum\s+standards?', r'\brequirements?', r'\bmust\s+meet',
        r'\bstandards?.*regulat', r'\bcomposition.*conduct',
        r'\bpolicies.*record[- ]keeping', r'\bmandatory\s+(?:standards?|requirements?)',
        r'\bshall\s+(?:meet|comply|maintain)', r'\bmust\s+(?:include|contain|provide)',
        r'\brequired\s+(?:by\s+law|under|pursuant\s+to)',
        r'\bregulatory\s+(?:standards?|requirements?|compliance)',
        r'\bstatutory\s+(?:mandate|requirement|provision)',
        r'\blegal\s+(?:standards?|requirements?|obligations?)',
        r'\bcompliance\s+with.*(?:statute|regulation|code)',
        r'\badministrative\s+(?:rule[s]?|regulation[s]?|code)',
    ]
    
    for pattern in statutory_indicators:
        if re.search(pattern, question, re.IGNORECASE):
            return True
    return False

def create_statutory_prompt(context_text: str, question: str, conversation_context: str, 
                          sources_searched: list, retrieval_method: str, document_id: str = None) -> str:
    """Create an enhanced prompt specifically for statutory analysis"""
    
    return f"""You are a legal research assistant specializing in statutory analysis. Your job is to extract COMPLETE, SPECIFIC information from legal documents.

STRICT SOURCE REQUIREMENTS:
- Answer ONLY based on the retrieved documents provided in the context
- Do NOT use general legal knowledge, training data, assumptions, or inferences beyond what's explicitly stated
- If information is not in the provided documents, state: "This information is not available in the provided documents"

üî¥ CRITICAL: You MUST extract actual numbers, durations, and specific requirements. NEVER use placeholders like "[duration not specified]" or "[requirements not listed]".

üî• CRITICAL FAILURE PREVENTION:
- If you write "[duration not specified]" you have FAILED at your job
- If you write "[duties not specified]" you have FAILED at your job  
- If you write "[requirements not listed]" you have FAILED at your job
- READ EVERY WORD of the context before claiming information is missing
- The human is counting on you to find the actual requirements
- Your job is to be a thorough legal researcher, not a lazy summarizer

MANDATORY EXTRACTION RULES:
1. üìñ READ EVERY WORD of the provided context before claiming anything is missing
2. üî¢ EXTRACT ALL NUMBERS: durations (60 minutes), quantities (25 people), percentages, dollar amounts
3. üìù QUOTE EXACT LANGUAGE: Use quotation marks for statutory text
4. üìã LIST ALL REQUIREMENTS: Number each requirement found (1., 2., 3., etc.)
5. üéØ BE SPECIFIC: Include section numbers, subsection letters, paragraph numbers
6. ‚ö†Ô∏è ONLY claim information is "missing" after thorough analysis of ALL provided text

INSTRUCTIONS FOR THOROUGH ANALYSIS:
1. **READ CAREFULLY**: Scan the entire context for information that answers the user's question
2. **EXTRACT COMPLETELY**: When extracting requirements, include FULL details (e.g., "60 minutes" not just "minimum of")
3. **QUOTE VERBATIM**: For statutory standards, use exact quotes: `"[Exact Text]" (Source)`
4. **ENUMERATE EXPLICITLY**: Present listed requirements as numbered points with full quotes
5. **CITE SOURCES**: Reference the document name for each fact
6. **BE COMPLETE**: Explicitly note missing standards: "Documents lack full subsection [X]"
7. **USE DECISIVE PHRASING**: State facts directly ("The statute requires...") - NEVER "documents indicate"

SOURCES SEARCHED: {', '.join(sources_searched)}
RETRIEVAL METHOD: {retrieval_method}
{f"DOCUMENT FILTER: Specific document {document_id}" if document_id else "DOCUMENT SCOPE: All available documents"}

RESPONSE FORMAT REQUIRED FOR STATUTORY QUESTIONS:

## SPECIFIC REQUIREMENTS FOUND:
[List each requirement with exact quotes and citations]

## NUMERICAL STANDARDS:
[Extract ALL numbers, durations, thresholds, limits]

## PROCEDURAL RULES:
[Detail composition, conduct, attendance, record-keeping rules]

## ROLES AND RESPONSIBILITIES:
[Define each party's specific duties with citations]

## INFORMATION NOT FOUND:
[Only list what is genuinely absent after thorough review]

EXAMPLES OF WHAT YOU MUST DO:

‚ùå WRONG: "The victim impact panel should be a minimum of [duration not specified]."
‚úÖ RIGHT: "The statute requires victim impact panels to be 'a minimum of sixty (60) minutes in duration' (RCW 10.01.230(3)(a))."

‚ùå WRONG: "Specific details on composition are not available"
‚úÖ RIGHT: "Panel composition requirements include: (1) 'at least two victim impact speakers' (RCW 10.01.230(2)(a)), (2) 'one trained facilitator' (RCW 10.01.230(2)(b)), (3) 'maximum of twenty-five participants per session' (RCW 10.01.230(2)(c))."

‚ùå WRONG: "The facilitator's role is not specified"
‚úÖ RIGHT: "The designated facilitator must: (1) 'maintain order during presentations' (RCW 10.01.230(4)(a)), (2) 'ensure compliance with attendance policies' (RCW 10.01.230(4)(b)), (3) 'submit quarterly reports to the Traffic Safety Commission' (RCW 10.01.230(4)(c))."

CONVERSATION HISTORY:
{conversation_context}

DOCUMENT CONTEXT TO ANALYZE WORD-BY-WORD:
{context_text}

USER QUESTION REQUIRING COMPLETE EXTRACTION:
{question}

MANDATORY STEPS FOR YOUR RESPONSE:
1. üîç Scan the ENTIRE context for specific numbers, requirements, and procedures - READ EVERY WORD
2. üìä Extract ALL quantitative information (minutes, hours, numbers of people, etc.) - NO PLACEHOLDERS ALLOWED
3. üìú Quote the exact statutory language for each requirement - FULL QUOTES, NOT SUMMARIES
4. üè∑Ô∏è Provide specific citations (section, subsection, paragraph) - EXACT REFERENCES
5. üìù Organize information clearly with headers and numbered lists
6. ‚ö†Ô∏è Only claim information is missing if it's truly not in the provided text AFTER reading every word twice

üö® BEFORE YOU RESPOND: Ask yourself these questions:
- Did I read every single word of the provided context?
- Did I look for numbers, durations, quantities in ALL sections?
- Am I using any lazy placeholders like "[not specified]"?
- Can I quote the exact text that supports each claim I'm making?

BEGIN THOROUGH STATUTORY ANALYSIS:

ADDITIONAL GUIDANCE:
- After fully answering based solely on the provided documents, if relevant key legal principles under Washington state law, any other U.S. state law, or U.S. federal law are not found in the sources, you may add a clearly labeled general legal principles disclaimer.
- This disclaimer must clearly state it is NOT based on the provided documents but represents general background knowledge of applicable Washington state, other state, and federal law.
- Do NOT use this disclaimer to answer the user's question directly; it serves only as supplementary context.
- This disclaimer must explicitly state that these principles are not found in the provided documents but are usually relevant legal background.
- Format this disclaimer distinctly at the end of the response under a heading such as "GENERAL LEGAL PRINCIPLES DISCLAIMER."

RESPONSE:"""

def create_regular_prompt(context_text: str, question: str, conversation_context: str, 
                         sources_searched: list, retrieval_method: str, document_id: str = None, 
                         instruction: str = "balanced") -> str:
    """Create the regular prompt for non-statutory questions"""
    
    return f"""You are a legal research assistant. Provide thorough, accurate responses based on the provided documents.

STRICT SOURCE REQUIREMENTS:
- Answer ONLY based on the retrieved documents provided in the context
- Do NOT use general legal knowledge, training data, assumptions, or inferences beyond what's explicitly stated
- If information is not in the provided documents, state: "This information is not available in the provided documents"

SOURCES SEARCHED: {', '.join(sources_searched)}
RETRIEVAL METHOD: {retrieval_method}
{f"DOCUMENT FILTER: Specific document {document_id}" if document_id else "DOCUMENT SCOPE: All available documents"}

HALLUCINATION CHECK - Before responding, verify:
1. Is each claim supported by the retrieved documents?
2. Am I adding information not present in the sources?
3. If uncertain, default to "information not available"

INSTRUCTIONS FOR THOROUGH ANALYSIS:
1. **READ CAREFULLY**: Scan the entire context for information that answers the user's question
2. **EXTRACT COMPLETELY**: When extracting requirements, include FULL details (e.g., "60 minutes" not just "minimum of")
3. **QUOTE VERBATIM**: For statutory standards, use exact quotes: `"[Exact Text]" (Source)`
4. **ENUMERATE EXPLICITLY**: Present listed requirements as numbered points with full quotes
5. **CITE SOURCES**: Reference the document name for each fact
6. **BE COMPLETE**: Explicitly note missing standards: "Documents lack full subsection [X]"
7. **USE DECISIVE PHRASING**: State facts directly ("The statute requires...") - NEVER "documents indicate"

RESPONSE STYLE: {instruction}

CONVERSATION HISTORY:
{conversation_context}

DOCUMENT CONTEXT (ANALYZE THOROUGHLY):
{context_text}

USER QUESTION:
{question}

RESPONSE APPROACH:
- **FIRST**: Identify what specific information the user is asking for. Do not reference any statute, case law, or principle unless it appears verbatim in the context.
- **SECOND**: Search the context thoroughly for that information  
- **THIRD**: Present any information found clearly and completely. At the end of your response, list all facts provided and their source documents for verification.
- **FOURTH**: Note what information is not available (if any)
- **ALWAYS**: Cite the source document for each fact provided

ADDITIONAL GUIDANCE:
- After fully answering based solely on the provided documents, if relevant key legal principles under Washington state law, any other U.S. state law, or U.S. federal law are not found in the sources, you may add a clearly labeled general legal principles disclaimer.
- This disclaimer must clearly state it is NOT based on the provided documents but represents general background knowledge of applicable Washington state, other state, and federal law.
- Do NOT use this disclaimer to answer the user's question directly; it serves only as supplementary context.
- This disclaimer must explicitly state that these principles are not found in the provided documents but are usually relevant legal background.
- Format this disclaimer distinctly at the end of the response under a heading such as "GENERAL LEGAL PRINCIPLES DISCLAIMER."

RESPONSE:"""

def process_query(question: str, session_id: str, user_id: Optional[str], search_scope: str, 
                 response_style: str = "balanced", use_enhanced_rag: bool = True, 
                 document_id: str = None) -> QueryResponse:
    """Main query processing function with enhanced statutory handling"""
    try:
        logger.info(f"Processing query - Question: '{question}', User: {user_id}, Scope: {search_scope}, Enhanced: {use_enhanced_rag}, Document: {document_id}")
        
        # Check if this is a statutory question
        is_statutory = detect_statutory_question(question)
        if is_statutory:
            logger.info("üèõÔ∏è Detected statutory/regulatory question - using enhanced extraction")
        
        if any(phrase in question.lower() for phrase in ["comprehensive analysis", "complete analysis", "full analysis"]):
            logger.info("Detected comprehensive analysis request")
            
            try:
                comp_request = ComprehensiveAnalysisRequest(
                    document_id=document_id,
                    analysis_types=[AnalysisType.COMPREHENSIVE],
                    user_id=user_id or "default_user",
                    session_id=session_id,
                    response_style=response_style
                )
                
                processor = ComprehensiveAnalysisProcessor()
                comp_result = processor.process_comprehensive_analysis(comp_request)
                
                formatted_response = f"""# Comprehensive Legal Document Analysis

## Document Summary
{comp_result.document_summary or 'No summary available'}

## Key Clauses Analysis
{comp_result.key_clauses or 'No clauses analysis available'}

## Risk Assessment
{comp_result.risk_assessment or 'No risk assessment available'}

## Timeline & Deadlines
{comp_result.timeline_deadlines or 'No timeline information available'}

## Party Obligations
{comp_result.party_obligations or 'No obligations analysis available'}

## Missing Clauses Analysis
{comp_result.missing_clauses or 'No missing clauses analysis available'}

---
**Analysis Confidence:** {comp_result.overall_confidence:.1%}
**Processing Time:** {comp_result.processing_time:.2f} seconds

**Sources:** {len(comp_result.sources_by_section.get('summary', []))} document sections analyzed
"""
                
                add_to_conversation(session_id, "user", question)
                add_to_conversation(session_id, "assistant", formatted_response)
                
                return QueryResponse(
                    response=formatted_response,
                    error=None,
                    context_found=True,
                    sources=comp_result.sources_by_section.get('summary', []),
                    session_id=session_id,
                    confidence_score=comp_result.overall_confidence,
                    expand_available=False,
                    sources_searched=["comprehensive_analysis"],
                    retrieval_method=comp_result.retrieval_method
                )
                
            except Exception as e:
                logger.error(f"Comprehensive analysis failed: {e}")
        
        questions = parse_multiple_questions(question) if use_enhanced_rag else [question]
        combined_query = " ".join(questions)
        
        conversation_context = get_conversation_context(session_id)
        
        # For statutory questions, get more context
        search_k = 20 if is_statutory else 10
        
        retrieved_results, sources_searched, retrieval_method = combined_search(
            combined_query, 
            user_id, 
            search_scope, 
            conversation_context,
            use_enhanced=use_enhanced_rag,
            k=search_k,
            document_id=document_id
        )
        
        if not retrieved_results:
            return QueryResponse(
                response="I couldn't find any relevant information to answer your question in the searched sources.",
                error=None,
                context_found=False,
                sources=[],
                session_id=session_id,
                confidence_score=0.1,
                sources_searched=sources_searched,
                retrieval_method=retrieval_method
            )
        
        # Format context for LLM - more context for statutory questions
        max_context_length = 8000 if is_statutory else 3000
        context_text, source_info = format_context_for_llm(retrieved_results, max_length=max_context_length)
        
        # Enhanced information extraction
        bill_match = re.search(r"(HB|SB|SSB|ESSB|SHB|ESHB)\s*(\d+)", question, re.IGNORECASE)
        statute_match = re.search(r"(RCW|USC|CFR|WAC)\s+(\d+\.\d+\.\d+|\d+)", question, re.IGNORECASE)
        extracted_info = {}

        if bill_match:
            # Bill-specific extraction
            bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
            logger.info(f"Searching for bill: {bill_number}")
            
            # Search specifically for chunks containing this bill
            bill_specific_results = []
            for doc, score in retrieved_results:
                if 'contains_bills' in doc.metadata and bill_number in doc.metadata['contains_bills']:
                    bill_specific_results.append((doc, score))
                    logger.info(f"Found {bill_number} in chunk {doc.metadata.get('chunk_index', 'unknown')} with score {score}")
            
            # If we found bill-specific chunks, prioritize them
            if bill_specific_results:
                logger.info(f"Using {len(bill_specific_results)} bill-specific chunks for {bill_number}")
                # Use the bill-specific chunks with boosted relevance
                boosted_results = [(doc, min(score + 0.3, 1.0)) for doc, score in bill_specific_results]
                retrieved_results = boosted_results + [r for r in retrieved_results if r not in bill_specific_results]
                retrieved_results = retrieved_results[:len(retrieved_results)]
            
            extracted_info = extract_bill_information(context_text, bill_number)
        elif statute_match:
            # Statute-specific extraction
            statute_citation = f"{statute_match.group(1)} {statute_match.group(2)}"
            logger.info(f"üèõÔ∏è Searching for statute: {statute_citation}")
            extracted_info = extract_statutory_information(context_text, statute_citation)
        else:
            # Universal extraction for any document type
            extracted_info = extract_universal_information(context_text, question)

        # Add extracted information to context to make it more visible to AI
        if extracted_info:
            enhancement = "\n\nKEY INFORMATION FOUND:\n"
            for key, value in extracted_info.items():
                if value:  # Only add if there's actual content
                    if isinstance(value, list):
                        enhancement += f"- {key.replace('_', ' ').title()}: {', '.join(value[:5])}\n"
                    else:
                        enhancement += f"- {key.replace('_', ' ').title()}: {value}\n"
            
            if enhancement.strip() != "KEY INFORMATION FOUND:":
                context_text += enhancement
        
        style_instructions = {
            "concise": "Please provide a concise answer (1-2 sentences) based on the context.",
            "balanced": "Please provide a balanced answer (2-3 paragraphs) based on the context.",
            "detailed": "Please provide a detailed answer with explanations based on the context."
        }
        
        instruction = style_instructions.get(response_style, style_instructions["balanced"])
        
        # Choose the appropriate prompt based on question type
        if is_statutory:
            prompt = create_statutory_prompt(context_text, question, conversation_context, 
                                           sources_searched, retrieval_method, document_id)
        else:
            prompt = create_regular_prompt(context_text, question, conversation_context, 
                                         sources_searched, retrieval_method, document_id, instruction)
        
        if FeatureFlags.AI_ENABLED and OPENROUTER_API_KEY:
            response_text = call_openrouter_api(prompt, OPENROUTER_API_KEY)
        else:
            response_text = f"Based on the retrieved documents:\n\n{context_text}\n\nPlease review this information to answer your question."
        
        relevant_sources = [s for s in source_info if s['relevance'] >= MIN_RELEVANCE_SCORE]
        
        if relevant_sources:
            response_text += "\n\n**SOURCES:**"
            for source in relevant_sources:
                source_type = source['source_type'].replace('_', ' ').title()
                page_info = f", Page {source['page']}" if source['page'] is not None else ""
                response_text += f"\n- [{source_type}] {source['file_name']}{page_info} (Relevance: {source['relevance']:.2f})"
        
        confidence_score = calculate_confidence_score(retrieved_results, len(response_text))
        
        add_to_conversation(session_id, "user", question)
        add_to_conversation(session_id, "assistant", response_text, source_info)
        
        return QueryResponse(
            response=response_text,
            error=None,
            context_found=True,
            sources=source_info,
            session_id=session_id,
            confidence_score=float(confidence_score),
            sources_searched=sources_searched,
            expand_available=len(questions) > 1 if use_enhanced_rag else False,
            retrieval_method=retrieval_method
        )
        
    except Exception as e:
        logger.error(f"Error processing query: {e}")
        traceback.print_exc()
        return QueryResponse(
            response=None,
            error=str(e),
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[],
            retrieval_method="error"
        )

def extract_statutory_information(context_text: str, statute_citation: str) -> dict:
    """Extract specific information from statutory text"""
    extracted_info = {
        "requirements": [],
        "durations": [],
        "numbers": [],
        "procedures": []
    }
    
    # Look for duration patterns
    duration_patterns = [
        r"minimum of (\d+) (?:minutes?|hours?)",
        r"at least (\d+) (?:minutes?|hours?)",
        r"(\d+)-(?:minute|hour) (?:minimum|maximum)",
        r"(?:shall|must) be (\d+) (?:minutes?|hours?)"
    ]
    
    for pattern in duration_patterns:
        matches = re.findall(pattern, context_text, re.IGNORECASE)
        for match in matches:
            extracted_info["durations"].append(f"{match} minutes/hours")
    
    # Look for numerical requirements
    number_patterns = [
        r"(?:maximum|minimum) of (\d+) (?:participants?|people|individuals?)",
        r"at least (\d+) (?:speakers?|facilitators?|members?)",
        r"no more than (\d+) (?:participants?|attendees?)"
    ]
    
    for pattern in number_patterns:
        matches = re.findall(pattern, context_text, re.IGNORECASE)
        for match in matches:
            extracted_info["numbers"].append(match)
    
    return extracted_info



=== legal_assistant/services/__init__.py ===
"""Services package"""
from .document_processor import SafeDocumentProcessor
from .container_manager import UserContainerManager, get_container_manager, initialize_container_manager
from .rag_service import (
    enhanced_retrieval_v2,
    combined_search,
    load_database,
    remove_duplicate_documents,
    calculate_confidence_score
)
from .analysis_service import ComprehensiveAnalysisProcessor
from .ai_service import call_openrouter_api
from .external_db_service import (
    LegalDatabaseInterface,
    LexisNexisInterface,
    WestlawInterface,
    search_external_databases
)

__all__ = [
    'SafeDocumentProcessor',
    'UserContainerManager',
    'get_container_manager',
    'initialize_container_manager',
    'enhanced_retrieval_v2',
    'combined_search',
    'load_database',
    'remove_duplicate_documents',
    'calculate_confidence_score',
    'ComprehensiveAnalysisProcessor',
    'call_openrouter_api',
    'LegalDatabaseInterface',
    'LexisNexisInterface',
    'WestlawInterface',
    'search_external_databases'
]



=== legal_assistant/services/ai_service.py ===
"""AI/LLM integration service"""
import logging
import requests
from typing import Optional
from ..config import AI_MODELS, OPENROUTER_API_KEY, OPENAI_API_BASE

logger = logging.getLogger(__name__)

def call_openrouter_api(prompt: str, api_key: str = None, api_base: str = None) -> str:
    """Call OpenRouter API with fallback models"""
    if not api_key:
        api_key = OPENROUTER_API_KEY
    if not api_base:
        api_base = OPENAI_API_BASE
    
    if not api_key:
        return "I apologize, but AI features are not configured. Please set OPENAI_API_KEY environment variable."
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:8000",
        "X-Title": "Legal Assistant"
    }
    
    for model in AI_MODELS:
        try:
            # Create a fresh payload for each call - no state contamination
            payload = {
                "model": model,
                "messages": [
                    {
                        "role": "system", 
                        "content": "You are a legal assistant. Respond only to the current query."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "temperature": 0.5,
                "max_tokens": 2000
            }
            
            response = requests.post(api_base + "/chat/completions", headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                logger.info(f"‚úÖ Successfully used model: {model}")
                return result['choices'][0]['message']['content'].strip()
                
        except Exception as e:
            logger.error(f"Error with model {model}: {e}")
            continue
    
    return "I'm experiencing technical difficulties. Please try again later."



=== legal_assistant/services/analysis_service.py ===
"""Comprehensive analysis service"""
import time
import logging
from typing import Dict, List, Tuple, Optional

from ..models import ComprehensiveAnalysisRequest, StructuredAnalysisResponse, AnalysisType
from ..config import COMPREHENSIVE_SEARCH_K, OPENROUTER_API_KEY, OPENAI_API_BASE
from .container_manager import get_container_manager
from .ai_service import call_openrouter_api
from ..utils.formatting import format_context_for_llm

logger = logging.getLogger(__name__)

class ComprehensiveAnalysisProcessor:
    def __init__(self):
        self.analysis_prompts = {
            "document_summary": "Analyze this document and provide a comprehensive summary including document type, purpose, main parties, key terms, important dates, and financial obligations.",
            "key_clauses": "Extract and analyze key legal clauses including termination, indemnification, liability, governing law, confidentiality, payment terms, and dispute resolution. For each clause, provide specific text references and implications.",
            "risk_assessment": "Identify and assess legal risks including unilateral rights, broad indemnification, unlimited liability, vague obligations, and unfavorable terms. Rate each risk (High/Medium/Low) and suggest mitigation strategies.",
            "timeline_deadlines": "Extract all time-related information including start/end dates, payment deadlines, notice periods, renewal terms, performance deadlines, and warranty periods. Present chronologically.",
            "party_obligations": "List all obligations for each party including what must be done, deadlines, conditions, performance standards, and consequences of non-compliance. Organize by party.",
            "missing_clauses": "Identify commonly expected clauses that may be missing such as force majeure, limitation of liability, dispute resolution, severability, assignment restrictions, and notice provisions. Explain the importance and risks of each missing clause."
        }
    
    def process_comprehensive_analysis(self, request: ComprehensiveAnalysisRequest) -> StructuredAnalysisResponse:
        start_time = time.time()
        
        try:
            search_results, sources_searched, retrieval_method = self._enhanced_document_specific_search(
                request.user_id, 
                request.document_id, 
                "comprehensive legal document analysis",
                k=COMPREHENSIVE_SEARCH_K
            )
            
            if not search_results:
                return StructuredAnalysisResponse(
                    warnings=["No relevant documents found for analysis"],
                    processing_time=time.time() - start_time,
                    retrieval_method="no_documents_found"
                )
            
            context_text, source_info = format_context_for_llm(search_results, max_length=8000)
            
            response = StructuredAnalysisResponse()
            response.sources_by_section = {}
            response.confidence_scores = {}
            response.retrieval_method = retrieval_method
            
            if AnalysisType.COMPREHENSIVE in request.analysis_types:
                comprehensive_prompt = self._create_comprehensive_prompt(context_text)
                
                try:
                    analysis_result = call_openrouter_api(comprehensive_prompt, OPENROUTER_API_KEY, OPENAI_API_BASE)
                    parsed_sections = self._parse_comprehensive_response(analysis_result)
                    
                    response.document_summary = parsed_sections.get("summary", "")
                    response.key_clauses = parsed_sections.get("clauses", "")
                    response.risk_assessment = parsed_sections.get("risks", "")
                    response.timeline_deadlines = parsed_sections.get("timeline", "")
                    response.party_obligations = parsed_sections.get("obligations", "")
                    response.missing_clauses = parsed_sections.get("missing", "")
                    
                    response.overall_confidence = self._calculate_comprehensive_confidence(parsed_sections, len(search_results))
                    
                    for section in ["summary", "clauses", "risks", "timeline", "obligations", "missing"]:
                        response.sources_by_section[section] = source_info
                        response.confidence_scores[section] = response.overall_confidence
                    
                except Exception as e:
                    logger.error(f"Comprehensive analysis failed: {e}")
                    response.warnings.append(f"Comprehensive analysis failed: {str(e)}")
                    response.overall_confidence = 0.1
            
            else:
                for analysis_type in request.analysis_types:
                    section_result = self._process_individual_analysis(analysis_type, context_text, source_info)
                    
                    if analysis_type == AnalysisType.SUMMARY:
                        response.document_summary = section_result["content"]
                    elif analysis_type == AnalysisType.CLAUSES:
                        response.key_clauses = section_result["content"]
                    elif analysis_type == AnalysisType.RISKS:
                        response.risk_assessment = section_result["content"]
                    elif analysis_type == AnalysisType.TIMELINE:
                        response.timeline_deadlines = section_result["content"]
                    elif analysis_type == AnalysisType.OBLIGATIONS:
                        response.party_obligations = section_result["content"]
                    elif analysis_type == AnalysisType.MISSING_CLAUSES:
                        response.missing_clauses = section_result["content"]
                    
                    response.confidence_scores[analysis_type.value] = section_result["confidence"]
                    response.sources_by_section[analysis_type.value] = source_info
                
                confidences = list(response.confidence_scores.values())
                response.overall_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            response.processing_time = time.time() - start_time
            logger.info(f"Comprehensive analysis completed in {response.processing_time:.2f}s with confidence {response.overall_confidence:.2f}")
            
            return response
            
        except Exception as e:
            logger.error(f"Comprehensive analysis processing failed: {e}")
            return StructuredAnalysisResponse(
                warnings=[f"Analysis processing failed: {str(e)}"],
                processing_time=time.time() - start_time,
                overall_confidence=0.0,
                retrieval_method="error"
            )
    
    def _enhanced_document_specific_search(self, user_id: str, document_id: Optional[str], query: str, k: int = 15) -> Tuple[List, List[str], str]:
        all_results = []
        sources_searched = []
        retrieval_method = "enhanced_document_specific"
        
        try:
            container_manager = get_container_manager()
            if document_id:
                user_results = container_manager.enhanced_search_user_container(
                    user_id, query, "", k=k, document_id=document_id
                )
                sources_searched.append(f"document_{document_id}")
                logger.info(f"Document-specific search for {document_id}: {len(user_results)} results")
            else:
                user_results = container_manager.enhanced_search_user_container(
                    user_id, query, "", k=k
                )
                sources_searched.append("all_user_documents")
                logger.info(f"All documents search: {len(user_results)} results")
            
            for doc, score in user_results:
                doc.metadata['source_type'] = 'user_container'
                doc.metadata['search_scope'] = 'document_specific' if document_id else 'all_user_docs'
                all_results.append((doc, score))
            
            return all_results[:k], sources_searched, retrieval_method
            
        except Exception as e:
            logger.error(f"Error in document-specific search: {e}")
            return [], [], "error"
    
    def _create_comprehensive_prompt(self, context_text: str) -> str:
        """Create a comprehensive analysis prompt with strict anti-hallucination measures"""
        return f"""You are a legal document analyst providing structured analysis of legal documents.

CRITICAL INSTRUCTIONS - PREVENT HALLUCINATION:
1. **ONLY analyze what is EXPLICITLY written in the provided document**
2. **NEVER add information from general legal knowledge or assumptions**
3. **If information is not in the document, state: "Not specified in the document"**
4. **Copy the exact words from the document when making claims - use "..." for direct quotes**
5. **Each statement must be traceable to specific text in the document**

ANALYSIS FRAMEWORK:

## DOCUMENT SUMMARY
- Document type (contract, agreement, memo, etc.)
- Primary purpose and scope
- Parties involved (full legal names as stated)
- Effective date and term duration
- Governing law and jurisdiction
**Required: Copy the exact words from the document that identify each element**

## KEY CLAUSES ANALYSIS
Identify and copy the exact text of the following clauses if present:
- Termination provisions (notice period, conditions, penalties)
- Indemnification (who indemnifies whom, scope, exceptions)
- Limitation of liability (caps, exclusions, carve-outs)
- Confidentiality (duration, scope, exceptions)
- Payment terms (amounts, schedules, late fees)
- Intellectual property (ownership, licenses, restrictions)
- Dispute resolution (arbitration, mediation, court selection)
**For each clause: copy the exact words in quotes and provide section reference**

## RISK ASSESSMENT
Analyze only risks explicitly present in the document:
- **HIGH RISK**: Unlimited liability, one-sided indemnification, no termination rights
- **MEDIUM RISK**: Broad confidentiality, strict deadlines, significant penalties
- **LOW RISK**: Standard commercial terms, mutual obligations, clear exit rights
**Required: Copy the exact words from the document that create each risk**

## TIMELINE & DEADLINES
Extract all time-related provisions in chronological order:
- Contract commencement date
- Milestone deadlines
- Notice periods (termination, breach, cure)
- Renewal/expiration dates
- Performance deadlines
**Format: [Date/Deadline] - [Obligation] - "Copy exact words here" (Section X.X)**

## PARTY OBLIGATIONS
List all obligations by party as stated in the document:

**[Party A Name]**:
- Obligation 1: "Copy exact words from document" (Section reference)
- Obligation 2: "Copy exact words from document" (Section reference)

**[Party B Name]**:
- Obligation 1: "Copy exact words from document" (Section reference)
- Obligation 2: "Copy exact words from document" (Section reference)

## MISSING CLAUSES ANALYSIS
Identify standard legal provisions that are NOT present:
- Force majeure
- Severability
- Entire agreement
- Amendment procedures
- Assignment restrictions
- Warranty disclaimers
- Insurance requirements
**Only list if you've confirmed they're actually missing after checking the entire document**

## CRITICAL AMBIGUITIES
Identify any vague or undefined terms that could cause disputes:
- Undefined technical terms
- Ambiguous deadlines ("promptly", "reasonable time")
- Unclear scope definitions
- Missing calculation methods
**Copy the ambiguous language exactly as it appears in the document**

HALLUCINATION PREVENTION CHECKLIST:
Before making any claim, verify:
‚úì Is this explicitly written in the document?
‚úì Can I copy the exact words supporting this?
‚úì Am I adding any external legal knowledge?
‚úì Have I checked the entire document for this information?

DOCUMENT TEXT TO ANALYZE:
{context_text}

IMPORTANT REMINDERS:
- If you cannot find specific information, state: "Not specified in the provided document"
- Do not suggest what "should" be in the document based on legal standards
- Do not interpret or infer beyond what is explicitly written
- Every factual claim must include the exact words from the document
- If the document seems incomplete, note what sections are present vs missing
- When quoting: Copy the text exactly as it appears, word-for-word, inside quotation marks

BEGIN ANALYSIS:"""
    
    def _parse_comprehensive_response(self, response_text: str) -> Dict[str, str]:
        sections = {}
        section_markers = {
            "summary": ["## DOCUMENT SUMMARY", "# DOCUMENT SUMMARY"],
            "clauses": ["## KEY CLAUSES ANALYSIS", "# KEY CLAUSES ANALYSIS", "## KEY CLAUSES"],
            "risks": ["## RISK ASSESSMENT", "# RISK ASSESSMENT", "## RISKS"],
            "timeline": ["## TIMELINE & DEADLINES", "# TIMELINE & DEADLINES", "## TIMELINE"],
            "obligations": ["## PARTY OBLIGATIONS", "# PARTY OBLIGATIONS", "## OBLIGATIONS"],
            "missing": ["## MISSING CLAUSES ANALYSIS", "# MISSING CLAUSES ANALYSIS", "## MISSING CLAUSES"]
        }
        
        lines = response_text.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            line_strip = line.strip()
            
            section_found = None
            for section_key, markers in section_markers.items():
                if any(line_strip.startswith(marker) for marker in markers):
                    section_found = section_key
                    break
            
            if section_found:
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                
                current_section = section_found
                current_content = []
            else:
                if current_section:
                    current_content.append(line)
        
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        for section_key in section_markers.keys():
            if section_key not in sections or not sections[section_key]:
                sections[section_key] = f"No {section_key.replace('_', ' ').title()} information found in the analysis."
        
        return sections
    
    def _process_individual_analysis(self, analysis_type: AnalysisType, context_text: str, source_info: List[Dict]) -> Dict:
        try:
            prompt = self.analysis_prompts.get(analysis_type.value, "Analyze this legal document.")
            full_prompt = f"{prompt}\n\nLEGAL DOCUMENT CONTEXT:\n{context_text}\n\nPlease provide a detailed analysis based ONLY on the provided context."
            
            result = call_openrouter_api(full_prompt, OPENROUTER_API_KEY, OPENAI_API_BASE)
            
            return {
                "content": result,
                "confidence": 0.7,
                "sources": source_info
            }
        except Exception as e:
            logger.error(f"Individual analysis failed for {analysis_type}: {e}")
            return {
                "content": f"Analysis failed for {analysis_type.value}: {str(e)}",
                "confidence": 0.1,
                "sources": []
            }
    
    def _calculate_comprehensive_confidence(self, parsed_sections: Dict[str, str], num_sources: int) -> float:
        try:
            successful_sections = sum(1 for content in parsed_sections.values() 
                                    if content and not content.startswith("No ") and len(content) > 50)
            section_factor = successful_sections / len(parsed_sections)
            
            avg_length = sum(len(content) for content in parsed_sections.values()) / len(parsed_sections)
            length_factor = min(1.0, avg_length / 200)
            
            source_factor = min(1.0, num_sources / 5)
            
            confidence = (section_factor * 0.5 + length_factor * 0.3 + source_factor * 0.2)
            return max(0.1, min(1.0, confidence))
            
        except Exception as e:
            logger.error(f"Error calculating confidence: {e}")
            return 0.5



=== legal_assistant/services/container_manager.py ===
"""User container management service"""
import os
import hashlib
import logging
import re
import traceback
import asyncio
from typing import Optional, List, Tuple, Dict, Callable
from datetime import datetime

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

from ..config import USER_CONTAINERS_PATH, FAST_EMBEDDING_MODELS
from ..core.exceptions import ContainerError
from ..core.dependencies import get_embeddings, get_nlp
from ..utils.text_processing import remove_duplicate_documents

logger = logging.getLogger(__name__)

class UserContainerManager:
    """Manages user-specific document containers with powerful embeddings"""
    
    def __init__(self, base_path: str):
        self.base_path = base_path
        self.embeddings = None
        self._initialize_embeddings()
        logger.info(f"UserContainerManager initialized with base path: {base_path}")
    
    def _initialize_embeddings(self):
        """Initialize embeddings with the best available model"""
        # Try to use the global embeddings if available
        global_embeddings = get_embeddings()
        
        if global_embeddings:
            self.embeddings = global_embeddings
            logger.info(f"Using global embeddings model")
            return
        
        # TEMPORARY: Use faster embeddings for large document processing
        for model_name in FAST_EMBEDDING_MODELS:
            try:
                self.embeddings = HuggingFaceEmbeddings(model_name=model_name)
                logger.info(f"‚úÖ UserContainerManager using FAST embeddings: {model_name}")
                return
            except Exception as e:
                logger.warning(f"Failed to load {model_name}: {e}")
                continue
        
        # Last resort fallback
        try:
            self.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            logger.warning("‚ö†Ô∏è Using fallback embeddings: all-MiniLM-L6-v2")
        except Exception as e:
            logger.error(f"‚ùå Failed to load any embeddings model: {e}")
            self.embeddings = None
    
    def create_user_container(self, user_id: str) -> str:
        """Create a new container for a user"""
        container_id = hashlib.sha256(user_id.encode()).hexdigest()[:16]
        container_path = os.path.join(self.base_path, container_id)
        os.makedirs(container_path, exist_ok=True)
        
        # Ensure embeddings are available
        if not self.embeddings:
            self._initialize_embeddings()
        
        if not self.embeddings:
            raise ContainerError("No embeddings model available for container creation")
        
        user_db = Chroma(
            collection_name=f"user_{container_id}",
            embedding_function=self.embeddings,
            persist_directory=container_path
        )
        
        logger.info(f"Created container for user {user_id}: {container_id}")
        return container_id
    
    def get_container_id(self, user_id: str) -> str:
        """Get container ID for a user"""
        return hashlib.sha256(user_id.encode()).hexdigest()[:16]
    
    def get_user_database(self, user_id: str) -> Optional[Chroma]:
        """Get user's database"""
        container_id = self.get_container_id(user_id)
        container_path = os.path.join(self.base_path, container_id)
        
        if not os.path.exists(container_path):
            logger.warning(f"Container not found for user {user_id}")
            return None
        
        # Ensure embeddings are available
        if not self.embeddings:
            self._initialize_embeddings()
        
        if not self.embeddings:
            logger.error("No embeddings model available for database access")
            return None
        
        return Chroma(
            collection_name=f"user_{container_id}",
            embedding_function=self.embeddings,
            persist_directory=container_path
        )
    
    def get_user_database_safe(self, user_id: str) -> Optional[Chroma]:
        """Get user database with enhanced error handling and recovery"""
        try:
            container_id = self.get_container_id(user_id)
            container_path = os.path.join(self.base_path, container_id)
            
            if not os.path.exists(container_path):
                logger.warning(f"Container not found for user {user_id}, creating new one")
                self.create_user_container(user_id)
            
            # Ensure embeddings are available
            if not self.embeddings:
                self._initialize_embeddings()
            
            if not self.embeddings:
                logger.error("No embeddings model available for safe database access")
                return None
            
            return Chroma(
                collection_name=f"user_{container_id}",
                embedding_function=self.embeddings,
                persist_directory=container_path
            )
            
        except Exception as e:
            logger.error(f"Error getting user database for {user_id}: {e}")
            try:
                logger.info(f"Attempting to recover by creating new container for {user_id}")
                self.create_user_container(user_id)
                container_id = self.get_container_id(user_id)
                container_path = os.path.join(self.base_path, container_id)
                
                if not self.embeddings:
                    self._initialize_embeddings()
                
                if not self.embeddings:
                    logger.error("No embeddings model available for recovery")
                    return None
                
                return Chroma(
                    collection_name=f"user_{container_id}",
                    embedding_function=self.embeddings,
                    persist_directory=container_path
                )
            except Exception as recovery_error:
                logger.error(f"Recovery failed for user {user_id}: {recovery_error}")
                return None
    
    def add_document_to_container(self, user_id: str, document_text: str, metadata: Dict, file_id: str = None) -> bool:
        """Add document to user's container with intelligent chunking"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                container_id = self.create_user_container(user_id)
                user_db = self.get_user_database_safe(user_id)
            
            # Use intelligent chunking
            chunks_with_metadata = self.intelligent_chunking(document_text, metadata)
            
            logger.info(f"Created {len(chunks_with_metadata)} chunks using intelligent chunking")
            
            # Process in batches
            batch_size = 25
            total_batches = (len(chunks_with_metadata) + batch_size - 1) // batch_size
            
            for batch_num in range(total_batches):
                start_idx = batch_num * batch_size
                end_idx = min(start_idx + batch_size, len(chunks_with_metadata))
                batch_chunks = chunks_with_metadata[start_idx:end_idx]
                
                logger.info(f"Processing batch {batch_num + 1}/{total_batches} ({len(batch_chunks)} chunks)")
                
                documents = []
                for chunk_data in batch_chunks:
                    # Merge metadata
                    doc_metadata = chunk_data['metadata']
                    doc_metadata['user_id'] = user_id
                    doc_metadata['upload_timestamp'] = datetime.utcnow().isoformat()
                    doc_metadata['chunk_size'] = len(chunk_data['text'])
                    
                    if file_id:
                        doc_metadata['file_id'] = file_id
                    
                    # Generate chunk hash for deduplication
                    doc_metadata['content_hash'] = hashlib.md5(chunk_data['text'].encode()).hexdigest()[:16]
                    
                    # Clean metadata for ChromaDB
                    clean_metadata = {}
                    for key, value in doc_metadata.items():
                        if isinstance(value, (str, int, float, bool)):
                            clean_metadata[key] = value
                        elif isinstance(value, list):
                            clean_metadata[key] = str(value)
                        elif value is None:
                            clean_metadata[key] = ""
                        else:
                            clean_metadata[key] = str(value)
                    
                    documents.append(Document(
                        page_content=chunk_data['text'],
                        metadata=clean_metadata
                    ))
                
                # Add batch to ChromaDB
                try:
                    user_db.add_documents(documents)
                    logger.info(f"‚úÖ Added batch {batch_num + 1} ({len(documents)} chunks)")
                except Exception as batch_error:
                    logger.error(f"‚ùå Batch {batch_num + 1} failed: {batch_error}")
                    return False
            
            logger.info(f"‚úÖ Successfully added ALL {len(chunks_with_metadata)} chunks for document {file_id or 'unknown'}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error in add_document_to_container: {e}")
            logger.error(f"Error type: {type(e).__name__}")
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return False
    
    async def add_document_to_container_async(
        self, 
        user_id: str, 
        document_text: str, 
        metadata: Dict, 
        file_id: str = None,
        progress_callback: Optional[Callable[[int], None]] = None
    ) -> int:
        """Add document to container asynchronously with progress tracking"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                container_id = self.create_user_container(user_id)
                user_db = self.get_user_database_safe(user_id)
            
            # Use improved chunking
            chunks_with_metadata = self.intelligent_chunking_v2(document_text, metadata)
            
            logger.info(f"Created {len(chunks_with_metadata)} chunks using intelligent chunking v2")
            
            # Process in smaller batches for better performance
            batch_size = 10  # Smaller batches for faster processing
            total_batches = (len(chunks_with_metadata) + batch_size - 1) // batch_size
            chunks_added = 0
            
            for batch_num in range(total_batches):
                start_idx = batch_num * batch_size
                end_idx = min(start_idx + batch_size, len(chunks_with_metadata))
                batch_chunks = chunks_with_metadata[start_idx:end_idx]
                
                # Process batch
                documents = []
                for chunk_data in batch_chunks:
                    doc_metadata = chunk_data['metadata'].copy()
                    doc_metadata.update({
                        'user_id': user_id,
                        'file_id': file_id,
                        'chunk_index': chunk_data['metadata']['chunk_index'],
                        'total_chunks': len(chunks_with_metadata),
                        'upload_timestamp': datetime.utcnow().isoformat(),
                        'chunk_size': len(chunk_data['text'])
                    })
                    
                    # Generate chunk hash for deduplication
                    doc_metadata['content_hash'] = hashlib.md5(chunk_data['text'].encode()).hexdigest()[:16]
                    
                    # Clean metadata for ChromaDB
                    clean_metadata = {}
                    for key, value in doc_metadata.items():
                        if isinstance(value, (str, int, float, bool)):
                            clean_metadata[key] = value
                        elif isinstance(value, list):
                            clean_metadata[key] = str(value)
                        elif value is None:
                            clean_metadata[key] = ""
                        else:
                            clean_metadata[key] = str(value)
                    
                    documents.append(Document(
                        page_content=chunk_data['text'],
                        metadata=clean_metadata
                    ))
                
                # Add batch
                user_db.add_documents(documents)
                chunks_added += len(documents)
                
                # Update progress
                if progress_callback:
                    progress = int((chunks_added / len(chunks_with_metadata)) * 100)
                    progress_callback(progress)
                
                # Yield control to prevent blocking
                await asyncio.sleep(0.1)
            
            return chunks_added
            
        except Exception as e:
            logger.error(f"Error in async document addition: {e}")
            raise
    
    def intelligent_chunking(self, text: str, metadata: Dict, doc_type: str = "general") -> List[Dict]:
        """Intelligent chunking based on document structure with metadata preservation"""
        
        # Detect document type if not specified
        if doc_type == "general":
            doc_type = self._detect_document_type(text)
        
        logger.info(f"Using {doc_type} chunking strategy")
        
        if doc_type == "legal":
            return self._chunk_legal_document(text, metadata)
        elif doc_type == "legislative":
            return self._chunk_legislative_document(text, metadata)
        else:
            return self._chunk_general_document(text, metadata)
    
    def intelligent_chunking_v2(self, text: str, metadata: Dict) -> List[Dict]:
        """Improved chunking with better context preservation"""
        
        # Detect document type
        doc_type = self._detect_document_type(text)
        logger.info(f"Document type detected: {doc_type}")
        
        # For legislative documents, use bill-aware chunking
        if doc_type == "legislative":
            return self._chunk_legislative_smart(text, metadata)
        elif doc_type == "legal":
            return self._chunk_legal_smart(text, metadata)
        else:
            return self._chunk_with_sliding_window(text, metadata)
    
    def _detect_document_type(self, text: str) -> str:
        """Detect document type based on content patterns"""
        
        # Legislative patterns
        bill_count = len(re.findall(r'\b(?:HB|SB|SHB|SSB|ESHB|ESSB)\s+\d+', text))
        if bill_count > 1:
            return "legislative"
        
        # Legal document patterns
        legal_patterns = [
            r'\b(?:WHEREAS|NOW THEREFORE|AGREEMENT|CONTRACT|PARTY|PARTIES)\b',
            r'\b(?:Section|Article|Clause)\s+\d+',
            r'\b(?:shall|hereby|herein|thereof|whereof)\b'
        ]
        
        legal_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in legal_patterns)
        if legal_score > 10:
            return "legal"
        
        return "general"
    
    def _chunk_legislative_smart(self, text: str, base_metadata: Dict) -> List[Dict]:
        """Smart chunking for legislative documents that keeps bills together"""
        chunks = []
        
        # Find all bill boundaries
        bill_pattern = r'(?=(?:HB|SB|SHB|SSB|ESHB|ESSB)\s+\d+)'
        bill_positions = [match.start() for match in re.finditer(bill_pattern, text)]
        
        if not bill_positions:
            # No bills found, use regular chunking
            return self._chunk_with_sliding_window(text, base_metadata)
        
        # Add end position
        bill_positions.append(len(text))
        
        # Process each bill as a unit
        for i in range(len(bill_positions) - 1):
            start = bill_positions[i]
            end = bill_positions[i + 1]
            bill_text = text[start:end].strip()
            
            # Extract bill number
            bill_match = re.match(r'((?:HB|SB|SHB|SSB|ESHB|ESSB)\s+\d+)', bill_text)
            bill_number = bill_match.group(1) if bill_match else f"Section_{i}"
            
            # If bill is too large, chunk it intelligently
            if len(bill_text) > 2000:
                # Find natural break points within the bill
                sub_chunks = self._chunk_bill_intelligently(bill_text, bill_number, base_metadata)
                chunks.extend(sub_chunks)
            else:
                # Keep entire bill as one chunk
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    'chunk_type': 'legislative_bill',
                    'bill_number': bill_number,
                    'contains_bills': bill_number,
                    'chunk_index': len(chunks),
                    'is_complete_bill': True
                })
                
                chunks.append({
                    'text': bill_text,
                    'metadata': chunk_metadata
                })
        
        return chunks
    
    def _chunk_bill_intelligently(self, bill_text: str, bill_number: str, base_metadata: Dict) -> List[Dict]:
        """Intelligently chunk a single bill while preserving context"""
        chunks = []
        
        # Look for section breaks within the bill
        section_pattern = r'\n\s*(?:Section|SECTION|Sec\.)\s+\d+'
        sections = re.split(section_pattern, bill_text)
        
        # Always include bill header in each chunk
        bill_header = bill_text[:200] if len(bill_text) > 200 else bill_text[:50]
        if '\n' in bill_header:
            bill_header = bill_header[:bill_header.find('\n')]
        
        current_chunk = f"[Bill {bill_number}]\n{bill_header}\n...\n\n"
        current_size = len(current_chunk)
        chunk_parts = []
        
        for section in sections:
            section_size = len(section)
            
            if current_size + section_size > 1500 and chunk_parts:
                # Save current chunk
                chunk_text = current_chunk + '\n\n'.join(chunk_parts)
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    'chunk_type': 'legislative_bill_part',
                    'bill_number': bill_number,
                    'contains_bills': bill_number,
                    'chunk_index': len(chunks),
                    'part_number': len(chunks) + 1
                })
                
                chunks.append({
                    'text': chunk_text,
                    'metadata': chunk_metadata
                })
                
                # Start new chunk with bill reference
                chunk_parts = [section]
                current_size = len(current_chunk) + section_size
            else:
                chunk_parts.append(section)
                current_size += section_size
        
        # Add remaining parts
        if chunk_parts:
            chunk_text = current_chunk + '\n\n'.join(chunk_parts)
            chunk_metadata = base_metadata.copy()
            chunk_metadata.update({
                'chunk_type': 'legislative_bill_part',
                'bill_number': bill_number,
                'contains_bills': bill_number,
                'chunk_index': len(chunks),
                'part_number': len(chunks) + 1,
                'is_final_part': True
            })
            
            chunks.append({
                'text': chunk_text,
                'metadata': chunk_metadata
            })
        
        return chunks
    
    def _chunk_legal_smart(self, text: str, base_metadata: Dict) -> List[Dict]:
        """Smart chunking for legal documents"""
        # Use your existing _chunk_legal_document method
        return self._chunk_legal_document(text, base_metadata)
    
    def _chunk_with_sliding_window(self, text: str, base_metadata: Dict, 
                                  chunk_size: int = 1000, overlap: int = 200) -> List[Dict]:
        """Improved sliding window with sentence boundaries"""
        chunks = []
        
        # Split into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        if not sentences:
            return [{
                'text': text,
                'metadata': base_metadata.copy()
            }]
        
        current_chunk = []
        current_size = 0
        
        for i, sentence in enumerate(sentences):
            sentence_size = len(sentence)
            
            # If adding this sentence exceeds chunk size
            if current_size + sentence_size > chunk_size and current_chunk:
                # Create chunk
                chunk_text = ' '.join(current_chunk)
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    'chunk_type': 'sliding_window',
                    'chunk_index': len(chunks),
                    'sentence_start': i - len(current_chunk),
                    'sentence_end': i
                })
                
                chunks.append({
                    'text': chunk_text,
                    'metadata': chunk_metadata
                })
                
                # Calculate overlap sentences
                overlap_size = 0
                overlap_sentences = []
                
                for sent in reversed(current_chunk):
                    if overlap_size + len(sent) <= overlap:
                        overlap_sentences.insert(0, sent)
                        overlap_size += len(sent)
                    else:
                        break
                
                # Start new chunk with overlap
                current_chunk = overlap_sentences + [sentence]
                current_size = overlap_size + sentence_size
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
        
        # Add remaining sentences
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            chunk_metadata = base_metadata.copy()
            chunk_metadata.update({
                'chunk_type': 'sliding_window',
                'chunk_index': len(chunks),
                'is_final_chunk': True
            })
            
            chunks.append({
                'text': chunk_text,
                'metadata': chunk_metadata
            })
        
        return chunks
    
    def _chunk_legal_document(self, text: str, base_metadata: Dict) -> List[Dict]:
        """Chunk legal documents preserving structure"""
        chunks = []
        
        # Legal section patterns
        section_patterns = [
            (r'\n\s*(?:ARTICLE|Article)\s+([IVX\d]+)[^\n]*', 'article'),
            (r'\n\s*(?:SECTION|Section)\s+(\d+(?:\.\d+)?)[^\n]*', 'section'),
            (r'\n\s*(\d+)\.\s+[A-Z][^\n]*', 'numbered_section'),
            (r'\n\s*\(([a-z])\)\s*', 'subsection'),
        ]
        
        # Find all section boundaries
        boundaries = []
        for pattern, section_type in section_patterns:
            for match in re.finditer(pattern, text):
                boundaries.append({
                    'pos': match.start(),
                    'type': section_type,
                    'label': match.group(1),
                    'full_match': match.group(0)
                })
        
        # Sort boundaries by position
        boundaries.sort(key=lambda x: x['pos'])
        
        # Create chunks based on boundaries
        current_pos = 0
        current_section_hierarchy = []
        
        for i, boundary in enumerate(boundaries):
            # Extract text before this boundary
            chunk_text = text[current_pos:boundary['pos']].strip()
            
            if chunk_text and len(chunk_text) > 50:  # Minimum chunk size
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    'chunk_type': 'legal_section',
                    'section_hierarchy': '/'.join(current_section_hierarchy),
                    'section_type': boundaries[i-1]['type'] if i > 0 else 'preamble',
                    'section_label': boundaries[i-1]['label'] if i > 0 else 'start',
                })
                
                # Split if too large
                if len(chunk_text) > 2000:
                    sub_chunks = self._split_large_section(chunk_text, chunk_metadata)
                    chunks.extend(sub_chunks)
                else:
                    chunks.append({
                        'text': chunk_text,
                        'metadata': chunk_metadata
                    })
            
            # Update hierarchy
            if boundary['type'] in ['article', 'section']:
                current_section_hierarchy = [f"{boundary['type']}_{boundary['label']}"]
            elif boundary['type'] == 'subsection':
                if current_section_hierarchy:
                    current_section_hierarchy.append(f"subsection_{boundary['label']}")
            
            current_pos = boundary['pos']
        
        # Don't forget the last chunk
        if current_pos < len(text):
            chunk_text = text[current_pos:].strip()
            if chunk_text:
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    'chunk_type': 'legal_section',
                    'section_hierarchy': '/'.join(current_section_hierarchy),
                    'section_type': 'final',
                })
                chunks.append({
                    'text': chunk_text,
                    'metadata': chunk_metadata
                })
        
        return self._add_chunk_relationships(chunks)
    
    def _chunk_legislative_document(self, text: str, base_metadata: Dict) -> List[Dict]:
        """Chunk legislative documents preserving bill structure"""
        chunks = []
        
        # Find all bills
        bill_pattern = r'\b((?:HB|SB|SHB|SSB|ESHB|ESSB)\s+\d+)[^\n]*\n'
        bill_matches = list(re.finditer(bill_pattern, text))
        
        if not bill_matches:
            # No clear bill structure, fall back to general chunking
            return self._chunk_general_document(text, base_metadata)
        
        # Chunk by bills
        for i, match in enumerate(bill_matches):
            start_pos = match.start()
            end_pos = bill_matches[i + 1].start() if i + 1 < len(bill_matches) else len(text)
            
            bill_text = text[start_pos:end_pos].strip()
            bill_number = match.group(1)
            
            # Extract bill metadata
            bill_metadata = base_metadata.copy()
            bill_metadata.update({
                'chunk_type': 'legislative_bill',
                'bill_number': bill_number,
                'contains_bills': bill_number,
            })
            
            # Extract additional bill info
            sponsor_match = re.search(r'Sponsors?\s*:\s*([^\n]+)', bill_text)
            if sponsor_match:
                bill_metadata['bill_sponsors'] = sponsor_match.group(1).strip()
            
            status_match = re.search(r'Final Status\s*:\s*([^\n]+)', bill_text)
            if status_match:
                bill_metadata['bill_status'] = status_match.group(1).strip()
            
            # Split if too large
            if len(bill_text) > 2000:
                sub_chunks = self._split_large_section(bill_text, bill_metadata)
                chunks.extend(sub_chunks)
            else:
                chunks.append({
                    'text': bill_text,
                    'metadata': bill_metadata
                })
        
        return self._add_chunk_relationships(chunks)
    
    def _chunk_general_document(self, text: str, base_metadata: Dict) -> List[Dict]:
        """Chunk general documents with sliding window and sentence boundaries"""
        chunks = []
        
        # First try paragraph-based chunking
        paragraphs = text.split('\n\n')
        
        if len(paragraphs) > 1:
            current_chunk = []
            current_size = 0
            
            for para in paragraphs:
                para_size = len(para)
                
                if current_size + para_size > 1000 and current_chunk:
                    # Save current chunk
                    chunk_text = '\n\n'.join(current_chunk)
                    chunk_metadata = base_metadata.copy()
                    chunk_metadata['chunk_type'] = 'paragraph_based'
                    
                    chunks.append({
                        'text': chunk_text,
                        'metadata': chunk_metadata
                    })
                    
                    # Start new chunk with overlap
                    if len(current_chunk) > 1:
                        current_chunk = [current_chunk[-1], para]
                        current_size = len(current_chunk[-1]) + para_size
                    else:
                        current_chunk = [para]
                        current_size = para_size
                else:
                    current_chunk.append(para)
                    current_size += para_size
            
            # Add remaining
            if current_chunk:
                chunk_text = '\n\n'.join(current_chunk)
                chunk_metadata = base_metadata.copy()
                chunk_metadata['chunk_type'] = 'paragraph_based'
                chunks.append({
                    'text': chunk_text,
                    'metadata': chunk_metadata
                })
        else:
            # Fall back to sliding window
            chunks = self._sliding_window_chunk(text, base_metadata, 1000, 200)
        
        return self._add_chunk_relationships(chunks)
    
    def _sliding_window_chunk(self, text: str, base_metadata: Dict, chunk_size: int, overlap: int) -> List[Dict]:
        """Sliding window chunking with sentence boundaries"""
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > chunk_size and current_chunk:
                # Save current chunk
                chunk_text = ' '.join(current_chunk)
                chunk_metadata = base_metadata.copy()
                chunk_metadata['chunk_type'] = 'sliding_window'
                
                chunks.append({
                    'text': chunk_text,
                    'metadata': chunk_metadata
                })
                
                # Calculate overlap
                overlap_sentences = []
                overlap_size = 0
                
                for s in reversed(current_chunk):
                    if overlap_size + len(s) <= overlap:
                        overlap_sentences.insert(0, s)
                        overlap_size += len(s)
                    else:
                        break
                
                current_chunk = overlap_sentences + [sentence]
                current_size = overlap_size + sentence_size
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
        
        # Add remaining
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            chunk_metadata = base_metadata.copy()
            chunk_metadata['chunk_type'] = 'sliding_window'
            chunks.append({
                'text': chunk_text,
                'metadata': chunk_metadata
            })
        
        return chunks
    
    def _split_large_section(self, text: str, metadata: Dict) -> List[Dict]:
        """Split large sections while preserving context"""
        return self._sliding_window_chunk(text, metadata, 1000, 300)
    
    def _add_chunk_relationships(self, chunks: List[Dict]) -> List[Dict]:
        """Add relationship metadata between chunks"""
        for i, chunk in enumerate(chunks):
            chunk['metadata']['chunk_index'] = i
            chunk['metadata']['total_chunks'] = len(chunks)
            
            if i > 0:
                chunk['metadata']['previous_chunk_index'] = i - 1
                # Add context from previous chunk
                prev_text = chunks[i-1]['text']
                chunk['metadata']['previous_context'] = prev_text[-200:] if len(prev_text) > 200 else prev_text
            
            if i < len(chunks) - 1:
                chunk['metadata']['next_chunk_index'] = i + 1
                # Add context from next chunk
                next_text = chunks[i+1]['text']
                chunk['metadata']['next_context'] = next_text[:200] if len(next_text) > 200 else next_text
        
        return chunks
    
    def hybrid_search_user_container(self, user_id: str, query: str, k: int = 10, document_id: str = None) -> List[Tuple]:
        """Perform hybrid search combining keyword and semantic search"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                logger.warning(f"No database available for user {user_id}")
                return []
            
            # Import hybrid searcher
            from .hybrid_search import get_hybrid_searcher
            searcher = get_hybrid_searcher()
            
            # Prepare filter
            filter_dict = None
            if document_id:
                filter_dict = {"file_id": document_id}
            
            # Perform hybrid search
            results = searcher.hybrid_search(
                query=query,
                vector_store=user_db,
                k=k,
                keyword_weight=0.3,
                semantic_weight=0.7,
                rerank=True,
                filter_dict=filter_dict
            )
            
            logger.info(f"Hybrid search returned {len(results)} results for query: '{query}'")
            
            # Log top results for debugging
            for i, (doc, score) in enumerate(results[:3]):
                logger.debug(f"Result {i+1}: Score={score:.3f}, Content preview: {doc.page_content[:100]}...")
            
            return results
            
        except Exception as e:
            logger.error(f"Hybrid search failed, falling back to semantic search: {e}")
            # Fall back to enhanced semantic search
            return self.enhanced_search_user_container(user_id, query, "", k, document_id)
    
    def search_user_container(self, user_id: str, query: str, k: int = 5, document_id: str = None) -> List[Tuple]:
        """Search with timeout protection"""
        return self.search_user_container_safe(user_id, query, k, document_id)
    
    def search_user_container_safe(self, user_id: str, query: str, k: int = 5, document_id: str = None) -> List[Tuple]:
        """Search with enhanced error handling and timeout protection"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                logger.warning(f"No database available for user {user_id}")
                return []
            
            filter_dict = None
            if document_id:
                filter_dict = {"file_id": document_id}
            
            try:
                results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                return results
            except Exception as search_error:
                logger.warning(f"Search failed for user {user_id}: {search_error}")
                return []
                
        except Exception as e:
            logger.error(f"Error in safe container search for user {user_id}: {e}")
            return []
    
    def enhanced_search_user_container(self, user_id: str, query: str, conversation_context: str, k: int = 12, document_id: str = None) -> List[Tuple]:
        """Enhanced search with timeout protection and bill-specific optimization"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                return []
            
            filter_dict = None
            if document_id:
                filter_dict = {"file_id": document_id}
            
            try:
                # Check if this is a bill-specific query
                bill_match = re.search(r"\b(HB|SB|SSB|ESSB|SHB|ESHB)\s+(\d+)\b", query, re.IGNORECASE)
                
                if bill_match:
                    bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
                    logger.info(f"Bill-specific search for: {bill_number}")
                    
                    # First, try to find chunks that contain this specific bill
                    try:
                        all_docs = user_db.get()
                        bill_specific_chunks = []
                        
                        for i, (doc_id, metadata, content) in enumerate(zip(all_docs['ids'], all_docs['metadatas'], all_docs['documents'])):
                            if metadata and 'contains_bills' in metadata:
                                if bill_number in metadata['contains_bills']:
                                    # Create a document object for this chunk
                                    doc_obj = Document(page_content=content, metadata=metadata)
                                    # Use a high relevance score since we found exact bill match
                                    bill_specific_chunks.append((doc_obj, 0.95))  # High relevance for exact matches
                                    logger.info(f"Found {bill_number} in chunk {metadata.get('chunk_index')} with boosted score")
                        
                        if bill_specific_chunks:
                            logger.info(f"Using {len(bill_specific_chunks)} bill-specific chunks with high relevance")
                            # Get additional context chunks with lower threshold
                            regular_results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                            
                            # Combine bill-specific (high score) with regular results
                            all_results = bill_specific_chunks + regular_results
                            return remove_duplicate_documents(all_results)[:k]
                    except Exception as bill_search_error:
                        logger.warning(f"Bill-specific search failed, falling back to regular search: {bill_search_error}")
                        # Fall through to regular search
                
                # Fallback to regular search
                direct_results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                expanded_query = f"{query} {conversation_context}"
                expanded_results = user_db.similarity_search_with_score(expanded_query, k=k, filter=filter_dict)
                
                sub_query_results = []
                nlp = get_nlp()
                if nlp:
                    doc = nlp(query)
                    for ent in doc.ents:
                        if ent.label_ in ["ORG", "PERSON", "LAW", "DATE"]:
                            sub_results = user_db.similarity_search_with_score(f"What is {ent.text}?", k=3, filter=filter_dict)
                            sub_query_results.extend(sub_results)
                
                all_results = direct_results + expanded_results + sub_query_results
                return remove_duplicate_documents(all_results)[:k]
                
            except Exception as search_error:
                logger.warning(f"Enhanced search failed for user {user_id}: {search_error}")
                return []
            
        except Exception as e:
            logger.error(f"Error in enhanced user container search: {e}")
            return []

# Global instance
_container_manager = None

def initialize_container_manager():
    """Initialize the global container manager"""
    global _container_manager
    _container_manager = UserContainerManager(USER_CONTAINERS_PATH)
    return _container_manager

def get_container_manager() -> UserContainerManager:
    """Get the global container manager instance"""
    if _container_manager is None:
        return initialize_container_manager()
    return _container_manager



=== legal_assistant/services/document_processor.py ===
"""Enhanced document processing service with OCR support"""
import os
import io
import logging
import tempfile
from typing import Tuple, List, Dict
import hashlib
from ..config import FeatureFlags
from ..core.exceptions import DocumentProcessingError

logger = logging.getLogger(__name__)

class SafeDocumentProcessor:
    """Safe document processor for various file types with enhanced processing capabilities"""
    
    @staticmethod
    def process_document_safe(file) -> Tuple[str, int, List[str]]:
        """
        Process uploaded document safely with enhanced processing and OCR fallback
        Returns: (content, pages_processed, warnings)
        """
        warnings = []
        content = ""
        pages_processed = 0
        
        try:
            filename = getattr(file, 'filename', 'unknown')
            file_ext = os.path.splitext(filename)[1].lower()
            file_content = file.file.read()
            
            logger.info(f"Processing file: {filename}, extension: {file_ext}, size: {len(file_content)} bytes")
            
            if file_ext == '.txt':
                content = file_content.decode('utf-8', errors='ignore')
                pages_processed = SafeDocumentProcessor._estimate_pages_from_text(content)
                logger.info(f"Text file processed: {len(content)} chars, {pages_processed} pages")
            elif file_ext == '.pdf':
                # Enhanced PDF processing with multiple fallbacks
                content, pages_processed = SafeDocumentProcessor._process_pdf_multi_method(file_content, warnings)
                logger.info(f"PDF processed: {len(content)} chars, {pages_processed} pages")
            elif file_ext == '.docx':
                content, pages_processed = SafeDocumentProcessor._process_docx_enhanced(file_content, warnings)
                logger.info(f"DOCX processed: {len(content)} chars, {pages_processed} pages")
            else:
                try:
                    content = file_content.decode('utf-8', errors='ignore')
                    pages_processed = SafeDocumentProcessor._estimate_pages_from_text(content)
                    warnings.append(f"File type {file_ext} processed as plain text")
                except Exception as e:
                    warnings.append(f"Could not process file: {str(e)}")
                    content = "Unable to process this file type"
                    pages_processed = 0
            
            # Log extraction quality
            logger.info(f"Content preview: {content[:200]}..." if len(content) > 200 else f"Content: {content}")
            
            # Validate extraction quality
            if not SafeDocumentProcessor._validate_extraction(content, filename):
                warnings.append("Low quality extraction detected - may need manual review")
                logger.warning(f"Low quality extraction for {filename}")
            
            file.file.seek(0)
            
        except Exception as e:
            logger.error(f"Error processing document: {str(e)}", exc_info=True)
            warnings.append(f"Error processing document: {str(e)}")
            content = "Error processing document"
            pages_processed = 0
        
        logger.info(f"Final result: {len(content)} chars, {pages_processed} pages, {len(warnings)} warnings")
        return content, pages_processed, warnings
    
    @staticmethod
    def quick_validate(file_content: bytes, file_ext: str) -> Tuple[str, int, List[str]]:
        """Quick validation to check if document can be processed"""
        warnings = []
        
        try:
            logger.info(f"Quick validating {file_ext} file, size: {len(file_content)} bytes")
            
            if file_ext == '.txt':
                content = file_content.decode('utf-8', errors='ignore')
                pages_estimated = max(1, len(content) // 2000)  # Quick estimation
                return content[:1000], pages_estimated, warnings
                
            elif file_ext == '.pdf':
                # Just try to extract first page
                if FeatureFlags.PYMUPDF_AVAILABLE:
                    try:
                        import fitz
                        doc = fitz.open(stream=file_content, filetype="pdf")
                        total_pages = len(doc)
                        
                        if total_pages > 0:
                            first_page = doc.load_page(0)
                            content = first_page.get_text()
                            doc.close()
                            
                            if not content.strip():
                                warnings.append("First page appears to be empty - may need OCR")
                                
                            return content[:1000], total_pages, warnings
                        else:
                            warnings.append("PDF has no pages")
                            return "", 0, warnings
                            
                    except Exception as e:
                        warnings.append(f"PDF validation failed: {str(e)}")
                        return "", 0, warnings
                else:
                    warnings.append("PDF processing not available")
                    return "", 0, warnings
                    
            elif file_ext == '.docx':
                # Try basic extraction
                try:
                    from docx import Document
                    doc = Document(io.BytesIO(file_content))
                    paragraphs = []
                    
                    # Check first few paragraphs
                    for i, para in enumerate(doc.paragraphs):
                        if i > 5:  # Just check first few paragraphs
                            break
                        if para.text.strip():
                            paragraphs.append(para.text)
                    
                    content = '\n'.join(paragraphs)
                    pages_estimated = max(1, len(content) // 2000)
                    return content[:1000], pages_estimated, warnings
                    
                except ImportError:
                    warnings.append("python-docx not available")
                    return "", 0, warnings
                except Exception as e:
                    warnings.append(f"DOCX validation failed: {str(e)}")
                    return "", 0, warnings
            else:
                # Try as plain text
                try:
                    content = file_content.decode('utf-8', errors='ignore')
                    pages_estimated = max(1, len(content) // 2000)
                    warnings.append(f"Unknown file type {file_ext} - treating as plain text")
                    return content[:1000], pages_estimated, warnings
                except Exception as e:
                    warnings.append(f"Cannot process file type {file_ext}: {str(e)}")
                    return "", 0, warnings
                    
        except Exception as e:
            logger.error(f"Quick validation error: {e}")
            warnings.append(f"Validation error: {str(e)}")
            return "", 0, warnings
    
    @staticmethod
    def process_document_from_bytes(file_content: bytes, filename: str, file_ext: str) -> Tuple[str, int, List[str]]:
        """Process document from bytes (for background processing)"""
        warnings = []
        
        try:
            logger.info(f"Processing document from bytes: {filename}, extension: {file_ext}, size: {len(file_content)} bytes")
            
            if file_ext == '.txt':
                content = file_content.decode('utf-8', errors='ignore')
                pages_processed = SafeDocumentProcessor._estimate_pages_from_text(content)
                logger.info(f"Text processed: {len(content)} chars, {pages_processed} pages")
                
            elif file_ext == '.pdf':
                content, pages_processed = SafeDocumentProcessor._process_pdf_multi_method(file_content, warnings)
                logger.info(f"PDF processed: {len(content)} chars, {pages_processed} pages")
                
            elif file_ext == '.docx':
                content, pages_processed = SafeDocumentProcessor._process_docx_enhanced(file_content, warnings)
                logger.info(f"DOCX processed: {len(content)} chars, {pages_processed} pages")
                
            else:
                # Handle unknown file types
                try:
                    content = file_content.decode('utf-8', errors='ignore')
                    pages_processed = SafeDocumentProcessor._estimate_pages_from_text(content)
                    warnings.append(f"File type {file_ext} processed as plain text")
                    logger.info(f"Unknown type processed as text: {len(content)} chars, {pages_processed} pages")
                except Exception as e:
                    warnings.append(f"Could not process file: {str(e)}")
                    content = "Unable to process this file type"
                    pages_processed = 0
                    logger.error(f"Failed to process unknown file type: {e}")
            
            # Validate extraction quality
            if content and not SafeDocumentProcessor._validate_extraction(content, filename):
                warnings.append("Low quality extraction detected - may need manual review")
                logger.warning(f"Low quality extraction for {filename}")
            
            logger.info(f"Background processing complete: {len(content)} chars, {pages_processed} pages, {len(warnings)} warnings")
            return content, pages_processed, warnings
            
        except Exception as e:
            logger.error(f"Error processing document from bytes: {e}", exc_info=True)
            return "", 0, [f"Processing failed: {str(e)}"]
    
    @staticmethod
    def get_processing_capabilities() -> Dict[str, bool]:
        """Return current processing capabilities for different methods"""
        return {
            'unstructured_available': FeatureFlags.UNSTRUCTURED_AVAILABLE,
            'pymupdf_available': FeatureFlags.PYMUPDF_AVAILABLE,
            'pdfplumber_available': FeatureFlags.PDFPLUMBER_AVAILABLE,
            'ocr_available': FeatureFlags.OCR_AVAILABLE,
            'docx_available': True,  # Usually available
            'supported_formats': ['.txt', '.pdf', '.docx']
        }
    
    @staticmethod
    def _validate_extraction(text: str, filename: str) -> bool:
        """Validate if extraction was successful"""
        if len(text) < 100:
            logger.warning(f"Extraction too short for {filename}: {len(text)} chars")
            return False
        
        # Check for common extraction failures
        if text.count('ÔøΩ') > 10:  # Unicode errors
            logger.warning(f"Many unicode errors in {filename}")
            return False
        
        # Check if mostly whitespace
        if len(text.strip()) < len(text) * 0.1:
            logger.warning(f"Mostly whitespace in {filename}")
            return False
        
        return True
    
    @staticmethod
    def _process_pdf_multi_method(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process PDF with multiple methods and OCR fallback"""
        
        # Method 1: Try Unstructured.io first (best quality)
        if FeatureFlags.UNSTRUCTURED_AVAILABLE:
            try:
                content, pages = SafeDocumentProcessor._process_pdf_unstructured(file_content, warnings)
                if content and len(content) > 100:
                    logger.info("‚úÖ PDF processed with Unstructured.io")
                    return content, pages
            except Exception as e:
                warnings.append(f"Unstructured.io failed: {str(e)}")
        
        # Method 2: Try PyMuPDF with layout preservation
        if FeatureFlags.PYMUPDF_AVAILABLE:
            try:
                content, pages = SafeDocumentProcessor._process_pdf_pymupdf_enhanced(file_content, warnings)
                if content and len(content) > 100:
                    logger.info("‚úÖ PDF processed with PyMuPDF")
                    return content, pages
            except Exception as e:
                warnings.append(f"PyMuPDF failed: {str(e)}")
        
        # Method 3: Try pdfplumber
        if FeatureFlags.PDFPLUMBER_AVAILABLE:
            try:
                content, pages = SafeDocumentProcessor._process_pdf_pdfplumber(file_content, warnings)
                if content and len(content) > 100:
                    logger.info("‚úÖ PDF processed with pdfplumber")
                    return content, pages
            except Exception as e:
                warnings.append(f"pdfplumber failed: {str(e)}")
        
        # Method 4: OCR fallback for scanned PDFs
        if FeatureFlags.OCR_AVAILABLE:
            try:
                content, pages = SafeDocumentProcessor._process_pdf_with_ocr(file_content, warnings)
                if content and len(content) > 100:
                    logger.info("‚úÖ PDF processed with OCR")
                    return content, pages
            except Exception as e:
                warnings.append(f"OCR failed: {str(e)}")
        
        warnings.append("All PDF processing methods failed")
        return "PDF processing failed - please try a different format", 0
    
    @staticmethod
    def _process_pdf_pymupdf_enhanced(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Enhanced PyMuPDF processing with better text extraction"""
        try:
            import fitz  # PyMuPDF
            
            doc = fitz.open(stream=file_content, filetype="pdf")
            all_text = []
            page_count = len(doc)  # Get actual page count
            
            for page_num in range(page_count):
                page = doc.load_page(page_num)
                
                # Try different extraction methods
                # Method 1: Get text with layout
                text = page.get_text("dict")
                page_text = SafeDocumentProcessor._reconstruct_layout(text)
                
                # If text is too short, might be scanned or have issues
                if len(page_text.strip()) < 50:
                    # Method 2: Simple text extraction
                    page_text = page.get_text()
                    
                    # If still too short, flag for OCR
                    if len(page_text.strip()) < 50:
                        warnings.append(f"Page {page_num + 1} has little text - may need OCR")
                
                all_text.append(f"\n--- Page {page_num + 1} ---\n{page_text}")
            
            doc.close()
            
            # Join all text
            final_text = "\n".join(all_text)
            return final_text, page_count
            
        except Exception as e:
            warnings.append(f"PyMuPDF enhanced extraction failed: {str(e)}")
            raise
    
    @staticmethod
    def _reconstruct_layout(text_dict):
        """Reconstruct text layout from PyMuPDF dict output"""
        blocks = []
        
        for block in sorted(text_dict.get("blocks", []), key=lambda b: (b.get("bbox", [0])[1], b.get("bbox", [0])[0])):
            if block.get("type") == 0:  # Text block
                block_lines = []
                
                for line in block.get("lines", []):
                    line_text = []
                    for span in line.get("spans", []):
                        span_text = span.get("text", "")
                        if span_text.strip():
                            line_text.append(span_text)
                    
                    if line_text:
                        block_lines.append(" ".join(line_text))
                
                if block_lines:
                    blocks.append("\n".join(block_lines))
        
        return "\n\n".join(blocks)
    
    @staticmethod
    def _process_pdf_with_ocr(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process PDF using OCR for scanned documents"""
        try:
            import pytesseract
            from pdf2image import convert_from_bytes
            from PIL import Image
            
            # Convert PDF to images
            images = convert_from_bytes(file_content, dpi=300)
            all_text = []
            
            for i, image in enumerate(images):
                try:
                    # Preprocess image for better OCR
                    image = SafeDocumentProcessor._preprocess_image_for_ocr(image)
                    
                    # Perform OCR
                    text = pytesseract.image_to_string(image, lang='eng', config='--psm 6')
                    
                    if text.strip():
                        all_text.append(f"\n--- Page {i + 1} (OCR) ---\n{text}")
                    else:
                        warnings.append(f"Page {i + 1}: No text found with OCR")
                        
                except Exception as e:
                    warnings.append(f"OCR failed for page {i + 1}: {str(e)}")
            
            if not all_text:
                raise ValueError("No text extracted with OCR")
                
            return "\n".join(all_text), len(images)
            
        except ImportError:
            warnings.append("OCR libraries not installed. Install pytesseract and pdf2image")
            raise
        except Exception as e:
            warnings.append(f"OCR processing failed: {str(e)}")
            raise
    
    @staticmethod
    def _preprocess_image_for_ocr(image):
        """Preprocess image to improve OCR accuracy"""
        try:
            from PIL import ImageEnhance, ImageFilter
            
            # Convert to grayscale
            image = image.convert('L')
            
            # Enhance contrast
            enhancer = ImageEnhance.Contrast(image)
            image = enhancer.enhance(2.0)
            
            # Remove noise
            image = image.filter(ImageFilter.MedianFilter(size=3))
            
            return image
        except Exception:
            return image  # Return original if preprocessing fails
    
    @staticmethod
    def _process_pdf_pdfplumber(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process PDF using pdfplumber"""
        try:
            import pdfplumber
            
            with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                all_text = []
                
                for i, page in enumerate(pdf.pages):
                    text = page.extract_text() or ""
                    
                    # Also try to extract tables
                    tables = page.extract_tables()
                    if tables:
                        for table in tables:
                            table_text = "\n".join(["\t".join([str(cell) if cell else "" for cell in row]) for row in table])
                            text += f"\n\n[Table]\n{table_text}\n"
                    
                    if text.strip():
                        all_text.append(f"\n--- Page {i + 1} ---\n{text}")
                
                return "\n".join(all_text), len(pdf.pages)
                
        except Exception as e:
            warnings.append(f"pdfplumber failed: {str(e)}")
            raise
    
    @staticmethod
    def _process_pdf_unstructured(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process PDF using Unstructured.io"""
        try:
            from unstructured.partition.auto import partition
            
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                temp_file.write(file_content)
                temp_file_path = temp_file.name
            
            try:
                elements = partition(filename=temp_file_path, strategy="hi_res")
                
                text_content = []
                page_count = 0
                
                for element in elements:
                    if hasattr(element, 'text') and element.text:
                        text_content.append(element.text)
                    
                    if hasattr(element, 'metadata') and element.metadata:
                        if hasattr(element.metadata, 'page_number'):
                            page_count = max(page_count, element.metadata.page_number)
                
                os.unlink(temp_file_path)
                
                final_text = "\n\n".join(text_content)
                if page_count == 0:
                    page_count = SafeDocumentProcessor._estimate_pages_from_text(final_text)
                
                return final_text, page_count
                
            except Exception as e:
                os.unlink(temp_file_path)
                raise
                
        except Exception as e:
            warnings.append(f"Unstructured.io processing failed: {str(e)}")
            raise
    
    @staticmethod
    def _estimate_pages_from_text(text: str) -> int:
        """Fixed page estimation based on content analysis"""
        if not text or len(text.strip()) == 0:
            logger.warning("Empty text provided for page estimation")
            return 0
        
        # Enhanced page estimation logic
        word_count = len(text.split())
        char_count = len(text)
        line_count = len(text.split('\n'))
        
        # Average words per page: 250-500 (legal documents tend to be dense)
        pages_by_words = max(1, word_count // 350)
        
        # Average characters per page: 1500-3000 (including spaces)
        pages_by_chars = max(1, char_count // 2000)
        
        # For documents with many line breaks (structured content)
        pages_by_lines = max(1, line_count // 50)
        
        # Use the median of the three estimates for better accuracy
        estimates = [pages_by_words, pages_by_chars, pages_by_lines]
        estimates.sort()
        estimated_pages = estimates[1]  # median
        
        # Log the estimation for debugging
        logger.debug(f"Page estimation: words={word_count}, chars={char_count}, lines={line_count}")
        logger.debug(f"Estimates: by_words={pages_by_words}, by_chars={pages_by_chars}, by_lines={pages_by_lines}")
        logger.debug(f"Final estimate: {estimated_pages} pages")
        
        # Ensure reasonable bounds
        return max(1, min(estimated_pages, 1000))
    
    @staticmethod
    def _process_pdf(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Legacy PDF processing method - redirect to multi-method"""
        return SafeDocumentProcessor._process_pdf_multi_method(file_content, warnings)
    
    @staticmethod
    def _process_pdf_enhanced(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Legacy enhanced PDF processing - redirect to multi-method"""
        return SafeDocumentProcessor._process_pdf_multi_method(file_content, warnings)
    
    @staticmethod
    def _process_docx_enhanced(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Enhanced DOCX processing with better page estimation"""
        if FeatureFlags.UNSTRUCTURED_AVAILABLE:
            try:
                from unstructured.partition.auto import partition
                
                with tempfile.NamedTemporaryFile(suffix='.docx', delete=False) as temp_file:
                    temp_file.write(file_content)
                    temp_file_path = temp_file.name
                
                try:
                    # Use Unstructured.io for advanced processing
                    elements = partition(filename=temp_file_path)
                    
                    text_content = []
                    for element in elements:
                        if hasattr(element, 'text') and element.text:
                            text_content.append(element.text)
                    
                    os.unlink(temp_file_path)
                    
                    final_text = "\n\n".join(text_content)
                    pages_estimated = SafeDocumentProcessor._estimate_pages_from_text(final_text)
                    return final_text, pages_estimated
                    
                except Exception as e:
                    os.unlink(temp_file_path)
                    warnings.append(f"Unstructured.io DOCX processing failed: {str(e)}")
                    
            except Exception as e:
                warnings.append(f"Unstructured.io DOCX setup failed: {str(e)}")
        
        # Fallback to existing DOCX processing
        return SafeDocumentProcessor._process_docx(file_content, warnings)
    
    @staticmethod
    def _process_docx(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process DOCX content with enhanced page estimation"""
        try:
            try:
                from docx import Document
                doc = Document(io.BytesIO(file_content))
                text_content = []
                
                # Extract text from paragraphs
                for paragraph in doc.paragraphs:
                    if paragraph.text.strip():
                        text_content.append(paragraph.text)
                
                # Also extract text from tables
                for table in doc.tables:
                    for row in table.rows:
                        row_text = []
                        for cell in row.cells:
                            if cell.text.strip():
                                row_text.append(cell.text.strip())
                        if row_text:
                            text_content.append("\t".join(row_text))
                
                final_text = "\n\n".join(text_content)
                pages_estimated = SafeDocumentProcessor._estimate_pages_from_text(final_text)
                return final_text, pages_estimated
                
            except ImportError:
                warnings.append("python-docx not available. Install with: pip install python-docx")
                return "DOCX processing not available", 0
            except Exception as e:
                warnings.append(f"Error processing DOCX: {str(e)}")
                return "Error processing DOCX", 0
                
        except Exception as e:
            warnings.append(f"Error processing DOCX: {str(e)}")
            return "Error processing DOCX", 0



=== legal_assistant/services/external_db_service.py ===
"""External legal database integration service"""
import logging
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
from ..config import LEXISNEXIS_API_KEY, LEXISNEXIS_API_ENDPOINT, WESTLAW_API_KEY, WESTLAW_API_ENDPOINT
from ..models import User

logger = logging.getLogger(__name__)

class LegalDatabaseInterface(ABC):
    """Abstract interface for external legal databases"""
    
    @abstractmethod
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        pass
    
    @abstractmethod
    def get_document(self, document_id: str) -> Dict:
        pass
    
    @abstractmethod
    def authenticate(self, credentials: Dict) -> bool:
        pass

class LexisNexisInterface(LegalDatabaseInterface):
    def __init__(self, api_key: str = None, api_endpoint: str = None):
        self.api_key = api_key or LEXISNEXIS_API_KEY
        self.api_endpoint = api_endpoint or LEXISNEXIS_API_ENDPOINT
        self.authenticated = False
    
    def authenticate(self, credentials: Dict) -> bool:
        logger.info("LexisNexis authentication placeholder")
        return False
    
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        logger.info(f"LexisNexis search placeholder for query: {query}")
        return []
    
    def get_document(self, document_id: str) -> Dict:
        logger.info(f"LexisNexis document retrieval placeholder for ID: {document_id}")
        return {}

class WestlawInterface(LegalDatabaseInterface):
    def __init__(self, api_key: str = None, api_endpoint: str = None):
        self.api_key = api_key or WESTLAW_API_KEY
        self.api_endpoint = api_endpoint or WESTLAW_API_ENDPOINT
        self.authenticated = False
    
    def authenticate(self, credentials: Dict) -> bool:
        logger.info("Westlaw authentication placeholder")
        return False
    
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        logger.info(f"Westlaw search placeholder for query: {query}")
        return []
    
    def get_document(self, document_id: str) -> Dict:
        logger.info(f"Westlaw document retrieval placeholder for ID: {document_id}")
        return {}

# External databases
external_databases = {
    "lexisnexis": LexisNexisInterface(),
    "westlaw": WestlawInterface()
}

def search_external_databases(query: str, databases: List[str], user: User) -> List[Dict]:
    """Search external legal databases"""
    results = []
    
    for db_name in databases:
        if db_name not in user.external_db_access:
            logger.warning(f"User {user.user_id} does not have access to {db_name}")
            continue
        
        if db_name in external_databases:
            db_interface = external_databases[db_name]
            try:
                db_results = db_interface.search(query)
                for result in db_results:
                    result['source_database'] = db_name
                    results.extend(db_results)
            except Exception as e:
                logger.error(f"Error searching {db_name}: {e}")
    
    return results



=== legal_assistant/services/hybrid_search.py ===
"""Hybrid search combining keyword and semantic search"""
import logging
import numpy as np
from typing import List, Tuple, Dict, Optional
from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder
from langchain.docstore.document import Document

logger = logging.getLogger(__name__)

class HybridSearcher:
    """Combines keyword search (BM25) with semantic search for better retrieval"""
    
    def __init__(self):
        self.reranker = None
        self._initialize_reranker()
    
    def _initialize_reranker(self):
        """Initialize the cross-encoder for reranking"""
        try:
            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            logger.info("‚úÖ Cross-encoder reranker initialized")
        except Exception as e:
            logger.warning(f"Could not initialize reranker: {e}")
            self.reranker = None
    
    def hybrid_search(self, 
                     query: str, 
                     vector_store,
                     k: int = 10,
                     keyword_weight: float = 0.3,
                     semantic_weight: float = 0.7,
                     rerank: bool = True,
                     filter_dict: Optional[Dict] = None) -> List[Tuple[Document, float]]:
        """
        Perform hybrid search combining keyword and semantic search
        
        Args:
            query: Search query
            vector_store: ChromaDB vector store
            k: Number of results to return
            keyword_weight: Weight for keyword search (0-1)
            semantic_weight: Weight for semantic search (0-1)
            rerank: Whether to use cross-encoder reranking
            filter_dict: Optional filter for search
            
        Returns:
            List of (document, score) tuples
        """
        try:
            # Step 1: Get all documents for keyword search
            all_docs = vector_store.get()
            
            if not all_docs or 'documents' not in all_docs:
                logger.warning("No documents found in vector store")
                return []
            
            # Apply filter if provided
            if filter_dict:
                filtered_indices = []
                for i, metadata in enumerate(all_docs.get('metadatas', [])):
                    match = all([metadata.get(k) == v for k, v in filter_dict.items()])
                    if match:
                        filtered_indices.append(i)
                
                if not filtered_indices:
                    logger.warning("No documents match the filter criteria")
                    return []
                
                # Filter documents
                documents = [all_docs['documents'][i] for i in filtered_indices]
                metadatas = [all_docs['metadatas'][i] for i in filtered_indices]
                ids = [all_docs['ids'][i] for i in filtered_indices]
            else:
                documents = all_docs['documents']
                metadatas = all_docs.get('metadatas', [{}] * len(documents))
                ids = all_docs.get('ids', list(range(len(documents))))
            
            # Create Document objects
            doc_objects = []
            for i, (content, metadata, doc_id) in enumerate(zip(documents, metadatas, ids)):
                doc = Document(page_content=content, metadata=metadata or {})
                doc.metadata['_id'] = doc_id
                doc_objects.append(doc)
            
            # Step 2: Keyword search with BM25
            keyword_scores = self._bm25_search(query, doc_objects)
            
            # Step 3: Semantic search
            semantic_results = vector_store.similarity_search_with_score(
                query, 
                k=min(k * 3, len(documents)),  # Get more for better reranking
                filter=filter_dict
            )
            
            # Create mapping of document content to semantic scores
            semantic_scores_map = {}
            for doc, score in semantic_results:
                # Use first 100 chars as key to handle duplicates
                key = doc.page_content[:100]
                semantic_scores_map[key] = 1 - score  # Convert distance to similarity
            
            # Step 4: Combine scores
            combined_results = []
            
            for i, doc in enumerate(doc_objects):
                key = doc.page_content[:100]
                
                # Get keyword score
                keyword_score = keyword_scores[i]
                
                # Get semantic score
                semantic_score = semantic_scores_map.get(key, 0.0)
                
                # Combine scores
                combined_score = (keyword_weight * keyword_score + 
                                semantic_weight * semantic_score)
                
                combined_results.append((doc, combined_score))
            
            # Sort by combined score
            combined_results.sort(key=lambda x: x[1], reverse=True)
            
            # Step 5: Rerank top results if reranker available
            if rerank and self.reranker and len(combined_results) > 0:
                top_results = combined_results[:min(k * 2, len(combined_results))]
                reranked_results = self._rerank_results(query, top_results)
                return reranked_results[:k]
            else:
                return combined_results[:k]
            
        except Exception as e:
            logger.error(f"Error in hybrid search: {e}")
            # Fall back to pure semantic search
            try:
                return vector_store.similarity_search_with_score(query, k=k, filter=filter_dict)
            except Exception as fallback_error:
                logger.error(f"Fallback search also failed: {fallback_error}")
                return []
    
    def _bm25_search(self, query: str, documents: List[Document]) -> np.ndarray:
        """Perform BM25 keyword search"""
        try:
            # Tokenize documents
            tokenized_docs = []
            for doc in documents:
                # Simple tokenization - could be improved
                tokens = doc.page_content.lower().split()
                tokenized_docs.append(tokens)
            
            # Initialize BM25
            bm25 = BM25Okapi(tokenized_docs)
            
            # Tokenize query
            tokenized_query = query.lower().split()
            
            # Get scores
            scores = bm25.get_scores(tokenized_query)
            
            # Normalize scores to 0-1
            if scores.max() > 0:
                scores = scores / scores.max()
            
            return scores
            
        except Exception as e:
            logger.error(f"BM25 search failed: {e}")
            # Return zero scores
            return np.zeros(len(documents))
    
    def _rerank_results(self, query: str, results: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:
        """Rerank results using cross-encoder"""
        try:
            if not results:
                return results
            
            # Prepare pairs for reranking
            pairs = [[query, doc.page_content] for doc, _ in results]
            
            # Get reranking scores
            rerank_scores = self.reranker.predict(pairs)
            
            # Combine with original scores (weighted average)
            reranked_results = []
            for i, (doc, original_score) in enumerate(results):
                # Normalize rerank score to 0-1
                rerank_score = (rerank_scores[i] + 10) / 20  # Assuming scores are -10 to 10
                
                # Weighted combination
                final_score = 0.3 * original_score + 0.7 * rerank_score
                reranked_results.append((doc, final_score))
            
            # Sort by final score
            reranked_results.sort(key=lambda x: x[1], reverse=True)
            
            return reranked_results
            
        except Exception as e:
            logger.error(f"Reranking failed: {e}")
            return results

# Global instance
_hybrid_searcher = None

def get_hybrid_searcher() -> HybridSearcher:
    """Get or create hybrid searcher instance"""
    global _hybrid_searcher
    if _hybrid_searcher is None:
        _hybrid_searcher = HybridSearcher()
    return _hybrid_searcher



=== legal_assistant/services/rag_service.py ===
"""RAG (Retrieval-Augmented Generation) operations service"""
import os
import logging
import numpy as np
import re
from typing import List, Tuple, Dict, Optional

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from ..config import (
    DEFAULT_CHROMA_PATH, DEFAULT_SEARCH_K, ENHANCED_SEARCH_K, 
    CONFIDENCE_WEIGHTS, FeatureFlags, MIN_RELEVANCE_SCORE, SEARCH_CONFIG
)
from ..core.dependencies import get_nlp
from ..core.exceptions import RetrievalError
from .container_manager import get_container_manager
from ..utils.text_processing import remove_duplicate_documents

logger = logging.getLogger(__name__)

def load_database():
    """Load the default database"""
    try:
        if not os.path.exists(DEFAULT_CHROMA_PATH):
            logger.warning(f"Default database path does not exist: {DEFAULT_CHROMA_PATH}")
            return None
        
        embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        db = Chroma(
            collection_name="default",
            embedding_function=embedding_function,
            persist_directory=DEFAULT_CHROMA_PATH
        )
        logger.debug("Default database loaded successfully")
        return db
    except Exception as e:
        logger.error(f"Failed to load default database: {e}")
        raise RetrievalError(f"Failed to load default database: {str(e)}")

def enhanced_retrieval_v2(db, query_text: str, conversation_history_context: str, k: int = ENHANCED_SEARCH_K, document_filter: Dict = None) -> Tuple[List, str]:
    """Enhanced retrieval with multiple strategies"""
    logger.info(f"[ENHANCED_RETRIEVAL] Original query: '{query_text}'")
    
    try:
        direct_results = db.similarity_search_with_score(query_text, k=k, filter=document_filter)
        logger.info(f"[ENHANCED_RETRIEVAL] Direct search returned {len(direct_results)} results")
        
        expanded_query = f"{query_text} {conversation_history_context}"
        expanded_results = db.similarity_search_with_score(expanded_query, k=k, filter=document_filter)
        logger.info(f"[ENHANCED_RETRIEVAL] Expanded search returned {len(expanded_results)} results")
        
        sub_queries = []
        nlp = get_nlp()
        if nlp:
            doc = nlp(query_text)
            for ent in doc.ents:
                if ent.label_ in ["ORG", "PERSON", "LAW", "DATE"]:
                    sub_queries.append(f"What is {ent.text}?")
        
        if not sub_queries:
            question_words = ["what", "who", "when", "where", "why", "how"]
            for word in question_words:
                if word in query_text.lower():
                    sub_queries.append(f"{word.capitalize()} {query_text.lower().replace(word, '').strip()}?")
        
        sub_query_results = []
        for sq in sub_queries[:3]:
            sq_results = db.similarity_search_with_score(sq, k=3, filter=document_filter)
            sub_query_results.extend(sq_results)
        
        logger.info(f"[ENHANCED_RETRIEVAL] Sub-query search returned {len(sub_query_results)} results")
        
        all_results = direct_results + expanded_results + sub_query_results
        unique_results = remove_duplicate_documents(all_results)
        top_results = unique_results[:k]
        
        logger.info(f"[ENHANCED_RETRIEVAL] Final results after deduplication: {len(top_results)}")
        return top_results, "enhanced_retrieval_v2"
        
    except Exception as e:
        logger.error(f"[ENHANCED_RETRIEVAL] Error in enhanced retrieval: {e}")
        basic_results = db.similarity_search_with_score(query_text, k=k, filter=document_filter)
        return basic_results, "basic_fallback"

def enhanced_retrieval_v3(db, query_text: str, conversation_history_context: str, 
                         k: int = ENHANCED_SEARCH_K, document_filter: Dict = None) -> Tuple[List, str]:
    """Enhanced retrieval with query understanding and result filtering"""
    logger.info(f"[ENHANCED_RETRIEVAL_V3] Query: '{query_text}'")
    
    try:
        # Step 1: Query analysis and expansion
        expanded_queries = expand_query_intelligently(query_text)
        logger.info(f"Expanded to {len(expanded_queries)} queries")
        
        all_results = []
        seen_content_hashes = set()
        
        # Step 2: Search with each query variant
        for query_variant in expanded_queries[:3]:  # Limit to top 3 variants
            results = db.similarity_search_with_score(
                query_variant, 
                k=k * 2,  # Get more results for filtering
                filter=document_filter
            )
            
            # Deduplicate and filter by relevance
            for doc, score in results:
                # Create content hash for deduplication
                content_hash = hash(doc.page_content[:200])
                
                # Apply stricter relevance filtering
                if score >= MIN_RELEVANCE_SCORE and content_hash not in seen_content_hashes:
                    # Boost score for exact matches
                    if SEARCH_CONFIG["boost_exact_matches"]:
                        score = boost_score_for_exact_matches(query_text, doc.page_content, score)
                    
                    all_results.append((doc, score))
                    seen_content_hashes.add(content_hash)
        
        # Step 3: Sort by relevance and apply cutoff
        all_results.sort(key=lambda x: x[1], reverse=True)
        
        # Step 4: Apply strict relevance cutoff
        filtered_results = [
            (doc, score) for doc, score in all_results 
            if score >= MIN_RELEVANCE_SCORE
        ]
        
        # Step 5: Rerank if enabled
        if SEARCH_CONFIG["rerank_enabled"] and len(filtered_results) > 0:
            filtered_results = rerank_results(query_text, filtered_results[:SEARCH_CONFIG["max_results_to_rerank"]])
        
        # Return top k results
        final_results = filtered_results[:k]
        
        logger.info(f"[ENHANCED_RETRIEVAL_V3] Returning {len(final_results)} results after filtering")
        return final_results, "enhanced_retrieval_v3"
        
    except Exception as e:
        logger.error(f"[ENHANCED_RETRIEVAL_V3] Error: {e}")
        # Fallback to basic search
        results = db.similarity_search_with_score(query_text, k=k, filter=document_filter)
        return [(doc, score) for doc, score in results if score >= MIN_RELEVANCE_SCORE], "basic_fallback"

def expand_query_intelligently(query: str) -> List[str]:
    """Intelligently expand query for better recall"""
    expanded = [query]  # Original query first
    
    # Add question variations
    if not query.endswith('?'):
        expanded.append(query + '?')
    
    # Handle bill/statute queries specially
    bill_match = re.search(r'(HB|SB|SSB|ESSB|SHB|ESHB)\s*(\d+)', query, re.IGNORECASE)
    if bill_match:
        bill_type = bill_match.group(1).upper()
        bill_num = bill_match.group(2)
        
        # Add variations
        expanded.extend([
            f"{bill_type} {bill_num}",
            f"{bill_type}{bill_num}",
            f"House Bill {bill_num}" if bill_type.startswith('H') else f"Senate Bill {bill_num}",
            f"bill {bill_num} {bill_type}"
        ])
    
    # Extract key entities for expansion
    key_terms = extract_key_terms(query)
    if key_terms:
        # Create focused query with just key terms
        expanded.append(' '.join(key_terms))
    
    # Remove duplicates while preserving order
    seen = set()
    unique_expanded = []
    for q in expanded:
        if q not in seen:
            seen.add(q)
            unique_expanded.append(q)
    
    return unique_expanded

def extract_key_terms(query: str) -> List[str]:
    """Extract key terms from query"""
    # Remove common words
    stop_words = {'what', 'is', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
                  'to', 'for', 'of', 'with', 'by', 'from', 'about', 'into', 'through',
                  'during', 'before', 'after', 'above', 'below', 'between', 'under'}
    
    words = query.lower().split()
    key_terms = [w for w in words if w not in stop_words and len(w) > 2]
    
    return key_terms

def boost_score_for_exact_matches(query: str, content: str, base_score: float) -> float:
    """Boost score for exact matches"""
    query_lower = query.lower()
    content_lower = content.lower()
    
    # Check for exact phrase match
    if query_lower in content_lower:
        return min(1.0, base_score * SEARCH_CONFIG["boost_factor"])
    
    # Check for all query terms present
    query_terms = extract_key_terms(query)
    if all(term in content_lower for term in query_terms):
        return min(1.0, base_score * 1.2)
    
    return base_score

def rerank_results(query: str, results: List[Tuple]) -> List[Tuple]:
    """Rerank results using cross-encoder if available"""
    try:
        from sentence_transformers import CrossEncoder
        
        # Initialize reranker (cache this in production)
        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        
        # Prepare pairs for reranking
        pairs = [[query, doc.page_content] for doc, _ in results]
        
        # Get reranking scores
        rerank_scores = reranker.predict(pairs)
        
        # Combine scores with original scores
        reranked = []
        for i, (doc, orig_score) in enumerate(results):
            # Normalize rerank score to 0-1
            rerank_score = (rerank_scores[i] + 10) / 20
            
            # Weighted combination
            final_score = 0.4 * orig_score + 0.6 * rerank_score
            reranked.append((doc, final_score))
        
        # Sort by final score
        reranked.sort(key=lambda x: x[1], reverse=True)
        
        logger.info("Results reranked successfully")
        return reranked
        
    except ImportError:
        logger.warning("Cross-encoder not available, skipping reranking")
        return results
    except Exception as e:
        logger.error(f"Reranking failed: {e}")
        return results

def hybrid_retrieval_default(db, query_text: str, k: int = ENHANCED_SEARCH_K, document_filter: Dict = None) -> Tuple[List, str]:
    """Hybrid retrieval for default database if hybrid search is available"""
    logger.info(f"[HYBRID_RETRIEVAL] Attempting hybrid search for default database")
    
    try:
        # Check if hybrid search is available
        if not FeatureFlags.HYBRID_SEARCH_AVAILABLE:
            logger.info("[HYBRID_RETRIEVAL] Hybrid search not available, falling back to enhanced retrieval")
            return enhanced_retrieval_v3(db, query_text, "", k, document_filter)
        
        # Import hybrid searcher
        from .hybrid_search import get_hybrid_searcher
        searcher = get_hybrid_searcher()
        
        # Perform hybrid search on default database
        results = searcher.hybrid_search(
            query=query_text,
            vector_store=db,
            k=k,
            keyword_weight=0.3,
            semantic_weight=0.7,
            rerank=True,
            filter_dict=document_filter
        )
        
        logger.info(f"[HYBRID_RETRIEVAL] Hybrid search returned {len(results)} results for default database")
        return results, "hybrid_search_default"
        
    except Exception as e:
        logger.error(f"[HYBRID_RETRIEVAL] Hybrid search failed for default database: {e}")
        # Fall back to enhanced retrieval
        return enhanced_retrieval_v3(db, query_text, "", k, document_filter)

def combined_search(query: str, user_id: Optional[str], search_scope: str, conversation_context: str, 
                   use_enhanced: bool = True, k: int = DEFAULT_SEARCH_K, document_id: str = None) -> Tuple[List, List[str], str]:
    """Combined search across all sources with hybrid search support"""
    all_results = []
    sources_searched = []
    retrieval_method = "basic"
    
    # Search default database
    if search_scope in ["all", "default_only"]:
        try:
            default_db = load_database()
            if default_db:
                # Try hybrid search first if available and enhanced search is requested
                if use_enhanced and FeatureFlags.HYBRID_SEARCH_AVAILABLE:
                    logger.info("[COMBINED_SEARCH] Using hybrid search for default database")
                    default_results, method = hybrid_retrieval_default(default_db, query, k=k)
                    retrieval_method = method
                elif use_enhanced:
                    logger.info("[COMBINED_SEARCH] Using enhanced retrieval v3 for default database")
                    default_results, method = enhanced_retrieval_v3(default_db, query, conversation_context, k=k)
                    retrieval_method = method
                else:
                    logger.info("[COMBINED_SEARCH] Using basic search for default database")
                    default_results = default_db.similarity_search_with_score(query, k=k)
                    retrieval_method = "basic_search"
                
                # Add source type metadata
                for doc, score in default_results:
                    doc.metadata['source_type'] = 'default_database'
                    all_results.append((doc, score))
                sources_searched.append("default_database")
                logger.info(f"[COMBINED_SEARCH] Added {len(default_results)} results from default database")
        except Exception as e:
            logger.error(f"Error searching default database: {e}")
    
    # Search user container
    if user_id and search_scope in ["all", "user_only"]:
        try:
            container_manager = get_container_manager()
            
            # Try hybrid search first if available
            if FeatureFlags.HYBRID_SEARCH_AVAILABLE and hasattr(container_manager, 'hybrid_search_user_container'):
                logger.info("[COMBINED_SEARCH] Using hybrid search for user container")
                user_results = container_manager.hybrid_search_user_container(
                    user_id, query, k=k, document_id=document_id
                )
                # Update retrieval method only if we haven't used hybrid for default DB
                if retrieval_method not in ["hybrid_search_default", "hybrid_search"]:
                    retrieval_method = "hybrid_search_user"
            elif use_enhanced:
                logger.info("[COMBINED_SEARCH] Using enhanced search for user container")
                user_results = container_manager.enhanced_search_user_container(
                    user_id, query, conversation_context, k=k, document_id=document_id
                )
                # Update retrieval method only if we haven't used a better method
                if retrieval_method not in ["hybrid_search_default", "hybrid_search", "enhanced_retrieval_v3"]:
                    retrieval_method = "enhanced_search_user"
            else:
                logger.info("[COMBINED_SEARCH] Using basic search for user container")
                user_results = container_manager.search_user_container(
                    user_id, query, k=k, document_id=document_id
                )
                if retrieval_method == "basic":
                    retrieval_method = "basic_search_user"
            
            # Add source type metadata
            for doc, score in user_results:
                doc.metadata['source_type'] = 'user_container'
                all_results.append((doc, score))
            if user_results:
                sources_searched.append("user_container")
                logger.info(f"[COMBINED_SEARCH] Added {len(user_results)} results from user container")
        except Exception as e:
            logger.error(f"Error searching user container: {e}")
    
    # Post-process results
    if use_enhanced or FeatureFlags.HYBRID_SEARCH_AVAILABLE:
        # Remove duplicates for enhanced/hybrid searches
        all_results = remove_duplicate_documents(all_results)
        logger.info(f"[COMBINED_SEARCH] Removed duplicates, {len(all_results)} unique results remaining")
    else:
        # Sort by score for basic search
        all_results.sort(key=lambda x: x[1], reverse=True)
    
    # Apply relevance threshold
    all_results = [(doc, score) for doc, score in all_results if score >= MIN_RELEVANCE_SCORE]
    
    # Limit to k results
    final_results = all_results[:k]
    
    logger.info(f"[COMBINED_SEARCH] Final search completed:")
    logger.info(f"  - Method: {retrieval_method}")
    logger.info(f"  - Sources: {sources_searched}")
    logger.info(f"  - Results: {len(final_results)}")
    logger.info(f"  - Hybrid available: {FeatureFlags.HYBRID_SEARCH_AVAILABLE}")
    
    return final_results, sources_searched, retrieval_method

def calculate_confidence_score(results_with_scores: List[Tuple], response_length: int) -> float:
    """Calculate confidence score for results"""
    try:
        if not results_with_scores:
            return 0.2
        
        scores = [score for _, score in results_with_scores]
        avg_relevance = np.mean(scores)
        doc_factor = min(1.0, len(results_with_scores) / 5.0)
        
        if len(scores) > 1:
            score_std = np.std(scores)
            consistency_factor = max(0.5, 1.0 - score_std)
        else:
            consistency_factor = 0.7
            
        completeness_factor = min(1.0, response_length / 500.0)
        
        confidence = (
            avg_relevance * CONFIDENCE_WEIGHTS["relevance"] +
            doc_factor * CONFIDENCE_WEIGHTS["document_count"] +
            consistency_factor * CONFIDENCE_WEIGHTS["consistency"] +
            completeness_factor * CONFIDENCE_WEIGHTS["completeness"]
        )
        
        confidence = max(0.0, min(1.0, confidence))
        return confidence
    
    except Exception as e:
        logger.error(f"Error calculating confidence score: {e}")
        return 0.5



=== legal_assistant/storage/__init__.py ===
"""Storage package"""
from .managers import (
    conversations,
    uploaded_files,
    user_sessions,
    add_to_conversation,
    get_conversation_context,
    cleanup_expired_conversations
)

__all__ = [
    'conversations',
    'uploaded_files',
    'user_sessions',
    'add_to_conversation',
    'get_conversation_context',
    'cleanup_expired_conversations'
]



=== legal_assistant/storage/managers.py ===
"""Global state management with document processing status"""
from typing import Dict, Optional, List, Any
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

# Global state stores
conversations: Dict[str, Dict] = {}
uploaded_files: Dict[str, Dict] = {}
user_sessions: Dict[str, Any] = {}
document_processing_status: Dict[str, Dict] = {}  # ADD THIS LINE - NEW: Track processing status

def add_to_conversation(session_id: str, role: str, content: str, sources: Optional[List] = None):
    """Add message to conversation"""
    if session_id not in conversations:
        conversations[session_id] = {
            'messages': [],
            'created_at': datetime.utcnow(),
            'last_accessed': datetime.utcnow()
        }
    
    message = {
        'role': role,
        'content': content,
        'timestamp': datetime.utcnow().isoformat(),
        'sources': sources or []
    }
    
    conversations[session_id]['messages'].append(message)
    conversations[session_id]['last_accessed'] = datetime.utcnow()

def get_conversation_context(session_id: str, max_length: int = 2000) -> str:
    """Get conversation context for a session"""
    if session_id not in conversations:
        return ""
    
    messages = conversations[session_id]['messages']
    context_parts = []
    recent_messages = messages[-4:]
    
    for msg in recent_messages:
        role = msg['role'].upper()
        content = msg['content']
        if len(content) > 800:
            content = content[:800] + "..."
        context_parts.append(f"{role}: {content}")
    
    if context_parts:
        return "Previous conversation:\n" + "\n".join(context_parts)
    return ""

def cleanup_expired_conversations():
    """Clean up expired conversations"""
    now = datetime.utcnow()
    expired_sessions = [
        session_id for session_id, data in conversations.items()
        if now - data['last_accessed'] > timedelta(hours=1)
    ]
    for session_id in expired_sessions:
        del conversations[session_id]
    if expired_sessions:
        logger.info(f"Cleaned up {len(expired_sessions)} expired conversations")



=== legal_assistant/tasks/__init__.py ===
"""Background tasks package"""
from .document_tasks import process_document_background

__all__ = ['process_document_background']



=== legal_assistant/tasks/document_tasks.py ===
"""Background tasks for document processing"""
import logging
import time
from datetime import datetime
from typing import Dict, Any

from ..services.document_processor import SafeDocumentProcessor
from ..services.container_manager import get_container_manager
from ..storage.managers import uploaded_files, document_processing_status

logger = logging.getLogger(__name__)

async def process_document_background(
    file_id: str,
    file_content: bytes,
    file_ext: str,
    filename: str,
    user_id: str
):
    """Process document in background with progress updates"""
    try:
        # Update status
        document_processing_status[file_id] = {
            'status': 'extracting',
            'progress': 10,
            'message': 'Extracting text from document...',
            'started_at': datetime.utcnow().isoformat()
        }
        
        # Process document
        start_time = time.time()
        content, pages_processed, warnings = SafeDocumentProcessor.process_document_from_bytes(
            file_content, filename, file_ext
        )
        
        if not content or len(content.strip()) < 50:
            raise ValueError("Document appears to be empty or could not be processed")
        
        # Update progress
        document_processing_status[file_id] = {
            'status': 'chunking',
            'progress': 40,
            'message': f'Creating searchable chunks from {pages_processed} pages...',
            'pages': pages_processed
        }
        
        # Prepare metadata
        metadata = {
            'source': filename,
            'file_id': file_id,
            'upload_date': datetime.utcnow().isoformat(),
            'user_id': user_id,
            'file_type': file_ext,
            'pages': pages_processed,
            'file_size': len(file_content),
            'content_length': len(content),
            'processing_warnings': warnings
        }
        
        # Add to container with progress tracking
        container_manager = get_container_manager()
        
        # Use async-friendly chunking
        chunks_created = await container_manager.add_document_to_container_async(
            user_id,
            content,
            metadata,
            file_id,
            progress_callback=lambda p: update_progress(file_id, p)
        )
        
        processing_time = time.time() - start_time
        
        # Update final status
        uploaded_files[file_id].update({
            'status': 'completed',
            'pages_processed': pages_processed,
            'chunks_created': chunks_created,
            'processing_time': processing_time,
            'content_length': len(content)
        })
        
        document_processing_status[file_id] = {
            'status': 'completed',
            'progress': 100,
            'message': f'Successfully processed {pages_processed} pages into {chunks_created} searchable chunks',
            'completed_at': datetime.utcnow().isoformat(),
            'processing_time': processing_time
        }
        
        logger.info(f"Document {file_id} processed successfully: {pages_processed} pages, {chunks_created} chunks in {processing_time:.2f}s")
        
    except Exception as e:
        logger.error(f"Error processing document {file_id}: {e}", exc_info=True)
        
        # Update error status
        uploaded_files[file_id]['status'] = 'failed'
        document_processing_status[file_id] = {
            'status': 'failed',
            'progress': 0,
            'message': f'Processing failed: {str(e)}',
            'error': str(e),
            'failed_at': datetime.utcnow().isoformat()
        }

def update_progress(file_id: str, progress: int):
    """Update processing progress"""
    if file_id in document_processing_status:
        current_status = document_processing_status[file_id]
        current_status['progress'] = min(40 + int(progress * 0.5), 90)  # 40-90% for chunking



=== legal_assistant/utils/__init__.py ===
"""Utilities package"""
from .text_processing import (
    parse_multiple_questions,
    semantic_chunking_with_bert,
    basic_text_chunking,
    remove_duplicate_documents,
    extract_bill_information,
    extract_universal_information
)
from .formatting import format_context_for_llm

__all__ = [
    'parse_multiple_questions',
    'semantic_chunking_with_bert',
    'basic_text_chunking',
    'remove_duplicate_documents',
    'extract_bill_information',
    'extract_universal_information',
    'format_context_for_llm'
]



=== legal_assistant/utils/formatting.py ===
"""Formatting utilities"""
import os
from typing import List, Tuple
from ..config import MIN_RELEVANCE_SCORE

def format_context_for_llm(results_with_scores: List[Tuple], max_length: int = 3000) -> Tuple[str, List]:
    """Format search results for LLM context"""
    context_parts = []
    source_info = []
    
    total_length = 0
    for i, (doc, score) in enumerate(results_with_scores):
        if total_length >= max_length:
            break
            
        content = doc.page_content.strip()
        metadata = doc.metadata
        
        source_path = metadata.get('source', 'unknown_source')
        page = metadata.get('page', None)
        source_type = metadata.get('source_type', 'unknown')
        
        display_source = os.path.basename(source_path)
        page_info = f" (Page {page})" if page is not None else ""
        source_prefix = f"[{source_type.upper()}]" if source_type != 'unknown' else ""
        
        if len(content) > 800:
            content = content[:800] + "... [truncated]"
            
        context_part = f"{source_prefix} [{display_source}{page_info}] (Relevance: {score:.2f}): {content}"
        context_parts.append(context_part)
        
        source_info.append({
            'id': i+1,
            'file_name': display_source,
            'page': page,
            'relevance': score,
            'full_path': source_path,
            'source_type': source_type
        })
        
        total_length += len(context_part)
    
    context_text = "\n\n".join(context_parts)
    return context_text, source_info



=== legal_assistant/utils/text_processing.py ===
"""Text processing utilities"""
import re
import logging
import numpy as np
from typing import List, Tuple, Dict, Any
from ..core.dependencies import get_sentence_model, get_sentence_model_name

logger = logging.getLogger(__name__)

def parse_multiple_questions(query_text: str) -> List[str]:
    """Parse multiple questions from a single query"""
    questions = []
    
    if ';' in query_text:
        parts = query_text.split(';')
        for part in parts:
            part = part.strip()
            if part:
                questions.append(part)
    elif '?' in query_text and query_text.count('?') > 1:
        parts = query_text.split('?')
        for part in parts:
            part = part.strip()
            if part:
                questions.append(part + '?')
    else:
        final_question = query_text
        if not final_question.endswith('?') and '?' not in final_question:
            final_question += '?'
        questions = [final_question]
    
    return questions

def semantic_chunking_with_bert(text: str, max_chunk_size: int = 1500, overlap: int = 300) -> List[str]:
    """Advanced semantic chunking with powerful BERT models for legal documents"""
    try:
        sentence_model = get_sentence_model()
        sentence_model_name = get_sentence_model_name()
        
        if sentence_model is None:
            logger.warning("No sentence model available, using basic chunking")
            return basic_text_chunking(text, max_chunk_size, overlap)
        
        logger.info(f"Using semantic chunking with model: {sentence_model_name}")
        
        # For legal documents, split on legal sections and paragraphs
        # Look for common legal document patterns
        legal_patterns = [
            r'\n\s*SECTION\s+\d+',
            r'\n\s*\d+\.\s+',  # Numbered sections
            r'\n\s*\([a-z]\)',  # Subsections (a), (b), etc.
            r'\n\s*WHEREAS',
            r'\n\s*NOW, THEREFORE',
            r'\n\s*Article\s+[IVX\d]+',
        ]
        
        # Split text into meaningful sections first
        sections = []
        current_pos = 0
        
        # Find legal section breaks
        for pattern in legal_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            for match in matches:
                if match.start() > current_pos:
                    section_text = text[current_pos:match.start()].strip()
                    if section_text:
                        sections.append(section_text)
                current_pos = match.start()
        
        # Add remaining text
        if current_pos < len(text):
            remaining_text = text[current_pos:].strip()
            if remaining_text:
                sections.append(remaining_text)
        
        # If no legal patterns found, fall back to paragraph splitting
        if not sections:
            sections = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not sections:
            sections = [text]
        
        # If document is small enough, return as single chunk
        if len(text) <= max_chunk_size:
            return [text]
        
        # Calculate embeddings for sections (batch processing for efficiency)
        try:
            section_embeddings = sentence_model.encode(sections, batch_size=32, show_progress_bar=False)
        except Exception as e:
            logger.warning(f"Embedding calculation failed: {e}, using basic chunking")
            return basic_text_chunking(text, max_chunk_size, overlap)
        
        # Advanced semantic grouping using cosine similarity
        chunks = []
        current_chunk = []
        current_chunk_size = 0
        
        for i, section in enumerate(sections):
            section_size = len(section)
            
            # If adding this section would exceed chunk size
            if current_chunk_size + section_size > max_chunk_size and current_chunk:
                
                # For legal documents, try to find natural breaking points
                if len(current_chunk) > 1:
                    # Calculate semantic similarity to decide on best split point
                    chunk_text = '\n\n'.join(current_chunk)
                    chunks.append(chunk_text)
                    
                    # Intelligent overlap: keep semantically similar content
                    if i > 0:
                        # Use similarity to determine overlap
                        prev_embedding = section_embeddings[i-1:i]
                        curr_embedding = section_embeddings[i:i+1]
                        
                        try:
                            similarity = np.dot(prev_embedding[0], curr_embedding[0])
                            if similarity > 0.7:  # High similarity - include more overlap
                                overlap_sections = current_chunk[-2:] if len(current_chunk) > 1 else current_chunk[-1:]
                            else:
                                overlap_sections = current_chunk[-1:] if current_chunk else []
                            
                            current_chunk = overlap_sections + [section]
                            current_chunk_size = sum(len(s) for s in current_chunk)
                        except:
                            # Fallback to simple overlap
                            current_chunk = [current_chunk[-1], section] if current_chunk else [section]
                            current_chunk_size = sum(len(s) for s in current_chunk)
                    else:
                        current_chunk = [section]
                        current_chunk_size = section_size
                else:
                    # Single large section - need to split it
                    if section_size > max_chunk_size:
                        # Split large section into smaller parts
                        large_section_chunks = basic_text_chunking(section, max_chunk_size, overlap)
                        chunks.extend(large_section_chunks[:-1])  # Add all but last
                        current_chunk = [large_section_chunks[-1]]  # Keep last for next iteration
                        current_chunk_size = len(large_section_chunks[-1])
                    else:
                        chunks.append(section)
                        current_chunk = []
                        current_chunk_size = 0
            else:
                current_chunk.append(section)
                current_chunk_size += section_size
        
        # Add remaining chunk
        if current_chunk:
            chunk_text = '\n\n'.join(current_chunk)
            chunks.append(chunk_text)
        
        # Ensure we have at least one chunk
        if not chunks:
            chunks = [text[:max_chunk_size]]
        
        logger.info(f"Semantic chunking created {len(chunks)} chunks from {len(sections)} sections")
        return chunks
        
    except Exception as e:
        logger.error(f"Advanced semantic chunking failed: {e}, falling back to basic chunking")
        return basic_text_chunking(text, max_chunk_size, overlap)

def basic_text_chunking(text: str, max_chunk_size: int = 1500, overlap: int = 300) -> List[str]:
    """Basic text chunking fallback"""
    if len(text) <= max_chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + max_chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at a sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        
        # Find the best breaking point
        break_point = max(last_period, last_newline)
        if break_point > start + max_chunk_size // 2:  # Only if break point is reasonable
            end = start + break_point + 1
        
        chunks.append(text[start:end])
        start = end - overlap  # Add overlap
    
    return chunks

def remove_duplicate_documents(results_with_scores: List[Tuple]) -> List[Tuple]:
    """Remove duplicate documents from search results"""
    if not results_with_scores:
        return []
    
    unique_results = []
    seen_content = set()
    
    for doc, score in results_with_scores:
        content_hash = hash(doc.page_content[:100])
        if content_hash not in seen_content:
            seen_content.add(content_hash)
            unique_results.append((doc, score))
    
    unique_results.sort(key=lambda x: x[1], reverse=True)
    return unique_results

def extract_bill_information(context_text: str, bill_number: str) -> Dict[str, str]:
    """Pre-extract bill information using regex patterns"""
    extracted_info = {}
    
    # Enhanced pattern to find bill information with more context
    bill_patterns = [
        rf"{bill_number}[^\n]*(?:\n(?:[^\n]*(?:sponsors?|final\s+status|enables|authorizes|establishes)[^\n]*\n?)*)",
        rf"{bill_number}.*?(?=\n\s*[A-Z]{{2,}}|\n\s*[A-Z]{{1,3}}\s+\d+|\Z)",
        rf"{bill_number}[^\n]*\n(?:[^\n]+\n?){{0,5}}"
    ]
    
    for pattern in bill_patterns:
        bill_match = re.search(pattern, context_text, re.DOTALL | re.IGNORECASE)
        if bill_match:
            bill_text = bill_match.group(0)
            logger.info(f"Found bill text for {bill_number}: {bill_text[:200]}...")
            
            # Extract sponsors with multiple patterns
            sponsor_patterns = [
                rf"Sponsors?\s*:\s*([^\n]+)",
                rf"Sponsor\s*:\s*([^\n]+)",
                rf"(?:Rep\.|Sen\.)\s+([^,\n]+(?:,\s*[^,\n]+)*)"
            ]
            
            for sponsor_pattern in sponsor_patterns:
                sponsor_match = re.search(sponsor_pattern, bill_text, re.IGNORECASE)
                if sponsor_match:
                    extracted_info["sponsors"] = sponsor_match.group(1).strip()
                    break
            
            # Extract final status with multiple patterns
            status_patterns = [
                rf"Final Status\s*:\s*([^\n]+)",
                rf"Status\s*:\s*([^\n]+)",
                rf"(?:C\s+\d+\s+L\s+\d+)"
            ]
            
            for status_pattern in status_patterns:
                status_match = re.search(status_pattern, bill_text, re.IGNORECASE)
                if status_match:
                    extracted_info["final_status"] = status_match.group(1).strip()
                    break
            
            # Extract description - everything after bill number until next bill or section
            desc_patterns = [
                rf"{bill_number}[^\n]*\n([^\n]+(?:\n[^\n]+)*?)(?=\n\s*[A-Z]{{2,}}|\n\s*[A-Z]{{1,3}}\s+\d+|\Z)",
                rf"{bill_number}[^\n]*\n([^\n]+)"
            ]
            
            for desc_pattern in desc_patterns:
                desc_match = re.search(desc_pattern, bill_text, re.IGNORECASE)
                if desc_match:
                    description = desc_match.group(1).strip()
                    # Clean up description
                    description = re.sub(r'\s+', ' ', description)
                    extracted_info["description"] = description
                    break
            
            logger.info(f"Extracted info for {bill_number}: {extracted_info}")
            return extracted_info
    
    logger.warning(f"No bill information found for {bill_number}")
    return extracted_info

def extract_universal_information(context_text: str, question: str) -> Dict[str, Any]:
    """Universal information extraction that works for any document type"""
    extracted_info = {
        "key_entities": [],
        "numbers_and_dates": [],
        "relationships": []
    }
    
    try:
        # Extract names (people, organizations, bills, cases, etc.)
        name_patterns = [
            r"\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*",  # Names
            r"(?:HB|SB|SSB|ESSB|SHB|ESHB)\s*\d+",  # Bill numbers
        ]
        
        for pattern in name_patterns:
            matches = re.findall(pattern, context_text)
            extracted_info["key_entities"].extend(matches[:10])  # Limit to prevent overflow
        
        # Extract numbers, dates, amounts
        number_patterns = [
            r"\$[\d,]+(?:\.\d{2})?",  # Dollar amounts
            r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}",  # Dates
            r"\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4}",  # Written dates
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, context_text, re.IGNORECASE)
            extracted_info["numbers_and_dates"].extend(matches[:10])
        
        # Extract relationships
        relationship_patterns = [
            r"(?:sponsors?|authored?\s+by):\s*([^.\n]+)",
            r"(?:final\s+status|status):\s*([^.\n]+)",
        ]
        
        for pattern in relationship_patterns:
            matches = re.findall(pattern, context_text, re.IGNORECASE)
            extracted_info["relationships"].extend(matches[:5])
    
    except Exception as e:
        logger.warning(f"Error in universal extraction: {e}")
    
    return extracted_info



=== requirements.txt ===
# FastAPI and server
fastapi==0.104.1
uvicorn[standard]==0.24.0

# LangChain core and components
langchain
langchain-community
langchain-chroma==0.1.4
langchain-huggingface==0.0.3

# Vector database
chromadb==0.4.17

# Embeddings and ML
sentence-transformers==2.6.1
huggingface-hub==0.23.1

# PDF processing
PyMuPDF==1.23.8
pypdf==3.17.4
pdfplumber==0.10.3
PyPDF2==3.0.1

# Document processing
python-docx==1.1.0

# HTTP and API
httpx==0.25.2
requests==2.31.0
aiohttp==3.9.1

# Environment and utilities
python-dotenv==1.0.0
pydantic==2.5.0
typing-extensions==4.8.0
python-multipart==0.0.6

# NLP dependencies
spacy==3.7.2
nltk==3.8.1
numpy==1.26.4

# OCR support
pytesseract==0.3.10
pdf2image==1.16.3
Pillow==10.1.0

# Hybrid search
rank-bm25==0.2.2



=== README.md ===
# Legal Assistant API - Developer Guide & Troubleshooting

## üèóÔ∏è Architecture Overview

The application follows a clean modular architecture:

```
legal_assistant/
‚îú‚îÄ‚îÄ api/           # API endpoints
‚îú‚îÄ‚îÄ core/          # Core functionality (auth, dependencies, exceptions)
‚îú‚îÄ‚îÄ models/        # Data models (Pydantic)
‚îú‚îÄ‚îÄ processors/    # Business logic processors
‚îú‚îÄ‚îÄ services/      # Service layer (AI, documents, RAG)
‚îú‚îÄ‚îÄ storage/       # State management
‚îî‚îÄ‚îÄ utils/         # Utility functions
```

## üîß Common Issues & Which Files to Edit

### 1. **Document Upload/Processing Issues**

**Problem**: Users report documents aren't being "read" properly or content isn't found

**Files to edit**:
- `services/document_processor.py` - Main document processing logic
- `services/container_manager.py` - Document storage and chunking
- `utils/text_processing.py` - Text chunking algorithms

**Current issues & solutions**:

1. **Poor PDF extraction**:
   - Current: Basic PyMuPDF/pdfplumber extraction
   - Solution: Add OCR support for scanned PDFs
   ```python
   # In services/document_processor.py, add:
   import pytesseract
   from PIL import Image
   
   def _process_pdf_with_ocr(self, file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
       # Add OCR processing for scanned PDFs
   ```

2. **Chunking breaks context**:
   - Current: Fixed-size chunking (1500 chars)
   - Solution: Implement sliding window with better overlap
   ```python
   # In services/container_manager.py, modify add_document_to_container:
   text_splitter = RecursiveCharacterTextSplitter(
       chunk_size=1000,  # Smaller chunks
       chunk_overlap=200,  # More overlap
       separators=["\n\n", "\n", ". ", "? ", "! ", "; ", ": ", " ", ""]
   )
   ```

3. **Metadata not properly stored**:
   - Current: Limited metadata extraction
   - Solution: Enhanced metadata with better indexing

### 2. **Search/Retrieval Issues**

**Problem**: Relevant content not found even when it exists

**Files to edit**:
- `services/rag_service.py` - RAG implementation
- `services/container_manager.py` - Search logic
- `processors/query_processor.py` - Query processing

**Solutions**:

1. **Improve search accuracy**:
   ```python
   # In services/container_manager.py, enhance search:
   def enhanced_search_user_container(self, user_id: str, query: str, ...):
       # Add hybrid search (keyword + semantic)
       keyword_results = self._keyword_search(query)
       semantic_results = user_db.similarity_search_with_score(query, k=k*2)
       
       # Rerank results using cross-encoder
       from sentence_transformers import CrossEncoder
       reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
       
       # Combine and rerank
   ```

2. **Better query expansion**:
   ```python
   # In services/rag_service.py:
   def expand_query(query: str) -> List[str]:
       # Add synonyms, related terms
       # Use WordNet or domain-specific expansions
   ```

### 3. **Citation/Source Attribution Issues**

**Problem**: Incorrect or missing citations

**Files to edit**:
- `utils/formatting.py` - Format context for LLM
- `processors/query_processor.py` - Response generation

**Solution**:
```python
# In utils/formatting.py, enhance source tracking:
def format_context_for_llm(results_with_scores: List[Tuple], max_length: int = 3000):
    # Add unique IDs to each chunk
    # Include page numbers, sections, and exact locations
    
    context_part = f"""
    [SOURCE_ID: {unique_id}]
    [DOCUMENT: {display_source}]
    [PAGE: {page}]
    [SECTION: {section}]
    [RELEVANCE: {score:.2f}]
    
    {content}
    """
```

### 4. **AI Response Quality Issues**

**Problem**: AI not following instructions or hallucinating

**Files to edit**:
- `processors/query_processor.py` - Prompt engineering
- `services/ai_service.py` - AI model configuration

**Solution**: Enhanced prompts (already partially implemented)

### 5. **Performance Issues**

**Problem**: Slow document processing or retrieval

**Files to edit**:
- `services/container_manager.py` - Batch processing
- `config.py` - Adjust batch sizes and limits

## üìö Enhanced Document Processing Solution

Here's a comprehensive solution to improve document reading:

### Step 1: Install Additional Dependencies

Add to `requirements.txt`:
```
# OCR support
pytesseract==0.3.10
pdf2image==1.16.3
Pillow==10.1.0

# Better text extraction
unstructured[pdf]==0.10.30
langchain-unstructured==0.1.0

# Reranking
sentence-transformers==2.6.1
rank-bm25==0.2.2
```

### Step 2: Create Enhanced Document Processor

Create `services/enhanced_document_processor.py`:

```python
import pytesseract
from pdf2image import convert_from_bytes
from PIL import Image
import fitz  # PyMuPDF
from typing import Tuple, List
import re

class EnhancedDocumentProcessor:
    @staticmethod
    def process_pdf_advanced(file_content: bytes) -> Tuple[str, int, List[str]]:
        """Enhanced PDF processing with OCR fallback"""
        warnings = []
        all_text = []
        
        try:
            # First try: PyMuPDF with layout preservation
            doc = fitz.open(stream=file_content, filetype="pdf")
            
            for page_num, page in enumerate(doc):
                # Extract text with layout
                text = page.get_text("dict")
                page_text = EnhancedDocumentProcessor._reconstruct_layout(text)
                
                # If text is too short, might be scanned
                if len(page_text.strip()) < 50:
                    warnings.append(f"Page {page_num + 1} appears to be scanned, using OCR")
                    # Convert to image and OCR
                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x scale
                    img_data = pix.tobytes("png")
                    img = Image.open(io.BytesIO(img_data))
                    ocr_text = pytesseract.image_to_string(img)
                    page_text = ocr_text if ocr_text else page_text
                
                all_text.append(f"--- Page {page_num + 1} ---\n{page_text}")
            
            doc.close()
            
        except Exception as e:
            warnings.append(f"Advanced extraction failed: {e}")
            # Fallback to basic extraction
            
        return "\n\n".join(all_text), len(all_text), warnings
    
    @staticmethod
    def _reconstruct_layout(text_dict):
        """Reconstruct text layout from PyMuPDF dict"""
        blocks = []
        for block in text_dict.get("blocks", []):
            if block["type"] == 0:  # Text block
                block_text = []
                for line in block.get("lines", []):
                    line_text = []
                    for span in line.get("spans", []):
                        line_text.append(span.get("text", ""))
                    block_text.append(" ".join(line_text))
                blocks.append("\n".join(block_text))
        return "\n\n".join(blocks)
```

### Step 3: Implement Hybrid Search

Create `services/hybrid_search.py`:

```python
from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder
import numpy as np

class HybridSearch:
    def __init__(self):
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    def search(self, query: str, documents: List[Document], k: int = 10):
        # Step 1: BM25 keyword search
        tokenized_docs = [doc.page_content.lower().split() for doc in documents]
        bm25 = BM25Okapi(tokenized_docs)
        tokenized_query = query.lower().split()
        bm25_scores = bm25.get_scores(tokenized_query)
        
        # Step 2: Semantic search (existing)
        # ... your existing semantic search ...
        
        # Step 3: Combine scores
        combined_scores = 0.3 * bm25_scores + 0.7 * semantic_scores
        
        # Step 4: Rerank top results
        top_indices = np.argsort(combined_scores)[-k*2:][::-1]
        pairs = [[query, documents[i].page_content] for i in top_indices]
        rerank_scores = self.reranker.predict(pairs)
        
        # Return reranked results
        reranked_indices = np.argsort(rerank_scores)[::-1][:k]
        return [(documents[top_indices[i]], rerank_scores[i]) for i in reranked_indices]
```

### Step 4: Better Chunking Strategy

Update `services/container_manager.py`:

```python
def intelligent_chunking(self, text: str, doc_type: str = "general") -> List[str]:
    """Intelligent chunking based on document structure"""
    
    if doc_type == "legal":
        # Legal document patterns
        section_patterns = [
            r'\n\s*(?:Section|SECTION|Article|ARTICLE)\s+\d+',
            r'\n\s*\d+\.\d+\s+[A-Z]',  # Numbered sections
            r'\n\s*\([a-z]\)\s+',  # Subsections
        ]
        
        chunks = []
        for pattern in section_patterns:
            sections = re.split(pattern, text)
            if len(sections) > 1:
                # Found sections, chunk by section
                for section in sections:
                    if len(section) > 2000:
                        # Section too large, sub-chunk
                        sub_chunks = self.sliding_window_chunk(section, 1000, 200)
                        chunks.extend(sub_chunks)
                    else:
                        chunks.append(section)
                return chunks
    
    # Fallback to sliding window
    return self.sliding_window_chunk(text, 1000, 200)

def sliding_window_chunk(self, text: str, chunk_size: int, overlap: int) -> List[str]:
    """Sliding window chunking with sentence boundaries"""
    sentences = text.split('. ')
    chunks = []
    current_chunk = []
    current_size = 0
    
    for sentence in sentences:
        sentence_size = len(sentence)
        
        if current_size + sentence_size > chunk_size and current_chunk:
            # Save current chunk
            chunks.append('. '.join(current_chunk) + '.')
            
            # Start new chunk with overlap
            overlap_sentences = []
            overlap_size = 0
            for s in reversed(current_chunk):
                if overlap_size + len(s) <= overlap:
                    overlap_sentences.insert(0, s)
                    overlap_size += len(s)
                else:
                    break
            
            current_chunk = overlap_sentences + [sentence]
            current_size = overlap_size + sentence_size
        else:
            current_chunk.append(sentence)
            current_size += sentence_size
    
    if current_chunk:
        chunks.append('. '.join(current_chunk) + '.')
    
    return chunks
```

## üöÄ Quick Fixes for Document Reading Issues

### 1. **Immediate Fix** - Increase chunk overlap:
```python
# In services/container_manager.py
chunk_overlap=500  # Increase from 300
```

### 2. **Add Document Validation**:
```python
# In services/document_processor.py
def validate_extraction(text: str, filename: str) -> bool:
    """Validate if extraction was successful"""
    if len(text) < 100:
        logger.warning(f"Extraction too short for {filename}")
        return False
    
    # Check for common extraction failures
    if text.count('ÔøΩ') > 10:  # Unicode errors
        return False
    
    return True
```

### 3. **Improve Metadata Storage**:
```python
# In services/container_manager.py
doc_metadata['content_hash'] = hashlib.md5(chunk.encode()).hexdigest()
doc_metadata['chunk_position'] = f"{start_idx}/{total_chunks}"
doc_metadata['previous_chunk_id'] = previous_chunk_id
doc_metadata['next_chunk_id'] = next_chunk_id
```

## üìä Monitoring & Debugging

### Add Logging for Document Processing:
```python
# In services/document_processor.py
logger.info(f"Processing {filename}: {file_size} bytes")
logger.info(f"Extracted {len(content)} characters, {pages} pages")
logger.info(f"Text sample: {content[:200]}...")
```

### Add Search Debugging:
```python
# In services/container_manager.py
logger.info(f"Search query: '{query}'")
logger.info(f"Found {len(results)} results")
for i, (doc, score) in enumerate(results[:3]):
    logger.info(f"Result {i}: Score={score:.3f}, Content={doc.page_content[:100]}...")
```

## üè• Health Checks

Add to `api/routers/admin.py`:
```python
@router.get("/document-processing-health")
async def check_processing_health():
    """Check document processing capabilities"""
    return {
        "pdf_processors": {
            "pymupdf": FeatureFlags.PYMUPDF_AVAILABLE,
            "pdfplumber": FeatureFlags.PDFPLUMBER_AVAILABLE,
            "unstructured": FeatureFlags.UNSTRUCTURED_AVAILABLE,
            "ocr": check_ocr_available(),
        },
        "chunking_methods": [
            "semantic_bert",
            "sliding_window",
            "recursive_character",
            "legal_structure"
        ],
        "search_methods": [
            "semantic",
            "keyword_bm25",
            "hybrid",
            "reranked"
        ]
    }
```

## üéØ Testing Document Processing

Create `tests/test_document_processing.py`:
```python
def test_document_extraction():
    """Test document extraction quality"""
    test_files = ["sample.pdf", "scanned.pdf", "complex_layout.pdf"]
    
    for file in test_files:
        content, pages, warnings = processor.process_document(file)
        
        assert len(content) > 100, f"Extraction failed for {file}"
        assert pages > 0, f"No pages detected in {file}"
        
        # Check for common issues
        assert content.count('ÔøΩ') < 10, "Unicode errors in extraction"
        assert not content.startswith("Error"), "Error in extraction"
```

## üí° Best Practices

1. **Always validate extracted content** before storing
2. **Log extraction metrics** for debugging
3. **Use multiple extraction methods** with fallbacks
4. **Store extraction metadata** for troubleshooting
5. **Implement content deduplication** to avoid redundancy
6. **Use hybrid search** for better retrieval
7. **Monitor chunk sizes** and adjust based on document types

## üÜò Emergency Fixes

If users can't find content that exists:

1. **Reduce chunk size** to 1000 characters
2. **Increase chunk overlap** to 500 characters  
3. **Clear and rebuild** the vector database
4. **Enable debug logging** for search queries
5. **Use multiple search strategies** in parallel

Remember: The key to good document retrieval is in the extraction and chunking quality. Poor extraction = poor retrieval, no matter how good your search is!



=== .env.example ===
# OpenRouter API Configuration
OPENAI_API_KEY=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
OPENAI_API_BASE=https://openrouter.ai/api/v1

# External Legal Database APIs (Optional)
LEXISNEXIS_API_KEY=
LEXISNEXIS_API_ENDPOINT=
WESTLAW_API_KEY=
WESTLAW_API_ENDPOINT=

# Server Configuration
PORT=8000



=== .gitignore ===
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/

# ChromaDB
chromadb-database/
user-containers/

# Environment
.env

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db



