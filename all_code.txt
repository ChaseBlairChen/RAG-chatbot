=== legal_assistant/__init__.py ===
"""Legal Assistant API - Multi-User Platform with Enhanced RAG and Comprehensive Analysis"""

__version__ = "10.0.0-SmartRAG-ComprehensiveAnalysis"
__author__ = "Legal Assistant Team"
__description__ = "Multi-User Legal Assistant with Enhanced RAG, Comprehensive Analysis, and External Database Integration"



=== legal_assistant/api/__init__.py ===
"""API package"""



=== legal_assistant/api/routers/__init__.py ===
"""API routers package"""
from . import query, documents, analysis, admin, health

__all__ = ['query', 'documents', 'analysis', 'admin', 'health']



=== legal_assistant/api/routers/admin.py ===
"""Admin endpoints"""
import os
import logging
from datetime import datetime
from fastapi import APIRouter, Form, Depends

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from ...models import User
from ...config import USER_CONTAINERS_PATH
from ...core.security import get_current_user
from ...services.container_manager import get_container_manager
from ...storage.managers import uploaded_files
from ...utils.formatting import format_context_for_llm
from ...utils.text_processing import extract_bill_information, extract_universal_information

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/cleanup-containers")
async def cleanup_orphaned_containers():
    """Clean up orphaned files in containers that are no longer tracked"""
    cleanup_results = {
        "containers_checked": 0,
        "orphaned_documents_found": 0,
        "cleanup_performed": False,
        "errors": []
    }
    
    try:
        if not os.path.exists(USER_CONTAINERS_PATH):
            return cleanup_results
        
        container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                         if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
        
        cleanup_results["containers_checked"] = len(container_dirs)
        tracked_file_ids = set(uploaded_files.keys())
        
        logger.info(f"Checking {len(container_dirs)} containers against {len(tracked_file_ids)} tracked files")
        
        for container_dir in container_dirs:
            try:
                container_path = os.path.join(USER_CONTAINERS_PATH, container_dir)
                
                try:
                    embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
                    db = Chroma(
                        collection_name=f"user_{container_dir}",
                        embedding_function=embedding_function,
                        persist_directory=container_path
                    )
                    
                    logger.info(f"Container {container_dir} loaded successfully")
                    
                except Exception as e:
                    logger.warning(f"Could not load container {container_dir}: {e}")
                    cleanup_results["errors"].append(f"Container {container_dir}: {str(e)}")
                    continue
                    
            except Exception as e:
                logger.error(f"Error processing container {container_dir}: {e}")
                cleanup_results["errors"].append(f"Container {container_dir}: {str(e)}")
        
        return cleanup_results
        
    except Exception as e:
        logger.error(f"Error during container cleanup: {e}")
        cleanup_results["errors"].append(str(e))
        return cleanup_results

@router.post("/sync-document-tracking")
async def sync_document_tracking():
    """Sync the uploaded_files tracking with what's actually in the containers"""
    sync_results = {
        "tracked_files": len(uploaded_files),
        "containers_found": 0,
        "sync_performed": False,
        "recovered_files": 0,
        "errors": []
    }
    
    try:
        if not os.path.exists(USER_CONTAINERS_PATH):
            return sync_results
        
        container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                         if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
        
        sync_results["containers_found"] = len(container_dirs)
        
        logger.info(f"Syncing document tracking: {len(uploaded_files)} tracked files, {len(container_dirs)} containers")
        
        return sync_results
        
    except Exception as e:
        logger.error(f"Error during document tracking sync: {e}")
        sync_results["errors"].append(str(e))
        return sync_results

@router.get("/document-health")
async def check_document_health():
    """Check the health of document tracking and containers"""
    health_info = {
        "timestamp": datetime.utcnow().isoformat(),
        "uploaded_files_count": len(uploaded_files),
        "container_directories": 0,
        "users_with_containers": 0,
        "orphaned_files": [],
        "container_errors": [],
        "recommendations": []
    }
    
    try:
        # Check container directories
        if os.path.exists(USER_CONTAINERS_PATH):
            container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                             if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
            health_info["container_directories"] = len(container_dirs)
            
            # Check which users have containers
            user_ids_with_files = set()
            for file_data in uploaded_files.values():
                if 'user_id' in file_data:
                    user_ids_with_files.add(file_data['user_id'])
            
            health_info["users_with_containers"] = len(user_ids_with_files)
            
            # Check for potential issues
            if len(container_dirs) > len(user_ids_with_files):
                health_info["recommendations"].append("Some containers may be orphaned - consider running cleanup")
            
            if len(uploaded_files) == 0 and len(container_dirs) > 0:
                health_info["recommendations"].append("Containers exist but no files are tracked - may need sync")
        
        # Check for files with missing metadata
        for file_id, file_data in uploaded_files.items():
            if not file_data.get('user_id'):
                health_info["orphaned_files"].append(file_id)
        
        if health_info["orphaned_files"]:
            health_info["recommendations"].append(f"{len(health_info['orphaned_files'])} files have missing user_id")
        
        logger.info(f"Document health check: {health_info['uploaded_files_count']} files, {health_info['container_directories']} containers")
        
        return health_info
        
    except Exception as e:
        logger.error(f"Error during document health check: {e}")
        health_info["container_errors"].append(str(e))
        return health_info

@router.post("/emergency-clear-tracking")
async def emergency_clear_document_tracking():
    """EMERGENCY: Clear all document tracking"""
    try:
        global uploaded_files
        backup_count = len(uploaded_files)
        uploaded_files.clear()
        
        logger.warning(f"EMERGENCY: Cleared tracking for {backup_count} files")
        
        return {
            "status": "completed",
            "cleared_files": backup_count,
            "warning": "All document tracking has been cleared. Users will need to re-upload documents.",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error during emergency clear: {e}")
        return {
            "status": "failed",
            "error": str(e)
        }

@router.get("/debug/test-bill-search")
async def debug_bill_search_get(
    bill_number: str,
    user_id: str
):
    """Debug bill-specific search functionality (GET version for browser testing)"""
    
    try:
        container_manager = get_container_manager()
        # Get user database
        user_db = container_manager.get_user_database_safe(user_id)
        if not user_db:
            return {"error": "No user database found"}
        
        # Get all documents and check metadata
        all_docs = user_db.get()
        found_chunks = []
        
        logger.info(f"Debugging search for bill: {bill_number}")
        logger.info(f"Total documents in database: {len(all_docs.get('ids', []))}")
        
        for i, (doc_id, metadata, content) in enumerate(zip(
            all_docs.get('ids', []), 
            all_docs.get('metadatas', []), 
            all_docs.get('documents', [])
        )):
            if metadata:
                chunk_index = metadata.get('chunk_index', 'unknown')
                contains_bills = metadata.get('contains_bills', '')
                
                if bill_number in contains_bills:
                    found_chunks.append({
                        'chunk_index': chunk_index,
                        'contains_bills': contains_bills,
                        'content_preview': content[:200] + "..." if len(content) > 200 else content
                    })
                    logger.info(f"Found {bill_number} in chunk {chunk_index}")
        
        # Also test direct text search
        direct_search = [content for content in all_docs.get('documents', []) if bill_number in content]
        
        return {
            "bill_number": bill_number,
            "user_id": user_id,
            "total_chunks": len(all_docs.get('ids', [])),
            "chunks_with_bill_metadata": found_chunks,
            "chunks_with_bill_in_text": len(direct_search),
            "text_search_preview": direct_search[0][:300] + "..." if direct_search else "Not found in text",
            "sample_metadata": all_docs.get('metadatas', [])[:2] if all_docs.get('metadatas') else []
        }
        
    except Exception as e:
        logger.error(f"Debug bill search failed: {e}")
        return {"error": str(e)}

@router.post("/debug/test-bill-search")
async def debug_bill_search(
    bill_number: str = Form(...),
    user_id: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Debug bill-specific search functionality"""
    
    try:
        container_manager = get_container_manager()
        # Get user database
        user_db = container_manager.get_user_database_safe(user_id)
        if not user_db:
            return {"error": "No user database found"}
        
        # Get all documents and check metadata
        all_docs = user_db.get()
        found_chunks = []
        
        logger.info(f"Debugging search for bill: {bill_number}")
        logger.info(f"Total documents in database: {len(all_docs.get('ids', []))}")
        
        for i, (doc_id, metadata, content) in enumerate(zip(
            all_docs.get('ids', []), 
            all_docs.get('metadatas', []), 
            all_docs.get('documents', [])
        )):
            if metadata:
                chunk_index = metadata.get('chunk_index', 'unknown')
                contains_bills = metadata.get('contains_bills', '')
                
                if bill_number in contains_bills:
                    found_chunks.append({
                        'chunk_index': chunk_index,
                        'contains_bills': contains_bills,
                        'content_preview': content[:200] + "..." if len(content) > 200 else content
                    })
                    logger.info(f"Found {bill_number} in chunk {chunk_index}")
        
        # Also test direct text search
        direct_search = [content for content in all_docs.get('documents', []) if bill_number in content]
        
        return {
            "bill_number": bill_number,
            "total_chunks": len(all_docs.get('ids', [])),
            "chunks_with_bill_metadata": found_chunks,
            "chunks_with_bill_in_text": len(direct_search),
            "text_search_preview": direct_search[0][:300] + "..." if direct_search else "Not found in text"
        }
        
    except Exception as e:
        logger.error(f"Debug bill search failed: {e}")
        return {"error": str(e)}

@router.post("/debug/test-extraction")
async def debug_test_extraction(
    question: str = Form(...),
    user_id: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Test information extraction for any question"""
    
    try:
        container_manager = get_container_manager()
        # Search user's documents
        user_results = container_manager.enhanced_search_user_container(user_id, question, "", k=5)
        
        if user_results:
            # Get context
            context_text, source_info = format_context_for_llm(user_results, max_length=3000)
            
            # Test extraction
            import re
            bill_match = re.search(r"(HB|SB|SSB|ESSB|SHB|ESHB)\s*(\d+)", question, re.IGNORECASE)
            if bill_match:
                bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
                extracted_info = extract_bill_information(context_text, bill_number)
            else:
                extracted_info = extract_universal_information(context_text, question)
            
            return {
                "question": question,
                "context_preview": context_text[:500] + "...",
                "extracted_info": extracted_info,
                "sources_found": len(user_results)
            }
        else:
            return {
                "question": question,
                "error": "No relevant documents found"
            }
            
    except Exception as e:
        return {"error": str(e)}



=== legal_assistant/api/routers/analysis.py ===
"""Analysis endpoints"""
import logging
from fastapi import APIRouter, Depends, HTTPException

from ...models import User, ComprehensiveAnalysisRequest, StructuredAnalysisResponse, AnalysisType
from ...core.security import get_current_user
from ...services.analysis_service import ComprehensiveAnalysisProcessor

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/comprehensive-analysis", response_model=StructuredAnalysisResponse)
async def comprehensive_document_analysis(
    request: ComprehensiveAnalysisRequest,
    current_user: User = Depends(get_current_user)
):
    """Comprehensive document analysis endpoint"""
    logger.info(f"Comprehensive analysis request: user={request.user_id}, doc={request.document_id}, types={request.analysis_types}")
    
    try:
        if request.user_id != current_user.user_id:
            raise HTTPException(status_code=403, detail="Cannot analyze documents for different user")
        
        processor = ComprehensiveAnalysisProcessor()
        result = processor.process_comprehensive_analysis(request)
        
        logger.info(f"Comprehensive analysis completed: confidence={result.overall_confidence:.2f}, time={result.processing_time:.2f}s")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Comprehensive analysis endpoint failed: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@router.post("/quick-analysis/{document_id}")
async def quick_document_analysis(
    document_id: str,
    analysis_type: AnalysisType = AnalysisType.COMPREHENSIVE,
    current_user: User = Depends(get_current_user)
):
    """Quick analysis endpoint for single documents"""
    try:
        request = ComprehensiveAnalysisRequest(
            document_id=document_id,
            analysis_types=[analysis_type],
            user_id=current_user.user_id,
            response_style="detailed"
        )
        
        processor = ComprehensiveAnalysisProcessor()
        result = processor.process_comprehensive_analysis(request)
        
        return {
            "success": True,
            "analysis": result,
            "message": f"Analysis completed with {result.overall_confidence:.1%} confidence"
        }
        
    except Exception as e:
        logger.error(f"Quick analysis failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "Analysis failed"
        }



=== legal_assistant/api/routers/documents.py ===
"""Document management endpoints"""
import os
import uuid
import logging
import traceback
from datetime import datetime
from fastapi import APIRouter, File, UploadFile, Depends, HTTPException

from ...models import User, DocumentUploadResponse
from ...config import MAX_FILE_SIZE, LEGAL_EXTENSIONS
from ...core.security import get_current_user
from ...services.document_processor import SafeDocumentProcessor
from ...services.container_manager import get_container_manager
from ...storage.managers import uploaded_files

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/user/upload", response_model=DocumentUploadResponse)
async def upload_user_document(
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user)
):
    """Enhanced upload endpoint with file_id tracking and timeout handling"""
    start_time = datetime.utcnow()
    
    try:
        # Check file size first (before reading)
        file.file.seek(0, 2)
        file_size = file.file.tell()
        file.file.seek(0)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400, 
                detail=f"File too large. Maximum size is {MAX_FILE_SIZE//1024//1024}MB. Your file: {file_size//1024//1024}MB"
            )
        
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in LEGAL_EXTENSIONS:
            raise HTTPException(status_code=400, detail=f"Unsupported file type. Supported: {LEGAL_EXTENSIONS}")
        
        logger.info(f"Processing upload: {file.filename} ({file_size//1024}KB) for user {current_user.user_id}")
        
        # Process document with timeout protection
        try:
            content, pages_processed, warnings = SafeDocumentProcessor.process_document_safe(file)
        except Exception as doc_error:
            logger.error(f"Document processing failed: {doc_error}")
            raise HTTPException(
                status_code=422, 
                detail=f"Failed to process document: {str(doc_error)}"
            )
        
        if not content or len(content.strip()) < 50:
            raise HTTPException(
                status_code=422,
                detail="Document appears to be empty or could not be processed properly"
            )
        
        file_id = str(uuid.uuid4())
        metadata = {
            'source': file.filename,
            'file_id': file_id,
            'upload_date': datetime.utcnow().isoformat(),
            'user_id': current_user.user_id,
            'file_type': file_ext,
            'pages': pages_processed,
            'file_size': file_size,
            'content_length': len(content),
            'processing_warnings': warnings
        }
        
        logger.info(f"Adding document to container: {len(content)} chars, {pages_processed} pages")
        
        # Add to container with timeout protection
        container_manager = get_container_manager()
        try:
            success = container_manager.add_document_to_container(
                current_user.user_id,
                content,
                metadata,
                file_id
            )
        except Exception as container_error:
            logger.error(f"Container operation failed: {container_error}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to store document: {str(container_error)}"
            )
        
        if not success:
            raise HTTPException(status_code=500, detail="Failed to add document to user container")
        
        session_id = str(uuid.uuid4())
        uploaded_files[file_id] = {
            'filename': file.filename,
            'user_id': current_user.user_id,
            'container_id': current_user.container_id,
            'pages_processed': pages_processed,
            'uploaded_at': datetime.utcnow(),
            'session_id': session_id,
            'file_size': file_size,
            'content_length': len(content)
        }
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        logger.info(f"Upload successful: {file.filename} processed in {processing_time:.2f}s")
        
        return DocumentUploadResponse(
            message=f"Document {file.filename} uploaded successfully ({pages_processed} pages, {len(content)} chars)",
            file_id=file_id,
            pages_processed=pages_processed,
            processing_time=processing_time,
            warnings=warnings,
            session_id=session_id,
            user_id=current_user.user_id,
            container_id=current_user.container_id or ""
        )
        
    except HTTPException:
        raise
    except Exception as e:
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        logger.error(f"Error uploading user document after {processing_time:.2f}s: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500, 
            detail=f"Upload failed after {processing_time:.2f}s: {str(e)}"
        )

@router.get("/user/documents")
async def list_user_documents(current_user: User = Depends(get_current_user)):
    """ENHANCED: List all documents in user's container with better error handling"""
    try:
        user_documents = []
        
        # Add timeout and better error handling
        for file_id, file_data in uploaded_files.items():
            try:
                if file_data.get('user_id') == current_user.user_id:
                    # Handle both datetime objects and strings
                    uploaded_at_str = file_data['uploaded_at']
                    if hasattr(uploaded_at_str, 'isoformat'):
                        uploaded_at_str = uploaded_at_str.isoformat()
                    elif not isinstance(uploaded_at_str, str):
                        uploaded_at_str = str(uploaded_at_str)
                    
                    user_documents.append({
                        'file_id': file_id,
                        'filename': file_data['filename'],
                        'uploaded_at': uploaded_at_str,
                        'pages_processed': file_data.get('pages_processed', 0),
                        'file_size': file_data.get('file_size', 0)
                    })
            except Exception as e:
                logger.warning(f"Error processing file {file_id}: {e}")
                continue
        
        logger.info(f"Retrieved {len(user_documents)} documents for user {current_user.user_id}")
        
        return {
            'user_id': current_user.user_id,
            'container_id': current_user.container_id,
            'documents': user_documents,
            'total_documents': len(user_documents)
        }
        
    except Exception as e:
        logger.error(f"Error listing user documents: {e}")
        # Return empty list instead of failing completely
        return {
            'user_id': current_user.user_id,
            'container_id': current_user.container_id or "unknown",
            'documents': [],
            'total_documents': 0,
            'error': str(e)
        }

@router.delete("/user/documents/{file_id}")
async def delete_user_document(
    file_id: str,
    current_user: User = Depends(get_current_user)
):
    """Delete a document from user's container"""
    if file_id not in uploaded_files:
        raise HTTPException(status_code=404, detail="Document not found")
    
    file_data = uploaded_files[file_id]
    if file_data.get('user_id') != current_user.user_id:
        raise HTTPException(status_code=403, detail="Unauthorized to delete this document")
    
    del uploaded_files[file_id]
    return {"message": "Document deleted successfully", "file_id": file_id}



=== legal_assistant/api/routers/external.py ===
"""External database endpoints"""
import logging
from typing import List
from fastapi import APIRouter, Form, Depends, HTTPException

from ...models import User
from ...core.security import get_current_user
from ...services.external_db_service import search_external_databases

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/external/search")
async def search_external_databases_endpoint(
    query: str = Form(...),
    databases: List[str] = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Search external legal databases (requires premium subscription)"""
    if current_user.subscription_tier not in ["premium", "enterprise"]:
        raise HTTPException(
            status_code=403, 
            detail="External database access requires premium subscription"
        )
    
    results = search_external_databases(query, databases, current_user)
    
    return {
        "query": query,
        "databases_searched": databases,
        "results": results,
        "total_results": len(results)
    }



=== legal_assistant/api/routers/health.py ===
"""Health check endpoints"""
import os
from datetime import datetime
from fastapi import APIRouter
from fastapi.responses import HTMLResponse

from ...config import (
    DEFAULT_CHROMA_PATH, USER_CONTAINERS_PATH, OPENROUTER_API_KEY, FeatureFlags
)
from ...models import ConversationHistory
from ...storage.managers import conversations
from ...core.dependencies import get_nlp, get_sentence_model, get_embeddings, get_sentence_model_name

router = APIRouter()

@router.get("/health")
def health_check():
    """Enhanced system health check with comprehensive analysis capabilities"""
    db_exists = os.path.exists(DEFAULT_CHROMA_PATH)
    
    nlp = get_nlp()
    sentence_model = get_sentence_model()
    embeddings = get_embeddings()
    sentence_model_name = get_sentence_model_name()
    
    return {
        "status": "healthy",
        "version": "10.0.0-SmartRAG-ComprehensiveAnalysis",
        "timestamp": datetime.utcnow().isoformat(),
        "ai_enabled": FeatureFlags.AI_ENABLED,
        "openrouter_api_configured": bool(OPENROUTER_API_KEY),
        "components": {
            "default_database": {
                "exists": db_exists,
                "path": DEFAULT_CHROMA_PATH
            },
            "user_containers": {
                "enabled": True,
                "base_path": USER_CONTAINERS_PATH,
                "active_containers": len(os.listdir(USER_CONTAINERS_PATH)) if os.path.exists(USER_CONTAINERS_PATH) else 0,
                "document_specific_retrieval": True,
                "file_id_tracking": True
            },
            "external_databases": {
                "lexisnexis": {
                    "configured": bool(os.environ.get("LEXISNEXIS_API_KEY")),
                    "status": "ready" if bool(os.environ.get("LEXISNEXIS_API_KEY")) else "not_configured"
                },
                "westlaw": {
                    "configured": bool(os.environ.get("WESTLAW_API_KEY")),
                    "status": "ready" if bool(os.environ.get("WESTLAW_API_KEY")) else "not_configured"
                }
            },
            "comprehensive_analysis": {
                "enabled": True,
                "analysis_types": [
                    "comprehensive",
                    "document_summary", 
                    "key_clauses",
                    "risk_assessment",
                    "timeline_deadlines", 
                    "party_obligations",
                    "missing_clauses"
                ],
                "structured_output": True,
                "document_specific": True,
                "confidence_scoring": True,
                "single_api_call": True
            },
            "enhanced_rag": {
                "enabled": True,
                "features": [
                    "multi_query_strategies",
                    "query_expansion",
                    "entity_extraction",
                    "sub_query_decomposition",
                    "confidence_scoring",
                    "duplicate_removal",
                    "document_specific_filtering"
                ],
                "nlp_model": nlp is not None,
                "sentence_model": sentence_model is not None,
                "sentence_model_name": sentence_model_name if sentence_model else "none",
                "embedding_model": getattr(embeddings, 'model_name', 'unknown') if embeddings else "none"
            },
            "document_processing": {
                "pdf_support": FeatureFlags.PYMUPDF_AVAILABLE or FeatureFlags.PDFPLUMBER_AVAILABLE,
                "pymupdf_available": FeatureFlags.PYMUPDF_AVAILABLE,
                "pdfplumber_available": FeatureFlags.PDFPLUMBER_AVAILABLE,
                "unstructured_available": FeatureFlags.UNSTRUCTURED_AVAILABLE,
                "docx_support": True,
                "txt_support": True,
                "safe_document_processor": True,
                "enhanced_page_estimation": True,
                "bert_semantic_chunking": sentence_model is not None,
                "advanced_legal_chunking": True,
                "embedding_model": sentence_model_name if sentence_model else "none"
            }
        },
        "new_endpoints": [
            "POST /comprehensive-analysis - Full structured analysis",
            "POST /quick-analysis/{document_id} - Quick single document analysis", 
            "Enhanced /ask - Detects comprehensive analysis requests",
            "Enhanced /user/upload - Stores file_id for targeting",
            "GET /admin/document-health - Check system health",
            "POST /admin/cleanup-containers - Clean orphaned containers",
            "POST /admin/emergency-clear-tracking - Reset document tracking"
        ],
        "features": [
            "‚úÖ User-specific document containers",
            "‚úÖ Enhanced RAG with multi-query strategies",
            "‚úÖ Combined search across all sources",
            "‚úÖ External legal database integration (ready)",
            "‚úÖ Subscription tier management",
            "‚úÖ Document access control",
            "‚úÖ Source attribution (default/user/external)",
            "‚úÖ Dynamic confidence scoring",
            "‚úÖ Query expansion and decomposition",
            "‚úÖ SafeDocumentProcessor for file handling",
            "üîß Optional authentication for debugging",
            "üÜï Comprehensive multi-analysis in single API call",
            "üÜï Document-specific analysis targeting",
            "üÜï Structured analysis responses with sections",
            "üÜï Enhanced confidence scoring per section",
            "üÜï File ID tracking for precise document retrieval",
            "üÜï Automatic comprehensive analysis detection",
            "üÜï Container cleanup and health monitoring",
            "üÜï Enhanced error handling and recovery",
            "üÜï Fixed page estimation with content analysis",
            "üÜï Unstructured.io integration for advanced processing",
            "üÜï BERT-based semantic chunking for better retrieval",
            "üÜï Enhanced information extraction (bills, sponsors, etc.)",
            "üÜï Legal-specific BERT models (InCaseLawBERT, legal-bert-base-uncased)",
            "üÜï Advanced semantic similarity for intelligent chunking",
            "üÜï Legal document pattern recognition for better segmentation"
        ],
        # Frontend compatibility fields
        "unified_mode": True,
        "enhanced_rag": True,
        "database_exists": db_exists,
        "database_path": DEFAULT_CHROMA_PATH,
        "api_key_configured": bool(OPENROUTER_API_KEY),
        "active_conversations": len(conversations)
    }

@router.get("/conversation/{session_id}", response_model=ConversationHistory)
async def get_conversation(session_id: str):
    """Get the conversation history for a session"""
    from fastapi import HTTPException
    
    if session_id not in conversations:
        raise HTTPException(status_code=404, detail="Session not found")
    
    return ConversationHistory(
        session_id=session_id,
        messages=conversations[session_id]['messages']
    )

@router.get("/subscription/status")
async def get_subscription_status():
    """Get user's subscription status and available features"""
    from ...core.security import get_current_user
    from fastapi import Depends
    
    current_user = await get_current_user()
    
    features = {
        "free": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": 10,
            "external_databases": [],
            "ai_analysis": True,
            "api_calls_per_month": 100,
            "enhanced_rag": True,
            "comprehensive_analysis": True
        },
        "premium": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": 100,
            "external_databases": ["lexisnexis", "westlaw"],
            "ai_analysis": True,
            "api_calls_per_month": 1000,
            "priority_support": True,
            "enhanced_rag": True,
            "comprehensive_analysis": True,
            "document_specific_analysis": True
        },
        "enterprise": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": "unlimited",
            "external_databases": ["lexisnexis", "westlaw", "bloomberg_law"],
            "ai_analysis": True,
            "api_calls_per_month": "unlimited",
            "priority_support": True,
            "custom_integrations": True,
            "enhanced_rag": True,
            "comprehensive_analysis": True,
            "document_specific_analysis": True,
            "bulk_analysis": True
        }
    }
    
    return {
        "user_id": current_user.user_id,
        "subscription_tier": current_user.subscription_tier,
        "features": features.get(current_user.subscription_tier, features["free"]),
        "external_db_access": current_user.external_db_access
    }

@router.get("/", response_class=HTMLResponse)
def get_interface():
    """Web interface with updated documentation for comprehensive analysis"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Legal Assistant - Complete Multi-Analysis Edition [MODULAR]</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background: #f8f9fa; }
            .container { max-width: 1200px; margin: 0 auto; }
            h1 { color: #2c3e50; }
            .feature-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 30px 0; }
            .feature-card { background: #fff; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px; }
            .endpoint { background: #f1f3f4; padding: 10px; margin: 10px 0; border-radius: 5px; font-family: monospace; }
            .status { padding: 5px 10px; border-radius: 15px; font-size: 12px; }
            .status-active { background: #d4edda; color: #155724; }
            .status-ready { background: #cce5ff; color: #004085; }
            .status-modular { background: #28a745; color: white; }
            .badge-modular { background: #17a2b8; color: white; padding: 2px 8px; border-radius: 10px; font-size: 11px; margin-left: 5px; }
            .code-example { background: #f8f9fa; border: 1px solid #e9ecef; border-radius: 5px; padding: 15px; margin: 10px 0; font-family: monospace; font-size: 12px; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>‚öñÔ∏è Legal Assistant API v10.0 <span class="badge-modular">MODULAR ARCHITECTURE</span></h1>
            <p>Complete Multi-User Platform with Enhanced RAG, Comprehensive Analysis, and Container Management</p>
            <div class="status status-modular">üéØ Clean Modular Structure - Easy to Maintain and Extend!</div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h3>üìÅ Modular Architecture</h3>
                    <p>Clean separation of concerns</p>
                    <ul>
                        <li>‚úÖ Services for business logic</li>
                        <li>‚úÖ API routers for endpoints</li>
                        <li>‚úÖ Models for data structures</li>
                        <li>‚úÖ Utils for helper functions</li>
                    </ul>
                </div>
                
                <div class="feature-card">
                    <h3>üöÄ Comprehensive Analysis</h3>
                    <p>All analysis types in a single efficient API call</p>
                    <ul>
                        <li>‚úÖ Document summary</li>
                        <li>‚úÖ Key clauses extraction</li>
                        <li>‚úÖ Risk assessment</li>
                        <li>‚úÖ Timeline & deadlines</li>
                        <li>‚úÖ Party obligations</li>
                        <li>‚úÖ Missing clauses detection</li>
                    </ul>
                </div>
                
                <div class="feature-card">
                    <h3>üõ†Ô∏è Enhanced Error Handling</h3>
                    <p>Robust container management with auto-recovery</p>
                    <ul>
                        <li>‚úÖ Timeout protection</li>
                        <li>‚úÖ Container auto-recovery</li>
                        <li>‚úÖ Graceful degradation</li>
                        <li>‚úÖ Health monitoring</li>
                    </ul>
                </div>
            </div>
            
            <h2>üì° API Reference</h2>
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>Core Endpoints</h4>
                    <div class="endpoint">POST /api/ask - Enhanced chat</div>
                    <div class="endpoint">POST /api/user/upload - Document upload</div>
                    <div class="endpoint">GET /api/user/documents - List documents</div>
                </div>
                
                <div class="feature-card">
                    <h4>Analysis Endpoints</h4>
                    <div class="endpoint">POST /api/comprehensive-analysis</div>
                    <div class="endpoint">POST /api/quick-analysis/{id}</div>
                </div>
                
                <div class="feature-card">
                    <h4>Admin Endpoints</h4>
                    <div class="endpoint">GET /api/admin/document-health</div>
                    <div class="endpoint">POST /api/admin/cleanup-containers</div>
                </div>
            </div>
            
            <p style="text-align: center; color: #7f8c8d; margin-top: 30px;">
                üéâ Modular Legal Assistant Backend - Clean Architecture üéâ
                <br>Version 10.0.0-SmartRAG-ComprehensiveAnalysis
                <br>Fully modularized for easy maintenance and extensibility!
            </p>
        </div>
    </body>
    </html>
    """



=== legal_assistant/api/routers/query.py ===
"""Query endpoints"""
import uuid
import logging
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException

from ...models import Query, QueryResponse, User
from ...core.security import get_current_user
from ...storage.managers import conversations, cleanup_expired_conversations
from ...processors.query_processor import process_query

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/ask", response_model=QueryResponse)
async def ask_question(query: Query, current_user: User = Depends(get_current_user)):
    """Enhanced ask endpoint with comprehensive analysis detection"""
    logger.info(f"Received ask request: {query}")
    
    cleanup_expired_conversations()
    
    session_id = query.session_id or str(uuid.uuid4())
    user_id = query.user_id or current_user.user_id
    
    if session_id not in conversations:
        conversations[session_id] = {
            "messages": [],
            "created_at": datetime.utcnow(),
            "last_accessed": datetime.utcnow()
        }
    else:
        conversations[session_id]["last_accessed"] = datetime.utcnow()
    
    user_question = query.question.strip()
    if not user_question:
        return QueryResponse(
            response=None,
            error="Question cannot be empty.",
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[]
        )
    
    response = process_query(
        user_question, 
        session_id, 
        user_id,
        query.search_scope or "all",
        query.response_style or "balanced",
        query.use_enhanced_rag if query.use_enhanced_rag is not None else True,
        query.document_id
    )
    return response

@router.post("/ask-debug", response_model=QueryResponse)
async def ask_question_debug(query: Query):
    """Debug version of ask endpoint without authentication"""
    logger.info(f"Debug ask request received: {query}")
    
    cleanup_expired_conversations()
    
    session_id = query.session_id or str(uuid.uuid4())
    user_id = query.user_id or "debug_user"
    
    if session_id not in conversations:
        conversations[session_id] = {
            "messages": [],
            "created_at": datetime.utcnow(),
            "last_accessed": datetime.utcnow()
        }
    else:
        conversations[session_id]["last_accessed"] = datetime.utcnow()
    
    user_question = query.question.strip()
    if not user_question:
        return QueryResponse(
            response=None,
            error="Question cannot be empty.",
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[]
        )
    
    response = process_query(
        user_question, 
        session_id, 
        user_id,
        query.search_scope or "all",
        query.response_style or "balanced",
        query.use_enhanced_rag if query.use_enhanced_rag is not None else True,
        query.document_id
    )
    return response



=== legal_assistant/config.py ===
"""Configuration and environment variables"""
import os
from typing import List

# API Configuration
OPENROUTER_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_API_BASE = os.environ.get("OPENAI_API_BASE", "https://openrouter.ai/api/v1")

# Database Paths
DEFAULT_CHROMA_PATH = os.path.abspath(os.path.join(os.getcwd(), "chromadb-database"))
USER_CONTAINERS_PATH = os.path.abspath(os.path.join(os.getcwd(), "user-containers"))

# File Processing
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
LEGAL_EXTENSIONS = {'.pdf', '.txt', '.docx', '.rtf'}

# External Database Configuration
LEXISNEXIS_API_KEY = os.environ.get("LEXISNEXIS_API_KEY")
LEXISNEXIS_API_ENDPOINT = os.environ.get("LEXISNEXIS_API_ENDPOINT")
WESTLAW_API_KEY = os.environ.get("WESTLAW_API_KEY")
WESTLAW_API_ENDPOINT = os.environ.get("WESTLAW_API_ENDPOINT")

# Model Names
EMBEDDING_MODELS = [
    "nlpaueb/legal-bert-base-uncased",
    "law-ai/InCaseLawBERT", 
    "sentence-transformers/all-mpnet-base-v2",
    "sentence-transformers/all-roberta-large-v1",
    "microsoft/DialoGPT-medium",
    "sentence-transformers/all-MiniLM-L12-v2",
    "all-MiniLM-L6-v2"
]

FAST_EMBEDDING_MODELS = [
    "all-MiniLM-L6-v2",
    "all-MiniLM-L12-v2",
]

# AI Models
AI_MODELS = [
    "moonshotai/kimi-k2:free",
    "deepseek/deepseek-chat",
    "microsoft/phi-3-mini-128k-instruct:free",
    "meta-llama/llama-3.2-3b-instruct:free",
    "google/gemma-2-9b-it:free",
    "mistralai/mistral-7b-instruct:free",
    "openchat/openchat-7b:free"
]

# Chunk Sizes
DEFAULT_CHUNK_SIZE = 1500
LEGISLATIVE_CHUNK_SIZE = 2000
DEFAULT_CHUNK_OVERLAP = 300
LEGISLATIVE_CHUNK_OVERLAP = 500

# Search Settings
DEFAULT_SEARCH_K = 10
ENHANCED_SEARCH_K = 12
COMPREHENSIVE_SEARCH_K = 20
MIN_RELEVANCE_SCORE = 0.15

# Confidence Score Weights
CONFIDENCE_WEIGHTS = {
    "relevance": 0.4,
    "document_count": 0.3,
    "consistency": 0.2,
    "completeness": 0.1
}

# Feature Flags
class FeatureFlags:
    AI_ENABLED: bool = bool(OPENROUTER_API_KEY)
    AIOHTTP_AVAILABLE: bool = False
    OPEN_SOURCE_NLP_AVAILABLE: bool = False
    PYMUPDF_AVAILABLE: bool = False
    PDFPLUMBER_AVAILABLE: bool = False
    UNSTRUCTURED_AVAILABLE: bool = False



=== legal_assistant/core/__init__.py ===
"""Core functionality package"""
from .dependencies import (
    initialize_nlp_models,
    initialize_feature_flags,
    get_nlp,
    get_sentence_model,
    get_embeddings
)
from .security import get_current_user, security
from .exceptions import (
    LegalAssistantException,
    DocumentProcessingError,
    ContainerError,
    RetrievalError,
    AnalysisError,
    AuthenticationError
)

__all__ = [
    'initialize_nlp_models',
    'initialize_feature_flags',
    'get_nlp',
    'get_sentence_model',
    'get_embeddings',
    'get_current_user',
    'security',
    'LegalAssistantException',
    'DocumentProcessingError',
    'ContainerError',
    'RetrievalError',
    'AnalysisError',
    'AuthenticationError'
]



=== legal_assistant/core/dependencies.py ===
"""Dependency injection and initialization"""
import logging
from typing import Optional
import spacy
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from ..config import EMBEDDING_MODELS, FAST_EMBEDDING_MODELS, FeatureFlags

logger = logging.getLogger(__name__)

# Global instances
nlp: Optional[spacy.Language] = None
sentence_model: Optional[SentenceTransformer] = None
embeddings: Optional[HuggingFaceEmbeddings] = None
sentence_model_name: Optional[str] = None

def initialize_nlp_models():
    """Initialize NLP models"""
    global nlp, sentence_model, embeddings, sentence_model_name
    
    # Load spaCy
    try:
        nlp = spacy.load("en_core_web_sm")
        logger.info("‚úÖ spaCy model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}")
        nlp = None
    
    # Load sentence transformer
    for model_name in EMBEDDING_MODELS:
        try:
            sentence_model = SentenceTransformer(model_name)
            sentence_model_name = model_name
            logger.info(f"‚úÖ Loaded powerful sentence model: {model_name}")
            break
        except Exception as e:
            logger.warning(f"Failed to load {model_name}: {e}")
            continue
    
    if sentence_model is None:
        logger.error("‚ùå Failed to load any sentence transformer model")
        sentence_model_name = "none"
    
    # Load embeddings
    try:
        if sentence_model_name and sentence_model_name != "none":
            embeddings = HuggingFaceEmbeddings(model_name=sentence_model_name)
            logger.info(f"‚úÖ Loaded embeddings with: {sentence_model_name}")
        else:
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            logger.info("‚ö†Ô∏è Using fallback embeddings: all-MiniLM-L6-v2")
    except Exception as e:
        logger.error(f"Failed to load embeddings: {e}")
        embeddings = None

def initialize_feature_flags():
    """Initialize feature availability flags"""
    # Check aiohttp
    try:
        import aiohttp
        FeatureFlags.AIOHTTP_AVAILABLE = True
    except ImportError:
        FeatureFlags.AIOHTTP_AVAILABLE = False
        print("‚ö†Ô∏è aiohttp not available - AI features disabled. Install with: pip install aiohttp")
    
    # Check open source NLP
    try:
        import torch
        from transformers import pipeline
        FeatureFlags.OPEN_SOURCE_NLP_AVAILABLE = True
        logger.info("‚úÖ Open-source NLP models available")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Open-source NLP models not available: {e}")
        print("Install with: pip install transformers torch")
    
    # Check PDF processors
    try:
        import fitz
        FeatureFlags.PYMUPDF_AVAILABLE = True
        print("‚úÖ PyMuPDF available - using enhanced PDF extraction")
    except ImportError as e:
        print(f"‚ö†Ô∏è PyMuPDF not available: {e}")
        print("Install with: pip install PyMuPDF")
    
    try:
        import pdfplumber
        FeatureFlags.PDFPLUMBER_AVAILABLE = True
        print("‚úÖ pdfplumber available - using enhanced PDF extraction")
    except ImportError as e:
        print(f"‚ö†Ô∏è pdfplumber not available: {e}")
        print("Install with: pip install pdfplumber")
    
    try:
        from unstructured.partition.auto import partition
        FeatureFlags.UNSTRUCTURED_AVAILABLE = True
        print("‚úÖ Unstructured.io available - using advanced document processing")
    except ImportError as e:
        print(f"‚ö†Ô∏è Unstructured.io not available: {e}")
        print("Install with: pip install unstructured[all-docs]")
    
    # Update AI enabled flag
    FeatureFlags.AI_ENABLED = bool(FeatureFlags.AIOHTTP_AVAILABLE and FeatureFlags.AI_ENABLED)
    
    print(f"Document processing status: PyMuPDF={FeatureFlags.PYMUPDF_AVAILABLE}, pdfplumber={FeatureFlags.PDFPLUMBER_AVAILABLE}, Unstructured={FeatureFlags.UNSTRUCTURED_AVAILABLE}")

def get_nlp():
    """Get NLP instance"""
    return nlp

def get_sentence_model():
    """Get sentence model instance"""
    return sentence_model

def get_embeddings():
    """Get embeddings instance"""
    return embeddings

def get_sentence_model_name():
    """Get sentence model name"""
    return sentence_model_name



=== legal_assistant/core/exceptions.py ===
"""Custom exceptions for the application"""

class LegalAssistantException(Exception):
    """Base exception for all custom exceptions"""
    pass

class DocumentProcessingError(LegalAssistantException):
    """Raised when document processing fails"""
    pass

class ContainerError(LegalAssistantException):
    """Raised when container operations fail"""
    pass

class RetrievalError(LegalAssistantException):
    """Raised when document retrieval fails"""
    pass

class AnalysisError(LegalAssistantException):
    """Raised when analysis operations fail"""
    pass

class AuthenticationError(LegalAssistantException):
    """Raised when authentication fails"""
    pass



=== legal_assistant/core/security.py ===
"""Authentication and authorization"""
from typing import Optional
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from ..models import User
from ..storage.managers import user_sessions

security = HTTPBearer(auto_error=False)

def get_current_user(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> User:
    """Get current user from credentials"""
    if credentials is None:
        default_user_id = "debug_user"
        if default_user_id not in user_sessions:
            from ..services.container_manager import get_container_manager
            container_manager = get_container_manager()
            user_sessions[default_user_id] = User(
                user_id=default_user_id,
                container_id=container_manager.get_container_id(default_user_id),
                subscription_tier="free"
            )
        return user_sessions[default_user_id]
    
    token = credentials.credentials
    user_id = f"user_{token[:8]}"
    
    if user_id not in user_sessions:
        from ..services.container_manager import get_container_manager
        container_manager = get_container_manager()
        user_sessions[user_id] = User(
            user_id=user_id,
            container_id=container_manager.get_container_id(user_id),
            subscription_tier="free"
        )
    
    return user_sessions[user_id]



=== legal_assistant/main.py ===
"""Main FastAPI application entry point"""
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from .config import DEFAULT_CHROMA_PATH, USER_CONTAINERS_PATH, FeatureFlags
from .core import initialize_nlp_models, initialize_feature_flags
from .services import initialize_container_manager
from .api.routers import query, documents, analysis, admin, health

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Create directories
os.makedirs(USER_CONTAINERS_PATH, exist_ok=True)

# Initialize everything
logger.info(f"Using DEFAULT_CHROMA_PATH: {DEFAULT_CHROMA_PATH}")
logger.info(f"Using USER_CONTAINERS_PATH: {USER_CONTAINERS_PATH}")

initialize_feature_flags()
initialize_nlp_models()
initialize_container_manager()

# Create FastAPI app
app = FastAPI(
    title="Unified Legal Assistant API",
    description="Multi-User Legal Assistant with Enhanced RAG, Comprehensive Analysis, and External Database Integration",
    version="10.0.0-SmartRAG-ComprehensiveAnalysis"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(query.router, tags=["queries"])
app.include_router(documents.router, tags=["documents"])
app.include_router(analysis.router, tags=["analysis"])
app.include_router(admin.router, prefix="/admin", tags=["admin"])
app.include_router(health.router, tags=["health"])

# Mount the home page from health router
app.mount("/", health.router)

def create_app():
    """Application factory"""
    return app

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"üöÄ Starting Modular Legal Assistant on port {port}")
    logger.info(f"ChromaDB Path: {DEFAULT_CHROMA_PATH}")
    logger.info(f"User Containers Path: {USER_CONTAINERS_PATH}")
    logger.info(f"AI Status: {'ENABLED' if FeatureFlags.AI_ENABLED else 'DISABLED - Set OPENAI_API_KEY to enable'}")
    logger.info(f"PDF processing: PyMuPDF={FeatureFlags.PYMUPDF_AVAILABLE}, pdfplumber={FeatureFlags.PDFPLUMBER_AVAILABLE}")
    logger.info("Features: Comprehensive analysis, document-specific targeting, container cleanup, enhanced error handling")
    logger.info("Version: 10.0.0-SmartRAG-ComprehensiveAnalysis")
    logger.info("üìÅ MODULAR ARCHITECTURE - Clean separation of concerns!")
    uvicorn.run("legal_assistant.main:app", host="0.0.0.0", port=port, reload=True)



=== legal_assistant/models/__init__.py ===
"""Models package"""
from .api_models import (
    User, Query, QueryResponse, ComprehensiveAnalysisRequest,
    StructuredAnalysisResponse, UserDocumentUpload, DocumentUploadResponse,
    ConversationHistory
)
from .enums import AnalysisType

__all__ = [
    'User', 'Query', 'QueryResponse', 'ComprehensiveAnalysisRequest',
    'StructuredAnalysisResponse', 'UserDocumentUpload', 'DocumentUploadResponse',
    'ConversationHistory', 'AnalysisType'
]



=== legal_assistant/models/api_models.py ===
"""Pydantic models for API requests and responses"""
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from datetime import datetime
from .enums import AnalysisType

class User(BaseModel):
    user_id: str
    email: Optional[str] = None
    container_id: Optional[str] = None
    subscription_tier: str = "free"
    external_db_access: List[str] = []

class Query(BaseModel):
    question: str
    session_id: Optional[str] = None
    response_style: Optional[str] = "balanced"
    user_id: Optional[str] = None
    search_scope: Optional[str] = "all"
    external_databases: Optional[List[str]] = []
    use_enhanced_rag: Optional[bool] = True
    document_id: Optional[str] = None

class QueryResponse(BaseModel):
    response: Optional[str] = None
    error: Optional[str] = None
    context_found: bool = False
    sources: Optional[list] = None
    session_id: str
    confidence_score: float = 0.0
    expand_available: bool = False
    sources_searched: List[str] = []
    retrieval_method: Optional[str] = None

class ComprehensiveAnalysisRequest(BaseModel):
    document_id: Optional[str] = None
    analysis_types: List[AnalysisType] = [AnalysisType.COMPREHENSIVE]
    user_id: str
    session_id: Optional[str] = None
    response_style: str = "detailed"

class StructuredAnalysisResponse(BaseModel):
    document_summary: Optional[str] = None
    key_clauses: Optional[str] = None
    risk_assessment: Optional[str] = None
    timeline_deadlines: Optional[str] = None
    party_obligations: Optional[str] = None
    missing_clauses: Optional[str] = None
    confidence_scores: Dict[str, float] = {}
    sources_by_section: Dict[str, List[Dict]] = {}
    overall_confidence: float = 0.0
    processing_time: float = 0.0
    warnings: List[str] = []
    retrieval_method: str = "comprehensive_analysis"

class UserDocumentUpload(BaseModel):
    user_id: str
    file_id: str
    filename: str
    upload_timestamp: str
    pages_processed: int
    metadata: Dict[str, Any]

class DocumentUploadResponse(BaseModel):
    message: str
    file_id: str
    pages_processed: int
    processing_time: float
    warnings: List[str]
    session_id: str
    user_id: str
    container_id: str

class ConversationHistory(BaseModel):
    session_id: str
    messages: List[Dict[str, Any]]



=== legal_assistant/models/enums.py ===
"""Models package"""
from .api_models import (
    User, Query, QueryResponse, ComprehensiveAnalysisRequest,
    StructuredAnalysisResponse, UserDocumentUpload, DocumentUploadResponse,
    ConversationHistory
)
from .enums import AnalysisType

__all__ = [
    'User', 'Query', 'QueryResponse', 'ComprehensiveAnalysisRequest',
    'StructuredAnalysisResponse', 'UserDocumentUpload', 'DocumentUploadResponse',
    'ConversationHistory', 'AnalysisType'
]



=== legal_assistant/processors/__init__.py ===
"""Processors package"""
from .query_processor import process_query

__all__ = ['process_query']



=== legal_assistant/processors/query_processor.py ===
"""Query processing logic"""
import re
import logging
import traceback
from typing import Optional

from ..models import QueryResponse, ComprehensiveAnalysisRequest, AnalysisType
from ..config import FeatureFlags, OPENROUTER_API_KEY, MIN_RELEVANCE_SCORE
from ..services import (
    ComprehensiveAnalysisProcessor,
    combined_search,
    calculate_confidence_score,
    call_openrouter_api
)
from ..storage.managers import add_to_conversation, get_conversation_context
from ..utils import (
    parse_multiple_questions,
    extract_bill_information,
    extract_universal_information,
    format_context_for_llm
)

logger = logging.getLogger(__name__)

def process_query(question: str, session_id: str, user_id: Optional[str], search_scope: str, 
                 response_style: str = "balanced", use_enhanced_rag: bool = True, 
                 document_id: str = None) -> QueryResponse:
    """Main query processing function"""
    try:
        logger.info(f"Processing query - Question: '{question}', User: {user_id}, Scope: {search_scope}, Enhanced: {use_enhanced_rag}, Document: {document_id}")
        
        if any(phrase in question.lower() for phrase in ["comprehensive analysis", "complete analysis", "full analysis"]):
            logger.info("Detected comprehensive analysis request")
            
            try:
                comp_request = ComprehensiveAnalysisRequest(
                    document_id=document_id,
                    analysis_types=[AnalysisType.COMPREHENSIVE],
                    user_id=user_id or "default_user",
                    session_id=session_id,
                    response_style=response_style
                )
                
                processor = ComprehensiveAnalysisProcessor()
                comp_result = processor.process_comprehensive_analysis(comp_request)
                
                formatted_response = f"""# Comprehensive Legal Document Analysis

## Document Summary
{comp_result.document_summary or 'No summary available'}

## Key Clauses Analysis
{comp_result.key_clauses or 'No clauses analysis available'}

## Risk Assessment
{comp_result.risk_assessment or 'No risk assessment available'}

## Timeline & Deadlines
{comp_result.timeline_deadlines or 'No timeline information available'}

## Party Obligations
{comp_result.party_obligations or 'No obligations analysis available'}

## Missing Clauses Analysis
{comp_result.missing_clauses or 'No missing clauses analysis available'}

---
**Analysis Confidence:** {comp_result.overall_confidence:.1%}
**Processing Time:** {comp_result.processing_time:.2f} seconds

**Sources:** {len(comp_result.sources_by_section.get('summary', []))} document sections analyzed
"""
                
                add_to_conversation(session_id, "user", question)
                add_to_conversation(session_id, "assistant", formatted_response)
                
                return QueryResponse(
                    response=formatted_response,
                    error=None,
                    context_found=True,
                    sources=comp_result.sources_by_section.get('summary', []),
                    session_id=session_id,
                    confidence_score=comp_result.overall_confidence,
                    expand_available=False,
                    sources_searched=["comprehensive_analysis"],
                    retrieval_method=comp_result.retrieval_method
                )
                
            except Exception as e:
                logger.error(f"Comprehensive analysis failed: {e}")
        
        questions = parse_multiple_questions(question) if use_enhanced_rag else [question]
        combined_query = " ".join(questions)
        
        conversation_context = get_conversation_context(session_id)
        
        retrieved_results, sources_searched, retrieval_method = combined_search(
            combined_query, 
            user_id, 
            search_scope, 
            conversation_context,
            use_enhanced=use_enhanced_rag,
            document_id=document_id
        )
        
        if not retrieved_results:
            return QueryResponse(
                response="I couldn't find any relevant information to answer your question in the searched sources.",
                error=None,
                context_found=False,
                sources=[],
                session_id=session_id,
                confidence_score=0.1,
                sources_searched=sources_searched,
                retrieval_method=retrieval_method
            )
        
        # Format context for LLM
        context_text, source_info = format_context_for_llm(retrieved_results)
        
        # Enhanced information extraction
        bill_match = re.search(r"(HB|SB|SSB|ESSB|SHB|ESHB)\s*(\d+)", question, re.IGNORECASE)
        extracted_info = {}

        if bill_match:
            # Bill-specific extraction
            bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
            logger.info(f"Searching for bill: {bill_number}")
            
            # Search specifically for chunks containing this bill
            bill_specific_results = []
            for doc, score in retrieved_results:
                if 'contains_bills' in doc.metadata and bill_number in doc.metadata['contains_bills']:
                    bill_specific_results.append((doc, score))
                    logger.info(f"Found {bill_number} in chunk {doc.metadata.get('chunk_index', 'unknown')} with score {score}")
            
            # If we found bill-specific chunks, prioritize them
            if bill_specific_results:
                logger.info(f"Using {len(bill_specific_results)} bill-specific chunks for {bill_number}")
                # Use the bill-specific chunks with boosted relevance
                boosted_results = [(doc, min(score + 0.3, 1.0)) for doc, score in bill_specific_results]
                retrieved_results = boosted_results + [r for r in retrieved_results if r not in bill_specific_results]
                retrieved_results = retrieved_results[:len(retrieved_results)]
            
            extracted_info = extract_bill_information(context_text, bill_number)
        else:
            # Universal extraction for any document type
            extracted_info = extract_universal_information(context_text, question)

        # Add extracted information to context to make it more visible to AI
        if extracted_info:
            enhancement = "\n\nKEY INFORMATION FOUND:\n"
            for key, value in extracted_info.items():
                if value:  # Only add if there's actual content
                    if isinstance(value, list):
                        enhancement += f"- {key.replace('_', ' ').title()}: {', '.join(value[:5])}\n"
                    else:
                        enhancement += f"- {key.replace('_', ' ').title()}: {value}\n"
            
            if enhancement.strip() != "KEY INFORMATION FOUND:":
                context_text += enhancement
        
        style_instructions = {
            "concise": "Please provide a concise answer (1-2 sentences) based on the context.",
            "balanced": "Please provide a balanced answer (2-3 paragraphs) based on the context.",
            "detailed": "Please provide a detailed answer with explanations based on the context."
        }
        
        instruction = style_instructions.get(response_style, style_instructions["balanced"])
        
        prompt = f"""You are a legal research assistant. Provide thorough, accurate responses based on the provided documents.

STRICT SOURCE REQUIREMENTS:
- Answer ONLY based on the retrieved documents provided in the context
- Do NOT use general legal knowledge, training data, assumptions, or inferences beyond what's explicitly stated
- If information is not in the provided documents, state: "This information is not available in the provided documents"

SOURCES SEARCHED: {', '.join(sources_searched)}
RETRIEVAL METHOD: {retrieval_method}
{f"DOCUMENT FILTER: Specific document {document_id}" if document_id else "DOCUMENT SCOPE: All available documents"}

HALLUCINATION CHECK - Before responding, verify:
1. Is each claim supported by the retrieved documents?
2. Am I adding information not present in the sources?
3. If uncertain, default to "information not available"

INSTRUCTIONS FOR THOROUGH ANALYSIS:
1. **READ CAREFULLY**: Scan the entire context for information that answers the user's question
2. **EXTRACT DIRECTLY**: When information is clearly stated, provide it exactly as written
3. **BE SPECIFIC**: Include names, numbers, dates, and details when present
4. **QUOTE WHEN HELPFUL**: Use direct quotes for key facts or important language
5. **CITE SOURCES**: Reference the document name for each piece of information
6. **BE COMPLETE**: Provide all relevant information found before saying anything is missing
7. **BE HONEST**: Only say information is unavailable when truly absent from the context

LEGAL ANALYSIS MODES:
1. **BASIC LEGAL RESEARCH** - For factual questions about legislation/statutes/regulations
   - Extract statutory/regulatory information, sponsors, dates, provisions
   
2. **COMPREHENSIVE LEGAL ANALYSIS** - For thorough analysis requiring multiple sources
   - Analyze legal implications, compliance obligations, practical impacts
   - Note ambiguities requiring clarification
   
3. **CASE LAW ANALYSIS** - When precedent needed but unavailable, state:
   "This analysis would benefit from relevant case law not available in the current documents."

HANDLING CONFLICTS:
- If documents contain conflicting information, present both views with citations
- Note the conflict explicitly: "Document A states X, while Document B states Y"

WHEN INFORMATION IS MISSING:
"Based on the provided documents, I cannot provide a complete answer. To provide thorough analysis, I would need documents containing: [specific missing elements]"

RESPONSE STYLE: {instruction}

CONVERSATION HISTORY:
{conversation_context}

DOCUMENT CONTEXT (ANALYZE THOROUGHLY):
{context_text}

USER QUESTION:
{questions}

RESPONSE APPROACH:
- **FIRST**: Identify what specific information the user is asking for
- **SECOND**: Search the context thoroughly for that information  
- **THIRD**: Present any information found clearly and completely
- **FOURTH**: Note what information is not available (if any)
- **ALWAYS**: Cite the source document for each fact provided

RESPONSE:"""
        
        if FeatureFlags.AI_ENABLED and OPENROUTER_API_KEY:
            response_text = call_openrouter_api(prompt, OPENROUTER_API_KEY)
        else:
            response_text = f"Based on the retrieved documents:\n\n{context_text}\n\nPlease review this information to answer your question."
        
        relevant_sources = [s for s in source_info if s['relevance'] >= MIN_RELEVANCE_SCORE]
        
        if relevant_sources:
            response_text += "\n\n**SOURCES:**"
            for source in relevant_sources:
                source_type = source['source_type'].replace('_', ' ').title()
                page_info = f", Page {source['page']}" if source['page'] is not None else ""
                response_text += f"\n- [{source_type}] {source['file_name']}{page_info} (Relevance: {source['relevance']:.2f})"
        
        confidence_score = calculate_confidence_score(retrieved_results, len(response_text))
        
        add_to_conversation(session_id, "user", question)
        add_to_conversation(session_id, "assistant", response_text, source_info)
        
        return QueryResponse(
            response=response_text,
            error=None,
            context_found=True,
            sources=source_info,
            session_id=session_id,
            confidence_score=float(confidence_score),
            sources_searched=sources_searched,
            expand_available=len(questions) > 1 if use_enhanced_rag else False,
            retrieval_method=retrieval_method
        )
        
    except Exception as e:
        logger.error(f"Error processing query: {e}")
        traceback.print_exc()
        return QueryResponse(
            response=None,
            error=str(e),
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[],
            retrieval_method="error"
        )



=== legal_assistant/services/__init__.py ===
"""Services package"""
from .document_processor import SafeDocumentProcessor
from .container_manager import UserContainerManager, get_container_manager, initialize_container_manager
from .rag_service import (
    enhanced_retrieval_v2,
    combined_search,
    load_database,
    remove_duplicate_documents,
    calculate_confidence_score
)
from .analysis_service import ComprehensiveAnalysisProcessor
from .ai_service import call_openrouter_api
from .external_db_service import (
    LegalDatabaseInterface,
    LexisNexisInterface,
    WestlawInterface,
    search_external_databases
)

__all__ = [
    'SafeDocumentProcessor',
    'UserContainerManager',
    'get_container_manager',
    'initialize_container_manager',
    'enhanced_retrieval_v2',
    'combined_search',
    'load_database',
    'remove_duplicate_documents',
    'calculate_confidence_score',
    'ComprehensiveAnalysisProcessor',
    'call_openrouter_api',
    'LegalDatabaseInterface',
    'LexisNexisInterface',
    'WestlawInterface',
    'search_external_databases'
]



=== legal_assistant/services/ai_service.py ===
"""AI/LLM integration service"""
import logging
import requests
from typing import Optional
from ..config import AI_MODELS, OPENROUTER_API_KEY, OPENAI_API_BASE

logger = logging.getLogger(__name__)

def call_openrouter_api(prompt: str, api_key: str = None, api_base: str = None) -> str:
    """Call OpenRouter API with fallback models"""
    if not api_key:
        api_key = OPENROUTER_API_KEY
    if not api_base:
        api_base = OPENAI_API_BASE
    
    if not api_key:
        return "I apologize, but AI features are not configured. Please set OPENAI_API_KEY environment variable."
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:8000",
        "X-Title": "Legal Assistant"
    }
    
    for model in AI_MODELS:
        try:
            payload = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.5,
                "max_tokens": 2000
            }
            
            response = requests.post(api_base + "/chat/completions", headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                logger.info(f"‚úÖ Successfully used model: {model}")
                return result['choices'][0]['message']['content'].strip()
                
        except Exception as e:
            logger.error(f"Error with model {model}: {e}")
            continue
    
    return "I apologize, but I'm experiencing technical difficulties. Please try again."



=== legal_assistant/services/analysis_service.py ===
"""Comprehensive analysis service"""
import time
import logging
from typing import Dict, List, Tuple, Optional

from ..models import ComprehensiveAnalysisRequest, StructuredAnalysisResponse, AnalysisType
from ..config import COMPREHENSIVE_SEARCH_K, OPENROUTER_API_KEY, OPENAI_API_BASE
from .container_manager import get_container_manager
from .ai_service import call_openrouter_api
from ..utils.formatting import format_context_for_llm

logger = logging.getLogger(__name__)

class ComprehensiveAnalysisProcessor:
    def __init__(self):
        self.analysis_prompts = {
            "document_summary": "Analyze this document and provide a comprehensive summary including document type, purpose, main parties, key terms, important dates, and financial obligations.",
            "key_clauses": "Extract and analyze key legal clauses including termination, indemnification, liability, governing law, confidentiality, payment terms, and dispute resolution. For each clause, provide specific text references and implications.",
            "risk_assessment": "Identify and assess legal risks including unilateral rights, broad indemnification, unlimited liability, vague obligations, and unfavorable terms. Rate each risk (High/Medium/Low) and suggest mitigation strategies.",
            "timeline_deadlines": "Extract all time-related information including start/end dates, payment deadlines, notice periods, renewal terms, performance deadlines, and warranty periods. Present chronologically.",
            "party_obligations": "List all obligations for each party including what must be done, deadlines, conditions, performance standards, and consequences of non-compliance. Organize by party.",
            "missing_clauses": "Identify commonly expected clauses that may be missing such as force majeure, limitation of liability, dispute resolution, severability, assignment restrictions, and notice provisions. Explain the importance and risks of each missing clause."
        }
    
    def process_comprehensive_analysis(self, request: ComprehensiveAnalysisRequest) -> StructuredAnalysisResponse:
        start_time = time.time()
        
        try:
            search_results, sources_searched, retrieval_method = self._enhanced_document_specific_search(
                request.user_id, 
                request.document_id, 
                "comprehensive legal document analysis",
                k=COMPREHENSIVE_SEARCH_K
            )
            
            if not search_results:
                return StructuredAnalysisResponse(
                    warnings=["No relevant documents found for analysis"],
                    processing_time=time.time() - start_time,
                    retrieval_method="no_documents_found"
                )
            
            context_text, source_info = format_context_for_llm(search_results, max_length=8000)
            
            response = StructuredAnalysisResponse()
            response.sources_by_section = {}
            response.confidence_scores = {}
            response.retrieval_method = retrieval_method
            
            if AnalysisType.COMPREHENSIVE in request.analysis_types:
                comprehensive_prompt = self._create_comprehensive_prompt(context_text)
                
                try:
                    analysis_result = call_openrouter_api(comprehensive_prompt, OPENROUTER_API_KEY, OPENAI_API_BASE)
                    parsed_sections = self._parse_comprehensive_response(analysis_result)
                    
                    response.document_summary = parsed_sections.get("summary", "")
                    response.key_clauses = parsed_sections.get("clauses", "")
                    response.risk_assessment = parsed_sections.get("risks", "")
                    response.timeline_deadlines = parsed_sections.get("timeline", "")
                    response.party_obligations = parsed_sections.get("obligations", "")
                    response.missing_clauses = parsed_sections.get("missing", "")
                    
                    response.overall_confidence = self._calculate_comprehensive_confidence(parsed_sections, len(search_results))
                    
                    for section in ["summary", "clauses", "risks", "timeline", "obligations", "missing"]:
                        response.sources_by_section[section] = source_info
                        response.confidence_scores[section] = response.overall_confidence
                    
                except Exception as e:
                    logger.error(f"Comprehensive analysis failed: {e}")
                    response.warnings.append(f"Comprehensive analysis failed: {str(e)}")
                    response.overall_confidence = 0.1
            
            else:
                for analysis_type in request.analysis_types:
                    section_result = self._process_individual_analysis(analysis_type, context_text, source_info)
                    
                    if analysis_type == AnalysisType.SUMMARY:
                        response.document_summary = section_result["content"]
                    elif analysis_type == AnalysisType.CLAUSES:
                        response.key_clauses = section_result["content"]
                    elif analysis_type == AnalysisType.RISKS:
                        response.risk_assessment = section_result["content"]
                    elif analysis_type == AnalysisType.TIMELINE:
                        response.timeline_deadlines = section_result["content"]
                    elif analysis_type == AnalysisType.OBLIGATIONS:
                        response.party_obligations = section_result["content"]
                    elif analysis_type == AnalysisType.MISSING_CLAUSES:
                        response.missing_clauses = section_result["content"]
                    
                    response.confidence_scores[analysis_type.value] = section_result["confidence"]
                    response.sources_by_section[analysis_type.value] = source_info
                
                confidences = list(response.confidence_scores.values())
                response.overall_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            response.processing_time = time.time() - start_time
            logger.info(f"Comprehensive analysis completed in {response.processing_time:.2f}s with confidence {response.overall_confidence:.2f}")
            
            return response
            
        except Exception as e:
            logger.error(f"Comprehensive analysis processing failed: {e}")
            return StructuredAnalysisResponse(
                warnings=[f"Analysis processing failed: {str(e)}"],
                processing_time=time.time() - start_time,
                overall_confidence=0.0,
                retrieval_method="error"
            )
    
    def _enhanced_document_specific_search(self, user_id: str, document_id: Optional[str], query: str, k: int = 15) -> Tuple[List, List[str], str]:
        all_results = []
        sources_searched = []
        retrieval_method = "enhanced_document_specific"
        
        try:
            container_manager = get_container_manager()
            if document_id:
                user_results = container_manager.enhanced_search_user_container(
                    user_id, query, "", k=k, document_id=document_id
                )
                sources_searched.append(f"document_{document_id}")
                logger.info(f"Document-specific search for {document_id}: {len(user_results)} results")
            else:
                user_results = container_manager.enhanced_search_user_container(
                    user_id, query, "", k=k
                )
                sources_searched.append("all_user_documents")
                logger.info(f"All documents search: {len(user_results)} results")
            
            for doc, score in user_results:
                doc.metadata['source_type'] = 'user_container'
                doc.metadata['search_scope'] = 'document_specific' if document_id else 'all_user_docs'
                all_results.append((doc, score))
            
            return all_results[:k], sources_searched, retrieval_method
            
        except Exception as e:
            logger.error(f"Error in document-specific search: {e}")
            return [], [], "error"
    
    def _create_comprehensive_prompt(self, context_text: str) -> str:
        return f"""You are a legal document analyst. Analyze the provided legal document and provide a comprehensive analysis with the following structured sections.

STRICT SOURCE REQUIREMENTS:
- Answer ONLY based on the retrieved documents provided in the context
- Do NOT use general legal knowledge, training data, assumptions, or inferences beyond what's explicitly stated
- If information is not in the provided documents, state: "This information is not available in the provided documents"

SOURCES SEARCHED: {', '.join(sources_searched)}
RETRIEVAL METHOD: {retrieval_method}
{f"DOCUMENT FILTER: Specific document {document_id}" if document_id else "DOCUMENT SCOPE: All available documents"}

HALLUCINATION CHECK - Before responding, verify:
1. Is each claim supported by the retrieved documents?
2. Am I adding information not present in the sources?
3. If any fact or phrase cannot be traced to a source document, it must not appear in the response.

# INSTRUCTIONS FOR THOROUGH ANALYSIS (Modified)
1. **READ CAREFULLY**: Scan the entire context for information that answers the user's question
2. **EXTRACT COMPLETELY**: When extracting requirements, include FULL details (e.g., "60 minutes" not just "minimum of"). 
3. **QUOTE VERBATIM**: For statutory standards, use exact quotes: `\"[Exact Text]\" (Source)` 
4. **ENUMERATE EXPLICITLY**: Present listed requirements as numbered points with full quotes 
5. **CITE SOURCES**: Reference the document name for each fact 
6. **BE COMPLETE**: Explicitly note missing standards: "Documents lack full subsection [X]" 
7. **USE DECISIVE PHRASING**: State facts directly ("The statute requires...") - NEVER "documents indicate" 

LEGAL ANALYSIS MODES:
1. **BASIC LEGAL RESEARCH** - For factual questions about legislation/statutes/regulations
   - Extract statutory/regulatory information, sponsors, dates, provisions
   
2. **COMPREHENSIVE LEGAL ANALYSIS** - For thorough analysis requiring multiple sources
   - Analyze legal implications, compliance obligations, practical impacts
   - Note ambiguities requiring clarification
   
3. **CASE LAW ANALYSIS** - When precedent needed but unavailable, state:
   "This analysis would benefit from relevant case law not available in the current documents."

HANDLING CONFLICTS:
- If documents contain conflicting information, present both views with citations
- Note the conflict explicitly: "Document A states X, while Document B states Y"

WHEN INFORMATION IS MISSING:
"Based on the provided documents, I cannot provide a complete answer. To provide thorough analysis, I would need documents containing: [specific missing elements]"

RESPONSE STYLE: {instruction}

CONVERSATION HISTORY:
{conversation_context}

DOCUMENT CONTEXT (ANALYZE THOROUGHLY):
{context_text}

USER QUESTION:
{questions}

RESPONSE APPROACH:
- **FIRST**: Identify what specific information the user is asking for. Do not reference any statute, case law, or principle unless it appears verbatim in the context.
- **SECOND**: Search the context thoroughly for that information  
- **THIRD**: Present any information found clearly and completely. At the end of your response, list all facts provided and their source documents for verification.
- **FOURTH**: Note what information is not available (if any)
- **ALWAYS**: Cite the source document for each fact provided

ADDITIONAL GUIDANCE:
- After fully answering based solely on the provided documents, if relevant key legal principles under Washington state law, any other U.S. state law, or U.S. federal law are not found in the sources, you may add a clearly labeled general legal principles disclaimer.
- This disclaimer must clearly state it is NOT based on the provided documents but represents general background knowledge of applicable Washington state, other state, and federal law.
- Do NOT use this disclaimer to answer the user‚Äôs question directly; it serves only as supplementary context.
- This disclaimer must explicitly state that these principles are not found in the provided documents but are usually relevant legal background.
- Format this disclaimer distinctly at the end of the response under a heading such as "GENERAL LEGAL PRINCIPLES DISCLAIMER."

RESPONSE:"""
    
    def _parse_comprehensive_response(self, response_text: str) -> Dict[str, str]:
        sections = {}
        section_markers = {
            "summary": ["## DOCUMENT SUMMARY", "# DOCUMENT SUMMARY"],
            "clauses": ["## KEY CLAUSES ANALYSIS", "# KEY CLAUSES ANALYSIS", "## KEY CLAUSES"],
            "risks": ["## RISK ASSESSMENT", "# RISK ASSESSMENT", "## RISKS"],
            "timeline": ["## TIMELINE & DEADLINES", "# TIMELINE & DEADLINES", "## TIMELINE"],
            "obligations": ["## PARTY OBLIGATIONS", "# PARTY OBLIGATIONS", "## OBLIGATIONS"],
            "missing": ["## MISSING CLAUSES ANALYSIS", "# MISSING CLAUSES ANALYSIS", "## MISSING CLAUSES"]
        }
        
        lines = response_text.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            line_strip = line.strip()
            
            section_found = None
            for section_key, markers in section_markers.items():
                if any(line_strip.startswith(marker) for marker in markers):
                    section_found = section_key
                    break
            
            if section_found:
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                
                current_section = section_found
                current_content = []
            else:
                if current_section:
                    current_content.append(line)
        
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        for section_key in section_markers.keys():
            if section_key not in sections or not sections[section_key]:
                sections[section_key] = f"No {section_key.replace('_', ' ').title()} information found in the analysis."
        
        return sections
    
    def _process_individual_analysis(self, analysis_type: AnalysisType, context_text: str, source_info: List[Dict]) -> Dict:
        try:
            prompt = self.analysis_prompts.get(analysis_type.value, "Analyze this legal document.")
            full_prompt = f"{prompt}\n\nLEGAL DOCUMENT CONTEXT:\n{context_text}\n\nPlease provide a detailed analysis based ONLY on the provided context."
            
            result = call_openrouter_api(full_prompt, OPENROUTER_API_KEY, OPENAI_API_BASE)
            
            return {
                "content": result,
                "confidence": 0.7,
                "sources": source_info
            }
        except Exception as e:
            logger.error(f"Individual analysis failed for {analysis_type}: {e}")
            return {
                "content": f"Analysis failed for {analysis_type.value}: {str(e)}",
                "confidence": 0.1,
                "sources": []
            }
    
    def _calculate_comprehensive_confidence(self, parsed_sections: Dict[str, str], num_sources: int) -> float:
        try:
            successful_sections = sum(1 for content in parsed_sections.values() 
                                    if content and not content.startswith("No ") and len(content) > 50)
            section_factor = successful_sections / len(parsed_sections)
            
            avg_length = sum(len(content) for content in parsed_sections.values()) / len(parsed_sections)
            length_factor = min(1.0, avg_length / 200)
            
            source_factor = min(1.0, num_sources / 5)
            
            confidence = (section_factor * 0.5 + length_factor * 0.3 + source_factor * 0.2)
            return max(0.1, min(1.0, confidence))
            
        except Exception as e:
            logger.error(f"Error calculating confidence: {e}")
            return 0.5



=== legal_assistant/services/container_manager.py ===
"""User container management service"""
import os
import hashlib
import logging
import re
import traceback
from typing import Optional, List, Tuple, Dict
from datetime import datetime

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

from ..config import USER_CONTAINERS_PATH, FAST_EMBEDDING_MODELS
from ..core.exceptions import ContainerError
from ..core.dependencies import get_embeddings, get_nlp
from ..utils.text_processing import remove_duplicate_documents

logger = logging.getLogger(__name__)

class UserContainerManager:
    """Manages user-specific document containers with powerful embeddings"""
    
    def __init__(self, base_path: str):
        self.base_path = base_path
        self.embeddings = None
        self._initialize_embeddings()
        logger.info(f"UserContainerManager initialized with base path: {base_path}")
    
    def _initialize_embeddings(self):
        """Initialize embeddings with the best available model"""
        # Try to use the global embeddings if available
        global_embeddings = get_embeddings()
        
        if global_embeddings:
            self.embeddings = global_embeddings
            logger.info(f"Using global embeddings model")
            return
        
        # TEMPORARY: Use faster embeddings for large document processing
        for model_name in FAST_EMBEDDING_MODELS:
            try:
                self.embeddings = HuggingFaceEmbeddings(model_name=model_name)
                logger.info(f"‚úÖ UserContainerManager using FAST embeddings: {model_name}")
                return
            except Exception as e:
                logger.warning(f"Failed to load {model_name}: {e}")
                continue
        
        # Last resort fallback
        try:
            self.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            logger.warning("‚ö†Ô∏è Using fallback embeddings: all-MiniLM-L6-v2")
        except Exception as e:
            logger.error(f"‚ùå Failed to load any embeddings model: {e}")
            self.embeddings = None
    
    def create_user_container(self, user_id: str) -> str:
        """Create a new container for a user"""
        container_id = hashlib.sha256(user_id.encode()).hexdigest()[:16]
        container_path = os.path.join(self.base_path, container_id)
        os.makedirs(container_path, exist_ok=True)
        
        # Ensure embeddings are available
        if not self.embeddings:
            self._initialize_embeddings()
        
        if not self.embeddings:
            raise ContainerError("No embeddings model available for container creation")
        
        user_db = Chroma(
            collection_name=f"user_{container_id}",
            embedding_function=self.embeddings,
            persist_directory=container_path
        )
        
        logger.info(f"Created container for user {user_id}: {container_id}")
        return container_id
    
    def get_container_id(self, user_id: str) -> str:
        """Get container ID for a user"""
        return hashlib.sha256(user_id.encode()).hexdigest()[:16]
    
    def get_user_database(self, user_id: str) -> Optional[Chroma]:
        """Get user's database"""
        container_id = self.get_container_id(user_id)
        container_path = os.path.join(self.base_path, container_id)
        
        if not os.path.exists(container_path):
            logger.warning(f"Container not found for user {user_id}")
            return None
        
        # Ensure embeddings are available
        if not self.embeddings:
            self._initialize_embeddings()
        
        if not self.embeddings:
            logger.error("No embeddings model available for database access")
            return None
        
        return Chroma(
            collection_name=f"user_{container_id}",
            embedding_function=self.embeddings,
            persist_directory=container_path
        )
    
    def get_user_database_safe(self, user_id: str) -> Optional[Chroma]:
        """Get user database with enhanced error handling and recovery"""
        try:
            container_id = self.get_container_id(user_id)
            container_path = os.path.join(self.base_path, container_id)
            
            if not os.path.exists(container_path):
                logger.warning(f"Container not found for user {user_id}, creating new one")
                self.create_user_container(user_id)
            
            # Ensure embeddings are available
            if not self.embeddings:
                self._initialize_embeddings()
            
            if not self.embeddings:
                logger.error("No embeddings model available for safe database access")
                return None
            
            return Chroma(
                collection_name=f"user_{container_id}",
                embedding_function=self.embeddings,
                persist_directory=container_path
            )
            
        except Exception as e:
            logger.error(f"Error getting user database for {user_id}: {e}")
            try:
                logger.info(f"Attempting to recover by creating new container for {user_id}")
                self.create_user_container(user_id)
                container_id = self.get_container_id(user_id)
                container_path = os.path.join(self.base_path, container_id)
                
                if not self.embeddings:
                    self._initialize_embeddings()
                
                if not self.embeddings:
                    logger.error("No embeddings model available for recovery")
                    return None
                
                return Chroma(
                    collection_name=f"user_{container_id}",
                    embedding_function=self.embeddings,
                    persist_directory=container_path
                )
            except Exception as recovery_error:
                logger.error(f"Recovery failed for user {user_id}: {recovery_error}")
                return None
    
    def add_document_to_container(self, user_id: str, document_text: str, metadata: Dict, file_id: str = None) -> bool:
        """Add document to user's container"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                container_id = self.create_user_container(user_id)
                user_db = self.get_user_database_safe(user_id)
            
            # Smart document type detection - lowered threshold
            bill_count = len(re.findall(r'\b(?:HB|SB|SHB|SSB|ESHB|ESSB)\s+\d+', document_text))
            is_legislative = bill_count > 1  # Lowered from 2 to 1 - even 1-2 bills = legislative
            
            if is_legislative:
                # Legislative document: Bill-aware chunking
                logger.info(f"Detected legislative document with {bill_count} bills - using bill-aware chunking")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=2000,
                    chunk_overlap=500,
                    length_function=len,
                    separators=["\n\n", "\nHB ", "\nSB ", "\nSHB ", "\nSSB ", "\nESHB ", "\nESSB ", "\n", " "]
                )
                chunking_method = 'bill_aware_chunking'
            else:
                # Regular document: Standard semantic chunking
                logger.info(f"Detected regular document ({bill_count} bills found) - using standard chunking")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1500,
                    chunk_overlap=300,
                    length_function=len,
                    separators=["\n\n", "\n", ". ", " ", ""]  # Natural breakpoints
                )
                chunking_method = 'semantic_chunking'
            
            chunks = text_splitter.split_text(document_text)
            logger.info(f"Created {len(chunks)} chunks using {chunking_method}")
            
            # Adjust batch size based on chunk size
            batch_size = 25 if is_legislative else 50
            total_batches = (len(chunks) + batch_size - 1) // batch_size
            
            for batch_num in range(total_batches):
                start_idx = batch_num * batch_size
                end_idx = min(start_idx + batch_size, len(chunks))
                batch_chunks = chunks[start_idx:end_idx]
                
                logger.info(f"Processing batch {batch_num + 1}/{total_batches} ({len(batch_chunks)} chunks)")
                
                documents = []
                for i, chunk in enumerate(batch_chunks):
                    doc_metadata = metadata.copy()
                    doc_metadata['chunk_index'] = start_idx + i
                    doc_metadata['total_chunks'] = len(chunks)
                    doc_metadata['user_id'] = user_id
                    doc_metadata['upload_timestamp'] = datetime.utcnow().isoformat()
                    doc_metadata['chunk_size'] = len(chunk)
                    doc_metadata['chunking_method'] = chunking_method
                    doc_metadata['document_type'] = 'legislative' if is_legislative else 'general'
                    
                    # Extract bill numbers for legislative docs only
                    if is_legislative:
                        bill_numbers = re.findall(r'\b(?:HB|SB|SHB|SSB|ESHB|ESSB)\s+\d+', chunk)
                        if bill_numbers:
                            doc_metadata['contains_bills'] = ', '.join(bill_numbers)
                            logger.info(f"Chunk {start_idx + i} contains bills: {bill_numbers}")
                    
                    if file_id:
                        doc_metadata['file_id'] = file_id
                    
                    # Clean metadata for ChromaDB
                    clean_metadata = {}
                    for key, value in doc_metadata.items():
                        if isinstance(value, (str, int, float, bool)):
                            clean_metadata[key] = value
                        elif isinstance(value, list):
                            clean_metadata[key] = str(value)
                        elif value is None:
                            clean_metadata[key] = ""
                        else:
                            clean_metadata[key] = str(value)
                    
                    documents.append(Document(
                        page_content=chunk,
                        metadata=clean_metadata
                    ))
                
                # Add batch to ChromaDB
                try:
                    user_db.add_documents(documents)
                    logger.info(f"‚úÖ Added batch {batch_num + 1} ({len(documents)} chunks)")
                except Exception as batch_error:
                    logger.error(f"‚ùå Batch {batch_num + 1} failed: {batch_error}")
                    return False
            
            logger.info(f"‚úÖ Successfully added ALL {len(chunks)} chunks for document {file_id or 'unknown'}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error in add_document_to_container: {e}")
            logger.error(f"Error type: {type(e).__name__}")
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return False
    
    def search_user_container(self, user_id: str, query: str, k: int = 5, document_id: str = None) -> List[Tuple]:
        """Search with timeout protection"""
        return self.search_user_container_safe(user_id, query, k, document_id)
    
    def search_user_container_safe(self, user_id: str, query: str, k: int = 5, document_id: str = None) -> List[Tuple]:
        """Search with enhanced error handling and timeout protection"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                logger.warning(f"No database available for user {user_id}")
                return []
            
            filter_dict = None
            if document_id:
                filter_dict = {"file_id": document_id}
            
            try:
                results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                return results
            except Exception as search_error:
                logger.warning(f"Search failed for user {user_id}: {search_error}")
                return []
                
        except Exception as e:
            logger.error(f"Error in safe container search for user {user_id}: {e}")
            return []
    
    def enhanced_search_user_container(self, user_id: str, query: str, conversation_context: str, k: int = 12, document_id: str = None) -> List[Tuple]:
        """Enhanced search with timeout protection and bill-specific optimization"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                return []
            
            filter_dict = None
            if document_id:
                filter_dict = {"file_id": document_id}
            
            try:
                # Check if this is a bill-specific query
                bill_match = re.search(r"\b(HB|SB|SSB|ESSB|SHB|ESHB)\s+(\d+)\b", query, re.IGNORECASE)
                
                if bill_match:
                    bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
                    logger.info(f"Bill-specific search for: {bill_number}")
                    
                    # First, try to find chunks that contain this specific bill
                    try:
                        all_docs = user_db.get()
                        bill_specific_chunks = []
                        
                        for i, (doc_id, metadata, content) in enumerate(zip(all_docs['ids'], all_docs['metadatas'], all_docs['documents'])):
                            if metadata and 'contains_bills' in metadata:
                                if bill_number in metadata['contains_bills']:
                                    # Create a document object for this chunk
                                    doc_obj = Document(page_content=content, metadata=metadata)
                                    # Use a high relevance score since we found exact bill match
                                    bill_specific_chunks.append((doc_obj, 0.95))  # High relevance for exact matches
                                    logger.info(f"Found {bill_number} in chunk {metadata.get('chunk_index')} with boosted score")
                        
                        if bill_specific_chunks:
                            logger.info(f"Using {len(bill_specific_chunks)} bill-specific chunks with high relevance")
                            # Get additional context chunks with lower threshold
                            regular_results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                            
                            # Combine bill-specific (high score) with regular results
                            all_results = bill_specific_chunks + regular_results
                            return remove_duplicate_documents(all_results)[:k]
                    except Exception as bill_search_error:
                        logger.warning(f"Bill-specific search failed, falling back to regular search: {bill_search_error}")
                        # Fall through to regular search
                
                # Fallback to regular search
                direct_results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                expanded_query = f"{query} {conversation_context}"
                expanded_results = user_db.similarity_search_with_score(expanded_query, k=k, filter=filter_dict)
                
                sub_query_results = []
                nlp = get_nlp()
                if nlp:
                    doc = nlp(query)
                    for ent in doc.ents:
                        if ent.label_ in ["ORG", "PERSON", "LAW", "DATE"]:
                            sub_results = user_db.similarity_search_with_score(f"What is {ent.text}?", k=3, filter=filter_dict)
                            sub_query_results.extend(sub_results)
                
                all_results = direct_results + expanded_results + sub_query_results
                return remove_duplicate_documents(all_results)[:k]
                
            except Exception as search_error:
                logger.warning(f"Enhanced search failed for user {user_id}: {search_error}")
                return []
            
        except Exception as e:
            logger.error(f"Error in enhanced user container search: {e}")
            return []

# Global instance
_container_manager = None

def initialize_container_manager():
    """Initialize the global container manager"""
    global _container_manager
    _container_manager = UserContainerManager(USER_CONTAINERS_PATH)
    return _container_manager

def get_container_manager() -> UserContainerManager:
    """Get the global container manager instance"""
    if _container_manager is None:
        return initialize_container_manager()
    return _container_manager



=== legal_assistant/services/document_processor.py ===
"""Document processing service"""
import os
import io
import logging
import tempfile
from typing import Tuple, List
from ..config import FeatureFlags
from ..core.exceptions import DocumentProcessingError

logger = logging.getLogger(__name__)

class SafeDocumentProcessor:
    """Safe document processor for various file types with enhanced processing capabilities"""
    
    @staticmethod
    def process_document_safe(file) -> Tuple[str, int, List[str]]:
        """
        Process uploaded document safely with enhanced processing
        Returns: (content, pages_processed, warnings)
        """
        warnings = []
        content = ""
        pages_processed = 0
        
        try:
            filename = getattr(file, 'filename', 'unknown')
            file_ext = os.path.splitext(filename)[1].lower()
            file_content = file.file.read()
            
            if file_ext == '.txt':
                content = file_content.decode('utf-8', errors='ignore')
                pages_processed = SafeDocumentProcessor._estimate_pages_from_text(content)
            elif file_ext == '.pdf':
                content, pages_processed = SafeDocumentProcessor._process_pdf_enhanced(file_content, warnings)
            elif file_ext == '.docx':
                content, pages_processed = SafeDocumentProcessor._process_docx_enhanced(file_content, warnings)
            else:
                try:
                    content = file_content.decode('utf-8', errors='ignore')
                    pages_processed = SafeDocumentProcessor._estimate_pages_from_text(content)
                    warnings.append(f"File type {file_ext} processed as plain text")
                except Exception as e:
                    warnings.append(f"Could not process file: {str(e)}")
                    content = "Unable to process this file type"
                    pages_processed = 0
            
            file.file.seek(0)
            
        except Exception as e:
            warnings.append(f"Error processing document: {str(e)}")
            content = "Error processing document"
            pages_processed = 0
        
        return content, pages_processed, warnings
    
    @staticmethod
    def _estimate_pages_from_text(text: str) -> int:
        """Fixed page estimation based on content analysis"""
        if not text:
            return 0
        
        # Enhanced page estimation logic
        word_count = len(text.split())
        char_count = len(text)
        line_count = len(text.split('\n'))
        
        # Average words per page: 250-500 (legal documents tend to be dense)
        pages_by_words = max(1, word_count // 350)
        
        # Average characters per page: 1500-3000 (including spaces)
        pages_by_chars = max(1, char_count // 2000)
        
        # For documents with many line breaks (structured content)
        pages_by_lines = max(1, line_count // 50)
        
        # Use the median of the three estimates for better accuracy
        estimates = [pages_by_words, pages_by_chars, pages_by_lines]
        estimates.sort()
        estimated_pages = estimates[1]  # median
        
        # Ensure reasonable bounds
        return max(1, min(estimated_pages, 1000))
    
    @staticmethod
    def _process_pdf_enhanced(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Enhanced PDF processing with Unstructured.io fallback"""
        # Try Unstructured.io first for best results
        if FeatureFlags.UNSTRUCTURED_AVAILABLE:
            try:
                from unstructured.partition.auto import partition
                
                with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                    temp_file.write(file_content)
                    temp_file_path = temp_file.name
                
                try:
                    # Use Unstructured.io for advanced processing
                    elements = partition(filename=temp_file_path)
                    
                    # Extract text and structure
                    text_content = ""
                    page_count = 0
                    
                    for element in elements:
                        if hasattr(element, 'text') and element.text:
                            text_content += element.text + "\n"
                        
                        # Try to get page information
                        if hasattr(element, 'metadata') and element.metadata:
                            if 'page_number' in element.metadata:
                                page_count = max(page_count, element.metadata['page_number'])
                    
                    # Clean up temp file
                    os.unlink(temp_file_path)
                    
                    if page_count == 0:
                        page_count = SafeDocumentProcessor._estimate_pages_from_text(text_content)
                    
                    return text_content, page_count
                    
                except Exception as e:
                    os.unlink(temp_file_path)
                    warnings.append(f"Unstructured.io processing failed: {str(e)}, falling back to PyMuPDF")
                    
            except Exception as e:
                warnings.append(f"Unstructured.io setup failed: {str(e)}")
        
        # Fallback to existing PDF processing
        return SafeDocumentProcessor._process_pdf(file_content, warnings)
    
    @staticmethod
    def _process_pdf(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process PDF content with enhanced page counting"""
        try:
            if FeatureFlags.PYMUPDF_AVAILABLE:
                try:
                    import fitz
                    doc = fitz.open(stream=file_content, filetype="pdf")
                    text_content = ""
                    pages = len(doc)
                    for page_num in range(pages):
                        page = doc.load_page(page_num)
                        text_content += page.get_text()
                    doc.close()
                    return text_content, pages
                except Exception as e:
                    warnings.append(f"PyMuPDF error: {str(e)}")
            
            if FeatureFlags.PDFPLUMBER_AVAILABLE:
                try:
                    import pdfplumber
                    with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                        text_content = ""
                        pages = len(pdf.pages)
                        for page in pdf.pages:
                            text_content += page.extract_text() or ""
                    return text_content, pages
                except Exception as e:
                    warnings.append(f"pdfplumber error: {str(e)}")
            
            warnings.append("No PDF processing libraries available. Install PyMuPDF or pdfplumber.")
            return "PDF processing not available", 0
            
        except Exception as e:
            warnings.append(f"Error processing PDF: {str(e)}")
            return "Error processing PDF", 0
    
    @staticmethod
    def _process_docx_enhanced(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Enhanced DOCX processing with better page estimation"""
        if FeatureFlags.UNSTRUCTURED_AVAILABLE:
            try:
                from unstructured.partition.auto import partition
                
                with tempfile.NamedTemporaryFile(suffix='.docx', delete=False) as temp_file:
                    temp_file.write(file_content)
                    temp_file_path = temp_file.name
                
                try:
                    # Use Unstructured.io for advanced processing
                    elements = partition(filename=temp_file_path)
                    
                    text_content = ""
                    for element in elements:
                        if hasattr(element, 'text') and element.text:
                            text_content += element.text + "\n"
                    
                    os.unlink(temp_file_path)
                    
                    pages_estimated = SafeDocumentProcessor._estimate_pages_from_text(text_content)
                    return text_content, pages_estimated
                    
                except Exception as e:
                    os.unlink(temp_file_path)
                    warnings.append(f"Unstructured.io DOCX processing failed: {str(e)}")
                    
            except Exception as e:
                warnings.append(f"Unstructured.io DOCX setup failed: {str(e)}")
        
        # Fallback to existing DOCX processing
        return SafeDocumentProcessor._process_docx(file_content, warnings)
    
    @staticmethod
    def _process_docx(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process DOCX content with enhanced page estimation"""
        try:
            try:
                from docx import Document
                doc = Document(io.BytesIO(file_content))
                text_content = ""
                for paragraph in doc.paragraphs:
                    text_content += paragraph.text + "\n"
                
                # Enhanced page estimation for DOCX
                pages_estimated = SafeDocumentProcessor._estimate_pages_from_text(text_content)
                return text_content, pages_estimated
                
            except ImportError:
                warnings.append("python-docx not available. Install with: pip install python-docx")
                return "DOCX processing not available", 0
            except Exception as e:
                warnings.append(f"Error processing DOCX: {str(e)}")
                return "Error processing DOCX", 0
                
        except Exception as e:
            warnings.append(f"Error processing DOCX: {str(e)}")
            return "Error processing DOCX", 0



=== legal_assistant/services/external_db_service.py ===
"""External legal database integration service"""
import logging
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
from ..config import LEXISNEXIS_API_KEY, LEXISNEXIS_API_ENDPOINT, WESTLAW_API_KEY, WESTLAW_API_ENDPOINT
from ..models import User

logger = logging.getLogger(__name__)

class LegalDatabaseInterface(ABC):
    """Abstract interface for external legal databases"""
    
    @abstractmethod
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        pass
    
    @abstractmethod
    def get_document(self, document_id: str) -> Dict:
        pass
    
    @abstractmethod
    def authenticate(self, credentials: Dict) -> bool:
        pass

class LexisNexisInterface(LegalDatabaseInterface):
    def __init__(self, api_key: str = None, api_endpoint: str = None):
        self.api_key = api_key or LEXISNEXIS_API_KEY
        self.api_endpoint = api_endpoint or LEXISNEXIS_API_ENDPOINT
        self.authenticated = False
    
    def authenticate(self, credentials: Dict) -> bool:
        logger.info("LexisNexis authentication placeholder")
        return False
    
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        logger.info(f"LexisNexis search placeholder for query: {query}")
        return []
    
    def get_document(self, document_id: str) -> Dict:
        logger.info(f"LexisNexis document retrieval placeholder for ID: {document_id}")
        return {}

class WestlawInterface(LegalDatabaseInterface):
    def __init__(self, api_key: str = None, api_endpoint: str = None):
        self.api_key = api_key or WESTLAW_API_KEY
        self.api_endpoint = api_endpoint or WESTLAW_API_ENDPOINT
        self.authenticated = False
    
    def authenticate(self, credentials: Dict) -> bool:
        logger.info("Westlaw authentication placeholder")
        return False
    
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        logger.info(f"Westlaw search placeholder for query: {query}")
        return []
    
    def get_document(self, document_id: str) -> Dict:
        logger.info(f"Westlaw document retrieval placeholder for ID: {document_id}")
        return {}

# External databases
external_databases = {
    "lexisnexis": LexisNexisInterface(),
    "westlaw": WestlawInterface()
}

def search_external_databases(query: str, databases: List[str], user: User) -> List[Dict]:
    """Search external legal databases"""
    results = []
    
    for db_name in databases:
        if db_name not in user.external_db_access:
            logger.warning(f"User {user.user_id} does not have access to {db_name}")
            continue
        
        if db_name in external_databases:
            db_interface = external_databases[db_name]
            try:
                db_results = db_interface.search(query)
                for result in db_results:
                    result['source_database'] = db_name
                    results.extend(db_results)
            except Exception as e:
                logger.error(f"Error searching {db_name}: {e}")
    
    return results



=== legal_assistant/services/rag_service.py ===
"""RAG (Retrieval-Augmented Generation) operations service"""
import os
import logging
import numpy as np
from typing import List, Tuple, Dict, Optional

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from ..config import DEFAULT_CHROMA_PATH, DEFAULT_SEARCH_K, ENHANCED_SEARCH_K, CONFIDENCE_WEIGHTS
from ..core.dependencies import get_nlp
from ..core.exceptions import RetrievalError
from .container_manager import get_container_manager
from ..utils.text_processing import remove_duplicate_documents

logger = logging.getLogger(__name__)

def load_database():
    """Load the default database"""
    try:
        if not os.path.exists(DEFAULT_CHROMA_PATH):
            logger.warning(f"Default database path does not exist: {DEFAULT_CHROMA_PATH}")
            return None
        
        embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        db = Chroma(
            collection_name="default",
            embedding_function=embedding_function,
            persist_directory=DEFAULT_CHROMA_PATH
        )
        logger.debug("Default database loaded successfully")
        return db
    except Exception as e:
        logger.error(f"Failed to load default database: {e}")
        raise RetrievalError(f"Failed to load default database: {str(e)}")

def enhanced_retrieval_v2(db, query_text: str, conversation_history_context: str, k: int = ENHANCED_SEARCH_K, document_filter: Dict = None) -> Tuple[List, str]:
    """Enhanced retrieval with multiple strategies"""
    logger.info(f"[ENHANCED_RETRIEVAL] Original query: '{query_text}'")
    
    try:
        direct_results = db.similarity_search_with_score(query_text, k=k, filter=document_filter)
        logger.info(f"[ENHANCED_RETRIEVAL] Direct search returned {len(direct_results)} results")
        
        expanded_query = f"{query_text} {conversation_history_context}"
        expanded_results = db.similarity_search_with_score(expanded_query, k=k, filter=document_filter)
        logger.info(f"[ENHANCED_RETRIEVAL] Expanded search returned {len(expanded_results)} results")
        
        sub_queries = []
        nlp = get_nlp()
        if nlp:
            doc = nlp(query_text)
            for ent in doc.ents:
                if ent.label_ in ["ORG", "PERSON", "LAW", "DATE"]:
                    sub_queries.append(f"What is {ent.text}?")
        
        if not sub_queries:
            question_words = ["what", "who", "when", "where", "why", "how"]
            for word in question_words:
                if word in query_text.lower():
                    sub_queries.append(f"{word.capitalize()} {query_text.lower().replace(word, '').strip()}?")
        
        sub_query_results = []
        for sq in sub_queries[:3]:
            sq_results = db.similarity_search_with_score(sq, k=3, filter=document_filter)
            sub_query_results.extend(sq_results)
        
        logger.info(f"[ENHANCED_RETRIEVAL] Sub-query search returned {len(sub_query_results)} results")
        
        all_results = direct_results + expanded_results + sub_query_results
        unique_results = remove_duplicate_documents(all_results)
        top_results = unique_results[:k]
        
        logger.info(f"[ENHANCED_RETRIEVAL] Final results after deduplication: {len(top_results)}")
        return top_results, "enhanced_retrieval_v2"
        
    except Exception as e:
        logger.error(f"[ENHANCED_RETRIEVAL] Error in enhanced retrieval: {e}")
        basic_results = db.similarity_search_with_score(query_text, k=k, filter=document_filter)
        return basic_results, "basic_fallback"

def combined_search(query: str, user_id: Optional[str], search_scope: str, conversation_context: str, 
                   use_enhanced: bool = True, k: int = DEFAULT_SEARCH_K, document_id: str = None) -> Tuple[List, List[str], str]:
    """Combined search across all sources"""
    all_results = []
    sources_searched = []
    retrieval_method = "basic"
    
    if search_scope in ["all", "default_only"]:
        try:
            default_db = load_database()
            if default_db:
                if use_enhanced:
                    default_results, method = enhanced_retrieval_v2(default_db, query, conversation_context, k=k)
                    retrieval_method = method
                else:
                    default_results = default_db.similarity_search_with_score(query, k=k)
                    retrieval_method = "basic_search"
                
                for doc, score in default_results:
                    doc.metadata['source_type'] = 'default_database'
                    all_results.append((doc, score))
                sources_searched.append("default_database")
        except Exception as e:
            logger.error(f"Error searching default database: {e}")
    
    if user_id and search_scope in ["all", "user_only"]:
        try:
            container_manager = get_container_manager()
            if use_enhanced:
                user_results = container_manager.enhanced_search_user_container(user_id, query, conversation_context, k=k, document_id=document_id)
            else:
                user_results = container_manager.search_user_container(user_id, query, k=k, document_id=document_id)
            
            for doc, score in user_results:
                doc.metadata['source_type'] = 'user_container'
                all_results.append((doc, score))
            if user_results:
                sources_searched.append("user_container")
        except Exception as e:
            logger.error(f"Error searching user container: {e}")
    
    if use_enhanced:
        all_results = remove_duplicate_documents(all_results)
    else:
        all_results.sort(key=lambda x: x[1], reverse=True)
    
    return all_results[:k], sources_searched, retrieval_method

def calculate_confidence_score(results_with_scores: List[Tuple], response_length: int) -> float:
    """Calculate confidence score for results"""
    try:
        if not results_with_scores:
            return 0.2
        
        scores = [score for _, score in results_with_scores]
        avg_relevance = np.mean(scores)
        doc_factor = min(1.0, len(results_with_scores) / 5.0)
        
        if len(scores) > 1:
            score_std = np.std(scores)
            consistency_factor = max(0.5, 1.0 - score_std)
        else:
            consistency_factor = 0.7
            
        completeness_factor = min(1.0, response_length / 500.0)
        
        confidence = (
            avg_relevance * CONFIDENCE_WEIGHTS["relevance"] +
            doc_factor * CONFIDENCE_WEIGHTS["document_count"] +
            consistency_factor * CONFIDENCE_WEIGHTS["consistency"] +
            completeness_factor * CONFIDENCE_WEIGHTS["completeness"]
        )
        
        confidence = max(0.0, min(1.0, confidence))
        return confidence
    
    except Exception as e:
        logger.error(f"Error calculating confidence score: {e}")
        return 0.5



=== legal_assistant/storage/__init__.py ===
"""Storage package"""
from .managers import (
    conversations,
    uploaded_files,
    user_sessions,
    add_to_conversation,
    get_conversation_context,
    cleanup_expired_conversations
)

__all__ = [
    'conversations',
    'uploaded_files',
    'user_sessions',
    'add_to_conversation',
    'get_conversation_context',
    'cleanup_expired_conversations'
]



=== legal_assistant/storage/managers.py ===
"""Global state management"""
from typing import Dict, Optional, List, Any
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

# Global state stores
conversations: Dict[str, Dict] = {}
uploaded_files: Dict[str, Dict] = {}
user_sessions: Dict[str, Any] = {}

def add_to_conversation(session_id: str, role: str, content: str, sources: Optional[List] = None):
    """Add message to conversation"""
    if session_id not in conversations:
        conversations[session_id] = {
            'messages': [],
            'created_at': datetime.utcnow(),
            'last_accessed': datetime.utcnow()
        }
    
    message = {
        'role': role,
        'content': content,
        'timestamp': datetime.utcnow().isoformat(),
        'sources': sources or []
    }
    
    conversations[session_id]['messages'].append(message)
    conversations[session_id]['last_accessed'] = datetime.utcnow()

def get_conversation_context(session_id: str, max_length: int = 2000) -> str:
    """Get conversation context for a session"""
    if session_id not in conversations:
        return ""
    
    messages = conversations[session_id]['messages']
    context_parts = []
    recent_messages = messages[-4:]
    
    for msg in recent_messages:
        role = msg['role'].upper()
        content = msg['content']
        if len(content) > 800:
            content = content[:800] + "..."
        context_parts.append(f"{role}: {content}")
    
    if context_parts:
        return "Previous conversation:\n" + "\n".join(context_parts)
    return ""

def cleanup_expired_conversations():
    """Clean up expired conversations"""
    now = datetime.utcnow()
    expired_sessions = [
        session_id for session_id, data in conversations.items()
        if now - data['last_accessed'] > timedelta(hours=1)
    ]
    for session_id in expired_sessions:
        del conversations[session_id]
    if expired_sessions:
        logger.info(f"Cleaned up {len(expired_sessions)} expired conversations")



=== legal_assistant/utils/__init__.py ===
"""Utilities package"""
from .text_processing import (
    parse_multiple_questions,
    semantic_chunking_with_bert,
    basic_text_chunking,
    remove_duplicate_documents,
    extract_bill_information,
    extract_universal_information
)
from .formatting import format_context_for_llm

__all__ = [
    'parse_multiple_questions',
    'semantic_chunking_with_bert',
    'basic_text_chunking',
    'remove_duplicate_documents',
    'extract_bill_information',
    'extract_universal_information',
    'format_context_for_llm'
]



=== legal_assistant/utils/formatting.py ===
"""Formatting utilities"""
import os
from typing import List, Tuple
from ..config import MIN_RELEVANCE_SCORE

def format_context_for_llm(results_with_scores: List[Tuple], max_length: int = 3000) -> Tuple[str, List]:
    """Format search results for LLM context"""
    context_parts = []
    source_info = []
    
    total_length = 0
    for i, (doc, score) in enumerate(results_with_scores):
        if total_length >= max_length:
            break
            
        content = doc.page_content.strip()
        metadata = doc.metadata
        
        source_path = metadata.get('source', 'unknown_source')
        page = metadata.get('page', None)
        source_type = metadata.get('source_type', 'unknown')
        
        display_source = os.path.basename(source_path)
        page_info = f" (Page {page})" if page is not None else ""
        source_prefix = f"[{source_type.upper()}]" if source_type != 'unknown' else ""
        
        if len(content) > 800:
            content = content[:800] + "... [truncated]"
            
        context_part = f"{source_prefix} [{display_source}{page_info}] (Relevance: {score:.2f}): {content}"
        context_parts.append(context_part)
        
        source_info.append({
            'id': i+1,
            'file_name': display_source,
            'page': page,
            'relevance': score,
            'full_path': source_path,
            'source_type': source_type
        })
        
        total_length += len(context_part)
    
    context_text = "\n\n".join(context_parts)
    return context_text, source_info



=== legal_assistant/utils/text_processing.py ===
"""Text processing utilities"""
import re
import logging
import numpy as np
from typing import List, Tuple, Dict, Any
from ..core.dependencies import get_sentence_model, get_sentence_model_name

logger = logging.getLogger(__name__)

def parse_multiple_questions(query_text: str) -> List[str]:
    """Parse multiple questions from a single query"""
    questions = []
    
    if ';' in query_text:
        parts = query_text.split(';')
        for part in parts:
            part = part.strip()
            if part:
                questions.append(part)
    elif '?' in query_text and query_text.count('?') > 1:
        parts = query_text.split('?')
        for part in parts:
            part = part.strip()
            if part:
                questions.append(part + '?')
    else:
        final_question = query_text
        if not final_question.endswith('?') and '?' not in final_question:
            final_question += '?'
        questions = [final_question]
    
    return questions

def semantic_chunking_with_bert(text: str, max_chunk_size: int = 1500, overlap: int = 300) -> List[str]:
    """Advanced semantic chunking with powerful BERT models for legal documents"""
    try:
        sentence_model = get_sentence_model()
        sentence_model_name = get_sentence_model_name()
        
        if sentence_model is None:
            logger.warning("No sentence model available, using basic chunking")
            return basic_text_chunking(text, max_chunk_size, overlap)
        
        logger.info(f"Using semantic chunking with model: {sentence_model_name}")
        
        # For legal documents, split on legal sections and paragraphs
        # Look for common legal document patterns
        legal_patterns = [
            r'\n\s*SECTION\s+\d+',
            r'\n\s*\d+\.\s+',  # Numbered sections
            r'\n\s*\([a-z]\)',  # Subsections (a), (b), etc.
            r'\n\s*WHEREAS',
            r'\n\s*NOW, THEREFORE',
            r'\n\s*Article\s+[IVX\d]+',
        ]
        
        # Split text into meaningful sections first
        sections = []
        current_pos = 0
        
        # Find legal section breaks
        for pattern in legal_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            for match in matches:
                if match.start() > current_pos:
                    section_text = text[current_pos:match.start()].strip()
                    if section_text:
                        sections.append(section_text)
                current_pos = match.start()
        
        # Add remaining text
        if current_pos < len(text):
            remaining_text = text[current_pos:].strip()
            if remaining_text:
                sections.append(remaining_text)
        
        # If no legal patterns found, fall back to paragraph splitting
        if not sections:
            sections = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not sections:
            sections = [text]
        
        # If document is small enough, return as single chunk
        if len(text) <= max_chunk_size:
            return [text]
        
        # Calculate embeddings for sections (batch processing for efficiency)
        try:
            section_embeddings = sentence_model.encode(sections, batch_size=32, show_progress_bar=False)
        except Exception as e:
            logger.warning(f"Embedding calculation failed: {e}, using basic chunking")
            return basic_text_chunking(text, max_chunk_size, overlap)
        
        # Advanced semantic grouping using cosine similarity
        chunks = []
        current_chunk = []
        current_chunk_size = 0
        
        for i, section in enumerate(sections):
            section_size = len(section)
            
            # If adding this section would exceed chunk size
            if current_chunk_size + section_size > max_chunk_size and current_chunk:
                
                # For legal documents, try to find natural breaking points
                if len(current_chunk) > 1:
                    # Calculate semantic similarity to decide on best split point
                    chunk_text = '\n\n'.join(current_chunk)
                    chunks.append(chunk_text)
                    
                    # Intelligent overlap: keep semantically similar content
                    if i > 0:
                        # Use similarity to determine overlap
                        prev_embedding = section_embeddings[i-1:i]
                        curr_embedding = section_embeddings[i:i+1]
                        
                        try:
                            similarity = np.dot(prev_embedding[0], curr_embedding[0])
                            if similarity > 0.7:  # High similarity - include more overlap
                                overlap_sections = current_chunk[-2:] if len(current_chunk) > 1 else current_chunk[-1:]
                            else:
                                overlap_sections = current_chunk[-1:] if current_chunk else []
                            
                            current_chunk = overlap_sections + [section]
                            current_chunk_size = sum(len(s) for s in current_chunk)
                        except:
                            # Fallback to simple overlap
                            current_chunk = [current_chunk[-1], section] if current_chunk else [section]
                            current_chunk_size = sum(len(s) for s in current_chunk)
                    else:
                        current_chunk = [section]
                        current_chunk_size = section_size
                else:
                    # Single large section - need to split it
                    if section_size > max_chunk_size:
                        # Split large section into smaller parts
                        large_section_chunks = basic_text_chunking(section, max_chunk_size, overlap)
                        chunks.extend(large_section_chunks[:-1])  # Add all but last
                        current_chunk = [large_section_chunks[-1]]  # Keep last for next iteration
                        current_chunk_size = len(large_section_chunks[-1])
                    else:
                        chunks.append(section)
                        current_chunk = []
                        current_chunk_size = 0
            else:
                current_chunk.append(section)
                current_chunk_size += section_size
        
        # Add remaining chunk
        if current_chunk:
            chunk_text = '\n\n'.join(current_chunk)
            chunks.append(chunk_text)
        
        # Ensure we have at least one chunk
        if not chunks:
            chunks = [text[:max_chunk_size]]
        
        logger.info(f"Semantic chunking created {len(chunks)} chunks from {len(sections)} sections")
        return chunks
        
    except Exception as e:
        logger.error(f"Advanced semantic chunking failed: {e}, falling back to basic chunking")
        return basic_text_chunking(text, max_chunk_size, overlap)

def basic_text_chunking(text: str, max_chunk_size: int = 1500, overlap: int = 300) -> List[str]:
    """Basic text chunking fallback"""
    if len(text) <= max_chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + max_chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at a sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        
        # Find the best breaking point
        break_point = max(last_period, last_newline)
        if break_point > start + max_chunk_size // 2:  # Only if break point is reasonable
            end = start + break_point + 1
        
        chunks.append(text[start:end])
        start = end - overlap  # Add overlap
    
    return chunks

def remove_duplicate_documents(results_with_scores: List[Tuple]) -> List[Tuple]:
    """Remove duplicate documents from search results"""
    if not results_with_scores:
        return []
    
    unique_results = []
    seen_content = set()
    
    for doc, score in results_with_scores:
        content_hash = hash(doc.page_content[:100])
        if content_hash not in seen_content:
            seen_content.add(content_hash)
            unique_results.append((doc, score))
    
    unique_results.sort(key=lambda x: x[1], reverse=True)
    return unique_results

def extract_bill_information(context_text: str, bill_number: str) -> Dict[str, str]:
    """Pre-extract bill information using regex patterns"""
    extracted_info = {}
    
    # Enhanced pattern to find bill information with more context
    bill_patterns = [
        rf"{bill_number}[^\n]*(?:\n(?:[^\n]*(?:sponsors?|final\s+status|enables|authorizes|establishes)[^\n]*\n?)*)",
        rf"{bill_number}.*?(?=\n\s*[A-Z]{{2,}}|\n\s*[A-Z]{{1,3}}\s+\d+|\Z)",
        rf"{bill_number}[^\n]*\n(?:[^\n]+\n?){{0,5}}"
    ]
    
    for pattern in bill_patterns:
        bill_match = re.search(pattern, context_text, re.DOTALL | re.IGNORECASE)
        if bill_match:
            bill_text = bill_match.group(0)
            logger.info(f"Found bill text for {bill_number}: {bill_text[:200]}...")
            
            # Extract sponsors with multiple patterns
            sponsor_patterns = [
                rf"Sponsors?\s*:\s*([^\n]+)",
                rf"Sponsor\s*:\s*([^\n]+)",
                rf"(?:Rep\.|Sen\.)\s+([^,\n]+(?:,\s*[^,\n]+)*)"
            ]
            
            for sponsor_pattern in sponsor_patterns:
                sponsor_match = re.search(sponsor_pattern, bill_text, re.IGNORECASE)
                if sponsor_match:
                    extracted_info["sponsors"] = sponsor_match.group(1).strip()
                    break
            
            # Extract final status with multiple patterns
            status_patterns = [
                rf"Final Status\s*:\s*([^\n]+)",
                rf"Status\s*:\s*([^\n]+)",
                rf"(?:C\s+\d+\s+L\s+\d+)"
            ]
            
            for status_pattern in status_patterns:
                status_match = re.search(status_pattern, bill_text, re.IGNORECASE)
                if status_match:
                    extracted_info["final_status"] = status_match.group(1).strip()
                    break
            
            # Extract description - everything after bill number until next bill or section
            desc_patterns = [
                rf"{bill_number}[^\n]*\n([^\n]+(?:\n[^\n]+)*?)(?=\n\s*[A-Z]{{2,}}|\n\s*[A-Z]{{1,3}}\s+\d+|\Z)",
                rf"{bill_number}[^\n]*\n([^\n]+)"
            ]
            
            for desc_pattern in desc_patterns:
                desc_match = re.search(desc_pattern, bill_text, re.IGNORECASE)
                if desc_match:
                    description = desc_match.group(1).strip()
                    # Clean up description
                    description = re.sub(r'\s+', ' ', description)
                    extracted_info["description"] = description
                    break
            
            logger.info(f"Extracted info for {bill_number}: {extracted_info}")
            return extracted_info
    
    logger.warning(f"No bill information found for {bill_number}")
    return extracted_info

def extract_universal_information(context_text: str, question: str) -> Dict[str, Any]:
    """Universal information extraction that works for any document type"""
    extracted_info = {
        "key_entities": [],
        "numbers_and_dates": [],
        "relationships": []
    }
    
    try:
        # Extract names (people, organizations, bills, cases, etc.)
        name_patterns = [
            r"\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*",  # Names
            r"(?:HB|SB|SSB|ESSB|SHB|ESHB)\s*\d+",  # Bill numbers
        ]
        
        for pattern in name_patterns:
            matches = re.findall(pattern, context_text)
            extracted_info["key_entities"].extend(matches[:10])  # Limit to prevent overflow
        
        # Extract numbers, dates, amounts
        number_patterns = [
            r"\$[\d,]+(?:\.\d{2})?",  # Dollar amounts
            r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}",  # Dates
            r"\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4}",  # Written dates
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, context_text, re.IGNORECASE)
            extracted_info["numbers_and_dates"].extend(matches[:10])
        
        # Extract relationships
        relationship_patterns = [
            r"(?:sponsors?|authored?\s+by):\s*([^.\n]+)",
            r"(?:final\s+status|status):\s*([^.\n]+)",
        ]
        
        for pattern in relationship_patterns:
            matches = re.findall(pattern, context_text, re.IGNORECASE)
            extracted_info["relationships"].extend(matches[:5])
    
    except Exception as e:
        logger.warning(f"Error in universal extraction: {e}")
    
    return extracted_info



=== requirements.txt ===
# FastAPI and server
fastapi==0.104.1
uvicorn[standard]==0.24.0

# LangChain core and components
langchain
langchain-community
langchain-chroma==0.1.4
langchain-huggingface==0.0.3

# Vector database
chromadb==0.4.17

# Embeddings and ML (CPU-only, minimal)
sentence-transformers==2.6.1
huggingface-hub==0.23.1
# Skip torch - it will be installed as dependency of sentence-transformers

# PDF processing
PyMuPDF==1.23.8
pypdf==3.17.4
pdfplumber==0.10.3
PyPDF2==3.0.1

# Document processing
python-docx==1.1.0

# HTTP and API
httpx==0.25.2
requests==2.31.0
aiohttp==3.9.1

# Environment and utilities
python-dotenv==1.0.0
pydantic==2.5.0

# Additional dependencies
typing-extensions==4.8.0
python-multipart==0.0.6

# NLP dependencies
spacy==3.7.2
numpy==1.26.4

# Skip transformers and other heavy ML libraries for now



=== .env.example ===
# OpenRouter API Configuration
OPENAI_API_KEY=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
OPENAI_API_BASE=https://openrouter.ai/api/v1

# External Legal Database APIs (Optional)
LEXISNEXIS_API_KEY=
LEXISNEXIS_API_ENDPOINT=
WESTLAW_API_KEY=
WESTLAW_API_ENDPOINT=

# Server Configuration
PORT=8000



=== .gitignore ===
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/

# ChromaDB
chromadb-database/
user-containers/

# Environment
.env

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db



=== legal_assistant/__init__.py ===
"""Legal Assistant API - Multi-User Platform with Enhanced RAG and Comprehensive Analysis"""

__version__ = "10.0.0-SmartRAG-ComprehensiveAnalysis"
__author__ = "Legal Assistant Team"
__description__ = "Multi-User Legal Assistant with Enhanced RAG, Comprehensive Analysis, and External Database Integration"



=== legal_assistant/api/__init__.py ===
"""API package"""



=== legal_assistant/api/routers/__init__.py ===
"""API routers package"""
from . import query, documents, analysis, admin, health

__all__ = ['query', 'documents', 'analysis', 'admin', 'health']



=== legal_assistant/api/routers/admin.py ===
"""Admin endpoints"""
import os
import logging
from datetime import datetime
from fastapi import APIRouter, Form, Depends

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from ...models import User
from ...config import USER_CONTAINERS_PATH
from ...core.security import get_current_user
from ...services.container_manager import get_container_manager
from ...storage.managers import uploaded_files
from ...utils.formatting import format_context_for_llm
from ...utils.text_processing import extract_bill_information, extract_universal_information

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/cleanup-containers")
async def cleanup_orphaned_containers():
    """Clean up orphaned files in containers that are no longer tracked"""
    cleanup_results = {
        "containers_checked": 0,
        "orphaned_documents_found": 0,
        "cleanup_performed": False,
        "errors": []
    }
    
    try:
        if not os.path.exists(USER_CONTAINERS_PATH):
            return cleanup_results
        
        container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                         if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
        
        cleanup_results["containers_checked"] = len(container_dirs)
        tracked_file_ids = set(uploaded_files.keys())
        
        logger.info(f"Checking {len(container_dirs)} containers against {len(tracked_file_ids)} tracked files")
        
        for container_dir in container_dirs:
            try:
                container_path = os.path.join(USER_CONTAINERS_PATH, container_dir)
                
                try:
                    embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
                    db = Chroma(
                        collection_name=f"user_{container_dir}",
                        embedding_function=embedding_function,
                        persist_directory=container_path
                    )
                    
                    logger.info(f"Container {container_dir} loaded successfully")
                    
                except Exception as e:
                    logger.warning(f"Could not load container {container_dir}: {e}")
                    cleanup_results["errors"].append(f"Container {container_dir}: {str(e)}")
                    continue
                    
            except Exception as e:
                logger.error(f"Error processing container {container_dir}: {e}")
                cleanup_results["errors"].append(f"Container {container_dir}: {str(e)}")
        
        return cleanup_results
        
    except Exception as e:
        logger.error(f"Error during container cleanup: {e}")
        cleanup_results["errors"].append(str(e))
        return cleanup_results

@router.post("/sync-document-tracking")
async def sync_document_tracking():
    """Sync the uploaded_files tracking with what's actually in the containers"""
    sync_results = {
        "tracked_files": len(uploaded_files),
        "containers_found": 0,
        "sync_performed": False,
        "recovered_files": 0,
        "errors": []
    }
    
    try:
        if not os.path.exists(USER_CONTAINERS_PATH):
            return sync_results
        
        container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                         if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
        
        sync_results["containers_found"] = len(container_dirs)
        
        logger.info(f"Syncing document tracking: {len(uploaded_files)} tracked files, {len(container_dirs)} containers")
        
        return sync_results
        
    except Exception as e:
        logger.error(f"Error during document tracking sync: {e}")
        sync_results["errors"].append(str(e))
        return sync_results

@router.get("/document-health")
async def check_document_health():
    """Check the health of document tracking and containers"""
    health_info = {
        "timestamp": datetime.utcnow().isoformat(),
        "uploaded_files_count": len(uploaded_files),
        "container_directories": 0,
        "users_with_containers": 0,
        "orphaned_files": [],
        "container_errors": [],
        "recommendations": []
    }
    
    try:
        # Check container directories
        if os.path.exists(USER_CONTAINERS_PATH):
            container_dirs = [d for d in os.listdir(USER_CONTAINERS_PATH) 
                             if os.path.isdir(os.path.join(USER_CONTAINERS_PATH, d))]
            health_info["container_directories"] = len(container_dirs)
            
            # Check which users have containers
            user_ids_with_files = set()
            for file_data in uploaded_files.values():
                if 'user_id' in file_data:
                    user_ids_with_files.add(file_data['user_id'])
            
            health_info["users_with_containers"] = len(user_ids_with_files)
            
            # Check for potential issues
            if len(container_dirs) > len(user_ids_with_files):
                health_info["recommendations"].append("Some containers may be orphaned - consider running cleanup")
            
            if len(uploaded_files) == 0 and len(container_dirs) > 0:
                health_info["recommendations"].append("Containers exist but no files are tracked - may need sync")
        
        # Check for files with missing metadata
        for file_id, file_data in uploaded_files.items():
            if not file_data.get('user_id'):
                health_info["orphaned_files"].append(file_id)
        
        if health_info["orphaned_files"]:
            health_info["recommendations"].append(f"{len(health_info['orphaned_files'])} files have missing user_id")
        
        logger.info(f"Document health check: {health_info['uploaded_files_count']} files, {health_info['container_directories']} containers")
        
        return health_info
        
    except Exception as e:
        logger.error(f"Error during document health check: {e}")
        health_info["container_errors"].append(str(e))
        return health_info

@router.post("/emergency-clear-tracking")
async def emergency_clear_document_tracking():
    """EMERGENCY: Clear all document tracking"""
    try:
        global uploaded_files
        backup_count = len(uploaded_files)
        uploaded_files.clear()
        
        logger.warning(f"EMERGENCY: Cleared tracking for {backup_count} files")
        
        return {
            "status": "completed",
            "cleared_files": backup_count,
            "warning": "All document tracking has been cleared. Users will need to re-upload documents.",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error during emergency clear: {e}")
        return {
            "status": "failed",
            "error": str(e)
        }

@router.get("/debug/test-bill-search")
async def debug_bill_search_get(
    bill_number: str,
    user_id: str
):
    """Debug bill-specific search functionality (GET version for browser testing)"""
    
    try:
        container_manager = get_container_manager()
        # Get user database
        user_db = container_manager.get_user_database_safe(user_id)
        if not user_db:
            return {"error": "No user database found"}
        
        # Get all documents and check metadata
        all_docs = user_db.get()
        found_chunks = []
        
        logger.info(f"Debugging search for bill: {bill_number}")
        logger.info(f"Total documents in database: {len(all_docs.get('ids', []))}")
        
        for i, (doc_id, metadata, content) in enumerate(zip(
            all_docs.get('ids', []), 
            all_docs.get('metadatas', []), 
            all_docs.get('documents', [])
        )):
            if metadata:
                chunk_index = metadata.get('chunk_index', 'unknown')
                contains_bills = metadata.get('contains_bills', '')
                
                if bill_number in contains_bills:
                    found_chunks.append({
                        'chunk_index': chunk_index,
                        'contains_bills': contains_bills,
                        'content_preview': content[:200] + "..." if len(content) > 200 else content
                    })
                    logger.info(f"Found {bill_number} in chunk {chunk_index}")
        
        # Also test direct text search
        direct_search = [content for content in all_docs.get('documents', []) if bill_number in content]
        
        return {
            "bill_number": bill_number,
            "user_id": user_id,
            "total_chunks": len(all_docs.get('ids', [])),
            "chunks_with_bill_metadata": found_chunks,
            "chunks_with_bill_in_text": len(direct_search),
            "text_search_preview": direct_search[0][:300] + "..." if direct_search else "Not found in text",
            "sample_metadata": all_docs.get('metadatas', [])[:2] if all_docs.get('metadatas') else []
        }
        
    except Exception as e:
        logger.error(f"Debug bill search failed: {e}")
        return {"error": str(e)}

@router.post("/debug/test-bill-search")
async def debug_bill_search(
    bill_number: str = Form(...),
    user_id: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Debug bill-specific search functionality"""
    
    try:
        container_manager = get_container_manager()
        # Get user database
        user_db = container_manager.get_user_database_safe(user_id)
        if not user_db:
            return {"error": "No user database found"}
        
        # Get all documents and check metadata
        all_docs = user_db.get()
        found_chunks = []
        
        logger.info(f"Debugging search for bill: {bill_number}")
        logger.info(f"Total documents in database: {len(all_docs.get('ids', []))}")
        
        for i, (doc_id, metadata, content) in enumerate(zip(
            all_docs.get('ids', []), 
            all_docs.get('metadatas', []), 
            all_docs.get('documents', [])
        )):
            if metadata:
                chunk_index = metadata.get('chunk_index', 'unknown')
                contains_bills = metadata.get('contains_bills', '')
                
                if bill_number in contains_bills:
                    found_chunks.append({
                        'chunk_index': chunk_index,
                        'contains_bills': contains_bills,
                        'content_preview': content[:200] + "..." if len(content) > 200 else content
                    })
                    logger.info(f"Found {bill_number} in chunk {chunk_index}")
        
        # Also test direct text search
        direct_search = [content for content in all_docs.get('documents', []) if bill_number in content]
        
        return {
            "bill_number": bill_number,
            "total_chunks": len(all_docs.get('ids', [])),
            "chunks_with_bill_metadata": found_chunks,
            "chunks_with_bill_in_text": len(direct_search),
            "text_search_preview": direct_search[0][:300] + "..." if direct_search else "Not found in text"
        }
        
    except Exception as e:
        logger.error(f"Debug bill search failed: {e}")
        return {"error": str(e)}

@router.post("/debug/test-extraction")
async def debug_test_extraction(
    question: str = Form(...),
    user_id: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Test information extraction for any question"""
    
    try:
        container_manager = get_container_manager()
        # Search user's documents
        user_results = container_manager.enhanced_search_user_container(user_id, question, "", k=5)
        
        if user_results:
            # Get context
            context_text, source_info = format_context_for_llm(user_results, max_length=3000)
            
            # Test extraction
            import re
            bill_match = re.search(r"(HB|SB|SSB|ESSB|SHB|ESHB)\s*(\d+)", question, re.IGNORECASE)
            if bill_match:
                bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
                extracted_info = extract_bill_information(context_text, bill_number)
            else:
                extracted_info = extract_universal_information(context_text, question)
            
            return {
                "question": question,
                "context_preview": context_text[:500] + "...",
                "extracted_info": extracted_info,
                "sources_found": len(user_results)
            }
        else:
            return {
                "question": question,
                "error": "No relevant documents found"
            }
            
    except Exception as e:
        return {"error": str(e)}



=== legal_assistant/api/routers/analysis.py ===
"""Analysis endpoints"""
import logging
from fastapi import APIRouter, Depends, HTTPException

from ...models import User, ComprehensiveAnalysisRequest, StructuredAnalysisResponse, AnalysisType
from ...core.security import get_current_user
from ...services.analysis_service import ComprehensiveAnalysisProcessor

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/comprehensive-analysis", response_model=StructuredAnalysisResponse)
async def comprehensive_document_analysis(
    request: ComprehensiveAnalysisRequest,
    current_user: User = Depends(get_current_user)
):
    """Comprehensive document analysis endpoint"""
    logger.info(f"Comprehensive analysis request: user={request.user_id}, doc={request.document_id}, types={request.analysis_types}")
    
    try:
        if request.user_id != current_user.user_id:
            raise HTTPException(status_code=403, detail="Cannot analyze documents for different user")
        
        processor = ComprehensiveAnalysisProcessor()
        result = processor.process_comprehensive_analysis(request)
        
        logger.info(f"Comprehensive analysis completed: confidence={result.overall_confidence:.2f}, time={result.processing_time:.2f}s")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Comprehensive analysis endpoint failed: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@router.post("/quick-analysis/{document_id}")
async def quick_document_analysis(
    document_id: str,
    analysis_type: AnalysisType = AnalysisType.COMPREHENSIVE,
    current_user: User = Depends(get_current_user)
):
    """Quick analysis endpoint for single documents"""
    try:
        request = ComprehensiveAnalysisRequest(
            document_id=document_id,
            analysis_types=[analysis_type],
            user_id=current_user.user_id,
            response_style="detailed"
        )
        
        processor = ComprehensiveAnalysisProcessor()
        result = processor.process_comprehensive_analysis(request)
        
        return {
            "success": True,
            "analysis": result,
            "message": f"Analysis completed with {result.overall_confidence:.1%} confidence"
        }
        
    except Exception as e:
        logger.error(f"Quick analysis failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "Analysis failed"
        }



=== legal_assistant/api/routers/documents.py ===
"""Document management endpoints"""
import os
import uuid
import logging
import traceback
from datetime import datetime
from fastapi import APIRouter, File, UploadFile, Depends, HTTPException

from ...models import User, DocumentUploadResponse
from ...config import MAX_FILE_SIZE, LEGAL_EXTENSIONS
from ...core.security import get_current_user
from ...services.document_processor import SafeDocumentProcessor
from ...services.container_manager import get_container_manager
from ...storage.managers import uploaded_files

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/user/upload", response_model=DocumentUploadResponse)
async def upload_user_document(
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user)
):
    """Enhanced upload endpoint with file_id tracking and timeout handling"""
    start_time = datetime.utcnow()
    
    try:
        # Check file size first (before reading)
        file.file.seek(0, 2)
        file_size = file.file.tell()
        file.file.seek(0)
        
        if file_size > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400, 
                detail=f"File too large. Maximum size is {MAX_FILE_SIZE//1024//1024}MB. Your file: {file_size//1024//1024}MB"
            )
        
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in LEGAL_EXTENSIONS:
            raise HTTPException(status_code=400, detail=f"Unsupported file type. Supported: {LEGAL_EXTENSIONS}")
        
        logger.info(f"Processing upload: {file.filename} ({file_size//1024}KB) for user {current_user.user_id}")
        
        # Process document with timeout protection
        try:
            content, pages_processed, warnings = SafeDocumentProcessor.process_document_safe(file)
        except Exception as doc_error:
            logger.error(f"Document processing failed: {doc_error}")
            raise HTTPException(
                status_code=422, 
                detail=f"Failed to process document: {str(doc_error)}"
            )
        
        if not content or len(content.strip()) < 50:
            raise HTTPException(
                status_code=422,
                detail="Document appears to be empty or could not be processed properly"
            )
        
        file_id = str(uuid.uuid4())
        metadata = {
            'source': file.filename,
            'file_id': file_id,
            'upload_date': datetime.utcnow().isoformat(),
            'user_id': current_user.user_id,
            'file_type': file_ext,
            'pages': pages_processed,
            'file_size': file_size,
            'content_length': len(content),
            'processing_warnings': warnings
        }
        
        logger.info(f"Adding document to container: {len(content)} chars, {pages_processed} pages")
        
        # Add to container with timeout protection
        container_manager = get_container_manager()
        try:
            success = container_manager.add_document_to_container(
                current_user.user_id,
                content,
                metadata,
                file_id
            )
        except Exception as container_error:
            logger.error(f"Container operation failed: {container_error}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to store document: {str(container_error)}"
            )
        
        if not success:
            raise HTTPException(status_code=500, detail="Failed to add document to user container")
        
        session_id = str(uuid.uuid4())
        uploaded_files[file_id] = {
            'filename': file.filename,
            'user_id': current_user.user_id,
            'container_id': current_user.container_id,
            'pages_processed': pages_processed,
            'uploaded_at': datetime.utcnow(),
            'session_id': session_id,
            'file_size': file_size,
            'content_length': len(content)
        }
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        logger.info(f"Upload successful: {file.filename} processed in {processing_time:.2f}s")
        
        return DocumentUploadResponse(
            message=f"Document {file.filename} uploaded successfully ({pages_processed} pages, {len(content)} chars)",
            file_id=file_id,
            pages_processed=pages_processed,
            processing_time=processing_time,
            warnings=warnings,
            session_id=session_id,
            user_id=current_user.user_id,
            container_id=current_user.container_id or ""
        )
        
    except HTTPException:
        raise
    except Exception as e:
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        logger.error(f"Error uploading user document after {processing_time:.2f}s: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500, 
            detail=f"Upload failed after {processing_time:.2f}s: {str(e)}"
        )

@router.get("/user/documents")
async def list_user_documents(current_user: User = Depends(get_current_user)):
    """ENHANCED: List all documents in user's container with better error handling"""
    try:
        user_documents = []
        
        # Add timeout and better error handling
        for file_id, file_data in uploaded_files.items():
            try:
                if file_data.get('user_id') == current_user.user_id:
                    # Handle both datetime objects and strings
                    uploaded_at_str = file_data['uploaded_at']
                    if hasattr(uploaded_at_str, 'isoformat'):
                        uploaded_at_str = uploaded_at_str.isoformat()
                    elif not isinstance(uploaded_at_str, str):
                        uploaded_at_str = str(uploaded_at_str)
                    
                    user_documents.append({
                        'file_id': file_id,
                        'filename': file_data['filename'],
                        'uploaded_at': uploaded_at_str,
                        'pages_processed': file_data.get('pages_processed', 0),
                        'file_size': file_data.get('file_size', 0)
                    })
            except Exception as e:
                logger.warning(f"Error processing file {file_id}: {e}")
                continue
        
        logger.info(f"Retrieved {len(user_documents)} documents for user {current_user.user_id}")
        
        return {
            'user_id': current_user.user_id,
            'container_id': current_user.container_id,
            'documents': user_documents,
            'total_documents': len(user_documents)
        }
        
    except Exception as e:
        logger.error(f"Error listing user documents: {e}")
        # Return empty list instead of failing completely
        return {
            'user_id': current_user.user_id,
            'container_id': current_user.container_id or "unknown",
            'documents': [],
            'total_documents': 0,
            'error': str(e)
        }

@router.delete("/user/documents/{file_id}")
async def delete_user_document(
    file_id: str,
    current_user: User = Depends(get_current_user)
):
    """Delete a document from user's container"""
    if file_id not in uploaded_files:
        raise HTTPException(status_code=404, detail="Document not found")
    
    file_data = uploaded_files[file_id]
    if file_data.get('user_id') != current_user.user_id:
        raise HTTPException(status_code=403, detail="Unauthorized to delete this document")
    
    del uploaded_files[file_id]
    return {"message": "Document deleted successfully", "file_id": file_id}



=== legal_assistant/api/routers/external.py ===
"""External database endpoints"""
import logging
from typing import List
from fastapi import APIRouter, Form, Depends, HTTPException

from ...models import User
from ...core.security import get_current_user
from ...services.external_db_service import search_external_databases

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/external/search")
async def search_external_databases_endpoint(
    query: str = Form(...),
    databases: List[str] = Form(...),
    current_user: User = Depends(get_current_user)
):
    """Search external legal databases (requires premium subscription)"""
    if current_user.subscription_tier not in ["premium", "enterprise"]:
        raise HTTPException(
            status_code=403, 
            detail="External database access requires premium subscription"
        )
    
    results = search_external_databases(query, databases, current_user)
    
    return {
        "query": query,
        "databases_searched": databases,
        "results": results,
        "total_results": len(results)
    }



=== legal_assistant/api/routers/health.py ===
"""Health check endpoints"""
import os
from datetime import datetime
from fastapi import APIRouter
from fastapi.responses import HTMLResponse

from ...config import (
    DEFAULT_CHROMA_PATH, USER_CONTAINERS_PATH, OPENROUTER_API_KEY, FeatureFlags
)
from ...models import ConversationHistory
from ...storage.managers import conversations
from ...core.dependencies import get_nlp, get_sentence_model, get_embeddings, get_sentence_model_name

router = APIRouter()

@router.get("/health")
def health_check():
    """Enhanced system health check with comprehensive analysis capabilities"""
    db_exists = os.path.exists(DEFAULT_CHROMA_PATH)
    
    nlp = get_nlp()
    sentence_model = get_sentence_model()
    embeddings = get_embeddings()
    sentence_model_name = get_sentence_model_name()
    
    return {
        "status": "healthy",
        "version": "10.0.0-SmartRAG-ComprehensiveAnalysis",
        "timestamp": datetime.utcnow().isoformat(),
        "ai_enabled": FeatureFlags.AI_ENABLED,
        "openrouter_api_configured": bool(OPENROUTER_API_KEY),
        "components": {
            "default_database": {
                "exists": db_exists,
                "path": DEFAULT_CHROMA_PATH
            },
            "user_containers": {
                "enabled": True,
                "base_path": USER_CONTAINERS_PATH,
                "active_containers": len(os.listdir(USER_CONTAINERS_PATH)) if os.path.exists(USER_CONTAINERS_PATH) else 0,
                "document_specific_retrieval": True,
                "file_id_tracking": True
            },
            "external_databases": {
                "lexisnexis": {
                    "configured": bool(os.environ.get("LEXISNEXIS_API_KEY")),
                    "status": "ready" if bool(os.environ.get("LEXISNEXIS_API_KEY")) else "not_configured"
                },
                "westlaw": {
                    "configured": bool(os.environ.get("WESTLAW_API_KEY")),
                    "status": "ready" if bool(os.environ.get("WESTLAW_API_KEY")) else "not_configured"
                }
            },
            "comprehensive_analysis": {
                "enabled": True,
                "analysis_types": [
                    "comprehensive",
                    "document_summary", 
                    "key_clauses",
                    "risk_assessment",
                    "timeline_deadlines", 
                    "party_obligations",
                    "missing_clauses"
                ],
                "structured_output": True,
                "document_specific": True,
                "confidence_scoring": True,
                "single_api_call": True
            },
            "enhanced_rag": {
                "enabled": True,
                "features": [
                    "multi_query_strategies",
                    "query_expansion",
                    "entity_extraction",
                    "sub_query_decomposition",
                    "confidence_scoring",
                    "duplicate_removal",
                    "document_specific_filtering"
                ],
                "nlp_model": nlp is not None,
                "sentence_model": sentence_model is not None,
                "sentence_model_name": sentence_model_name if sentence_model else "none",
                "embedding_model": getattr(embeddings, 'model_name', 'unknown') if embeddings else "none"
            },
            "document_processing": {
                "pdf_support": FeatureFlags.PYMUPDF_AVAILABLE or FeatureFlags.PDFPLUMBER_AVAILABLE,
                "pymupdf_available": FeatureFlags.PYMUPDF_AVAILABLE,
                "pdfplumber_available": FeatureFlags.PDFPLUMBER_AVAILABLE,
                "unstructured_available": FeatureFlags.UNSTRUCTURED_AVAILABLE,
                "docx_support": True,
                "txt_support": True,
                "safe_document_processor": True,
                "enhanced_page_estimation": True,
                "bert_semantic_chunking": sentence_model is not None,
                "advanced_legal_chunking": True,
                "embedding_model": sentence_model_name if sentence_model else "none"
            }
        },
        "new_endpoints": [
            "POST /comprehensive-analysis - Full structured analysis",
            "POST /quick-analysis/{document_id} - Quick single document analysis", 
            "Enhanced /ask - Detects comprehensive analysis requests",
            "Enhanced /user/upload - Stores file_id for targeting",
            "GET /admin/document-health - Check system health",
            "POST /admin/cleanup-containers - Clean orphaned containers",
            "POST /admin/emergency-clear-tracking - Reset document tracking"
        ],
        "features": [
            "‚úÖ User-specific document containers",
            "‚úÖ Enhanced RAG with multi-query strategies",
            "‚úÖ Combined search across all sources",
            "‚úÖ External legal database integration (ready)",
            "‚úÖ Subscription tier management",
            "‚úÖ Document access control",
            "‚úÖ Source attribution (default/user/external)",
            "‚úÖ Dynamic confidence scoring",
            "‚úÖ Query expansion and decomposition",
            "‚úÖ SafeDocumentProcessor for file handling",
            "üîß Optional authentication for debugging",
            "üÜï Comprehensive multi-analysis in single API call",
            "üÜï Document-specific analysis targeting",
            "üÜï Structured analysis responses with sections",
            "üÜï Enhanced confidence scoring per section",
            "üÜï File ID tracking for precise document retrieval",
            "üÜï Automatic comprehensive analysis detection",
            "üÜï Container cleanup and health monitoring",
            "üÜï Enhanced error handling and recovery",
            "üÜï Fixed page estimation with content analysis",
            "üÜï Unstructured.io integration for advanced processing",
            "üÜï BERT-based semantic chunking for better retrieval",
            "üÜï Enhanced information extraction (bills, sponsors, etc.)",
            "üÜï Legal-specific BERT models (InCaseLawBERT, legal-bert-base-uncased)",
            "üÜï Advanced semantic similarity for intelligent chunking",
            "üÜï Legal document pattern recognition for better segmentation"
        ],
        # Frontend compatibility fields
        "unified_mode": True,
        "enhanced_rag": True,
        "database_exists": db_exists,
        "database_path": DEFAULT_CHROMA_PATH,
        "api_key_configured": bool(OPENROUTER_API_KEY),
        "active_conversations": len(conversations)
    }

@router.get("/conversation/{session_id}", response_model=ConversationHistory)
async def get_conversation(session_id: str):
    """Get the conversation history for a session"""
    from fastapi import HTTPException
    
    if session_id not in conversations:
        raise HTTPException(status_code=404, detail="Session not found")
    
    return ConversationHistory(
        session_id=session_id,
        messages=conversations[session_id]['messages']
    )

@router.get("/subscription/status")
async def get_subscription_status():
    """Get user's subscription status and available features"""
    from ...core.security import get_current_user
    from fastapi import Depends
    
    current_user = await get_current_user()
    
    features = {
        "free": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": 10,
            "external_databases": [],
            "ai_analysis": True,
            "api_calls_per_month": 100,
            "enhanced_rag": True,
            "comprehensive_analysis": True
        },
        "premium": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": 100,
            "external_databases": ["lexisnexis", "westlaw"],
            "ai_analysis": True,
            "api_calls_per_month": 1000,
            "priority_support": True,
            "enhanced_rag": True,
            "comprehensive_analysis": True,
            "document_specific_analysis": True
        },
        "enterprise": {
            "default_database_access": True,
            "user_container": True,
            "max_documents": "unlimited",
            "external_databases": ["lexisnexis", "westlaw", "bloomberg_law"],
            "ai_analysis": True,
            "api_calls_per_month": "unlimited",
            "priority_support": True,
            "custom_integrations": True,
            "enhanced_rag": True,
            "comprehensive_analysis": True,
            "document_specific_analysis": True,
            "bulk_analysis": True
        }
    }
    
    return {
        "user_id": current_user.user_id,
        "subscription_tier": current_user.subscription_tier,
        "features": features.get(current_user.subscription_tier, features["free"]),
        "external_db_access": current_user.external_db_access
    }

@router.get("/", response_class=HTMLResponse)
def get_interface():
    """Web interface with updated documentation for comprehensive analysis"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Legal Assistant - Complete Multi-Analysis Edition [MODULAR]</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background: #f8f9fa; }
            .container { max-width: 1200px; margin: 0 auto; }
            h1 { color: #2c3e50; }
            .feature-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 30px 0; }
            .feature-card { background: #fff; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px; }
            .endpoint { background: #f1f3f4; padding: 10px; margin: 10px 0; border-radius: 5px; font-family: monospace; }
            .status { padding: 5px 10px; border-radius: 15px; font-size: 12px; }
            .status-active { background: #d4edda; color: #155724; }
            .status-ready { background: #cce5ff; color: #004085; }
            .status-modular { background: #28a745; color: white; }
            .badge-modular { background: #17a2b8; color: white; padding: 2px 8px; border-radius: 10px; font-size: 11px; margin-left: 5px; }
            .code-example { background: #f8f9fa; border: 1px solid #e9ecef; border-radius: 5px; padding: 15px; margin: 10px 0; font-family: monospace; font-size: 12px; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>‚öñÔ∏è Legal Assistant API v10.0 <span class="badge-modular">MODULAR ARCHITECTURE</span></h1>
            <p>Complete Multi-User Platform with Enhanced RAG, Comprehensive Analysis, and Container Management</p>
            <div class="status status-modular">üéØ Clean Modular Structure - Easy to Maintain and Extend!</div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h3>üìÅ Modular Architecture</h3>
                    <p>Clean separation of concerns</p>
                    <ul>
                        <li>‚úÖ Services for business logic</li>
                        <li>‚úÖ API routers for endpoints</li>
                        <li>‚úÖ Models for data structures</li>
                        <li>‚úÖ Utils for helper functions</li>
                    </ul>
                </div>
                
                <div class="feature-card">
                    <h3>üöÄ Comprehensive Analysis</h3>
                    <p>All analysis types in a single efficient API call</p>
                    <ul>
                        <li>‚úÖ Document summary</li>
                        <li>‚úÖ Key clauses extraction</li>
                        <li>‚úÖ Risk assessment</li>
                        <li>‚úÖ Timeline & deadlines</li>
                        <li>‚úÖ Party obligations</li>
                        <li>‚úÖ Missing clauses detection</li>
                    </ul>
                </div>
                
                <div class="feature-card">
                    <h3>üõ†Ô∏è Enhanced Error Handling</h3>
                    <p>Robust container management with auto-recovery</p>
                    <ul>
                        <li>‚úÖ Timeout protection</li>
                        <li>‚úÖ Container auto-recovery</li>
                        <li>‚úÖ Graceful degradation</li>
                        <li>‚úÖ Health monitoring</li>
                    </ul>
                </div>
            </div>
            
            <h2>üì° API Reference</h2>
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>Core Endpoints</h4>
                    <div class="endpoint">POST /api/ask - Enhanced chat</div>
                    <div class="endpoint">POST /api/user/upload - Document upload</div>
                    <div class="endpoint">GET /api/user/documents - List documents</div>
                </div>
                
                <div class="feature-card">
                    <h4>Analysis Endpoints</h4>
                    <div class="endpoint">POST /api/comprehensive-analysis</div>
                    <div class="endpoint">POST /api/quick-analysis/{id}</div>
                </div>
                
                <div class="feature-card">
                    <h4>Admin Endpoints</h4>
                    <div class="endpoint">GET /api/admin/document-health</div>
                    <div class="endpoint">POST /api/admin/cleanup-containers</div>
                </div>
            </div>
            
            <p style="text-align: center; color: #7f8c8d; margin-top: 30px;">
                üéâ Modular Legal Assistant Backend - Clean Architecture üéâ
                <br>Version 10.0.0-SmartRAG-ComprehensiveAnalysis
                <br>Fully modularized for easy maintenance and extensibility!
            </p>
        </div>
    </body>
    </html>
    """



=== legal_assistant/api/routers/query.py ===
"""Query endpoints"""
import uuid
import logging
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException

from ...models import Query, QueryResponse, User
from ...core.security import get_current_user
from ...storage.managers import conversations, cleanup_expired_conversations
from ...processors.query_processor import process_query

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/ask", response_model=QueryResponse)
async def ask_question(query: Query, current_user: User = Depends(get_current_user)):
    """Enhanced ask endpoint with comprehensive analysis detection"""
    logger.info(f"Received ask request: {query}")
    
    cleanup_expired_conversations()
    
    session_id = query.session_id or str(uuid.uuid4())
    user_id = query.user_id or current_user.user_id
    
    if session_id not in conversations:
        conversations[session_id] = {
            "messages": [],
            "created_at": datetime.utcnow(),
            "last_accessed": datetime.utcnow()
        }
    else:
        conversations[session_id]["last_accessed"] = datetime.utcnow()
    
    user_question = query.question.strip()
    if not user_question:
        return QueryResponse(
            response=None,
            error="Question cannot be empty.",
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[]
        )
    
    response = process_query(
        user_question, 
        session_id, 
        user_id,
        query.search_scope or "all",
        query.response_style or "balanced",
        query.use_enhanced_rag if query.use_enhanced_rag is not None else True,
        query.document_id
    )
    return response

@router.post("/ask-debug", response_model=QueryResponse)
async def ask_question_debug(query: Query):
    """Debug version of ask endpoint without authentication"""
    logger.info(f"Debug ask request received: {query}")
    
    cleanup_expired_conversations()
    
    session_id = query.session_id or str(uuid.uuid4())
    user_id = query.user_id or "debug_user"
    
    if session_id not in conversations:
        conversations[session_id] = {
            "messages": [],
            "created_at": datetime.utcnow(),
            "last_accessed": datetime.utcnow()
        }
    else:
        conversations[session_id]["last_accessed"] = datetime.utcnow()
    
    user_question = query.question.strip()
    if not user_question:
        return QueryResponse(
            response=None,
            error="Question cannot be empty.",
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[]
        )
    
    response = process_query(
        user_question, 
        session_id, 
        user_id,
        query.search_scope or "all",
        query.response_style or "balanced",
        query.use_enhanced_rag if query.use_enhanced_rag is not None else True,
        query.document_id
    )
    return response



=== legal_assistant/config.py ===
"""Configuration and environment variables"""
import os
from typing import List

# API Configuration
OPENROUTER_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_API_BASE = os.environ.get("OPENAI_API_BASE", "https://openrouter.ai/api/v1")

# Database Paths
DEFAULT_CHROMA_PATH = os.path.abspath(os.path.join(os.getcwd(), "chromadb-database"))
USER_CONTAINERS_PATH = os.path.abspath(os.path.join(os.getcwd(), "user-containers"))

# File Processing
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
LEGAL_EXTENSIONS = {'.pdf', '.txt', '.docx', '.rtf'}

# External Database Configuration
LEXISNEXIS_API_KEY = os.environ.get("LEXISNEXIS_API_KEY")
LEXISNEXIS_API_ENDPOINT = os.environ.get("LEXISNEXIS_API_ENDPOINT")
WESTLAW_API_KEY = os.environ.get("WESTLAW_API_KEY")
WESTLAW_API_ENDPOINT = os.environ.get("WESTLAW_API_ENDPOINT")

# Model Names
EMBEDDING_MODELS = [
    "nlpaueb/legal-bert-base-uncased",
    "law-ai/InCaseLawBERT", 
    "sentence-transformers/all-mpnet-base-v2",
    "sentence-transformers/all-roberta-large-v1",
    "microsoft/DialoGPT-medium",
    "sentence-transformers/all-MiniLM-L12-v2",
    "all-MiniLM-L6-v2"
]

FAST_EMBEDDING_MODELS = [
    "all-MiniLM-L6-v2",
    "all-MiniLM-L12-v2",
]

# AI Models
AI_MODELS = [
    "moonshotai/kimi-k2:free",
    "deepseek/deepseek-chat",
    "microsoft/phi-3-mini-128k-instruct:free",
    "meta-llama/llama-3.2-3b-instruct:free",
    "google/gemma-2-9b-it:free",
    "mistralai/mistral-7b-instruct:free",
    "openchat/openchat-7b:free"
]

# Chunk Sizes
DEFAULT_CHUNK_SIZE = 1500
LEGISLATIVE_CHUNK_SIZE = 2000
DEFAULT_CHUNK_OVERLAP = 300
LEGISLATIVE_CHUNK_OVERLAP = 500

# Search Settings
DEFAULT_SEARCH_K = 10
ENHANCED_SEARCH_K = 12
COMPREHENSIVE_SEARCH_K = 20
MIN_RELEVANCE_SCORE = 0.15

# Confidence Score Weights
CONFIDENCE_WEIGHTS = {
    "relevance": 0.4,
    "document_count": 0.3,
    "consistency": 0.2,
    "completeness": 0.1
}

# Feature Flags
class FeatureFlags:
    AI_ENABLED: bool = bool(OPENROUTER_API_KEY)
    AIOHTTP_AVAILABLE: bool = False
    OPEN_SOURCE_NLP_AVAILABLE: bool = False
    PYMUPDF_AVAILABLE: bool = False
    PDFPLUMBER_AVAILABLE: bool = False
    UNSTRUCTURED_AVAILABLE: bool = False



=== legal_assistant/core/__init__.py ===
"""Core functionality package"""
from .dependencies import (
    initialize_nlp_models,
    initialize_feature_flags,
    get_nlp,
    get_sentence_model,
    get_embeddings
)
from .security import get_current_user, security
from .exceptions import (
    LegalAssistantException,
    DocumentProcessingError,
    ContainerError,
    RetrievalError,
    AnalysisError,
    AuthenticationError
)

__all__ = [
    'initialize_nlp_models',
    'initialize_feature_flags',
    'get_nlp',
    'get_sentence_model',
    'get_embeddings',
    'get_current_user',
    'security',
    'LegalAssistantException',
    'DocumentProcessingError',
    'ContainerError',
    'RetrievalError',
    'AnalysisError',
    'AuthenticationError'
]



=== legal_assistant/core/dependencies.py ===
"""Dependency injection and initialization"""
import logging
from typing import Optional
import spacy
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from ..config import EMBEDDING_MODELS, FAST_EMBEDDING_MODELS, FeatureFlags

logger = logging.getLogger(__name__)

# Global instances
nlp: Optional[spacy.Language] = None
sentence_model: Optional[SentenceTransformer] = None
embeddings: Optional[HuggingFaceEmbeddings] = None
sentence_model_name: Optional[str] = None

def initialize_nlp_models():
    """Initialize NLP models"""
    global nlp, sentence_model, embeddings, sentence_model_name
    
    # Load spaCy
    try:
        nlp = spacy.load("en_core_web_sm")
        logger.info("‚úÖ spaCy model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}")
        nlp = None
    
    # Load sentence transformer
    for model_name in EMBEDDING_MODELS:
        try:
            sentence_model = SentenceTransformer(model_name)
            sentence_model_name = model_name
            logger.info(f"‚úÖ Loaded powerful sentence model: {model_name}")
            break
        except Exception as e:
            logger.warning(f"Failed to load {model_name}: {e}")
            continue
    
    if sentence_model is None:
        logger.error("‚ùå Failed to load any sentence transformer model")
        sentence_model_name = "none"
    
    # Load embeddings
    try:
        if sentence_model_name and sentence_model_name != "none":
            embeddings = HuggingFaceEmbeddings(model_name=sentence_model_name)
            logger.info(f"‚úÖ Loaded embeddings with: {sentence_model_name}")
        else:
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            logger.info("‚ö†Ô∏è Using fallback embeddings: all-MiniLM-L6-v2")
    except Exception as e:
        logger.error(f"Failed to load embeddings: {e}")
        embeddings = None

def initialize_feature_flags():
    """Initialize feature availability flags"""
    # Check aiohttp
    try:
        import aiohttp
        FeatureFlags.AIOHTTP_AVAILABLE = True
    except ImportError:
        FeatureFlags.AIOHTTP_AVAILABLE = False
        print("‚ö†Ô∏è aiohttp not available - AI features disabled. Install with: pip install aiohttp")
    
    # Check open source NLP
    try:
        import torch
        from transformers import pipeline
        FeatureFlags.OPEN_SOURCE_NLP_AVAILABLE = True
        logger.info("‚úÖ Open-source NLP models available")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Open-source NLP models not available: {e}")
        print("Install with: pip install transformers torch")
    
    # Check PDF processors
    try:
        import fitz
        FeatureFlags.PYMUPDF_AVAILABLE = True
        print("‚úÖ PyMuPDF available - using enhanced PDF extraction")
    except ImportError as e:
        print(f"‚ö†Ô∏è PyMuPDF not available: {e}")
        print("Install with: pip install PyMuPDF")
    
    try:
        import pdfplumber
        FeatureFlags.PDFPLUMBER_AVAILABLE = True
        print("‚úÖ pdfplumber available - using enhanced PDF extraction")
    except ImportError as e:
        print(f"‚ö†Ô∏è pdfplumber not available: {e}")
        print("Install with: pip install pdfplumber")
    
    try:
        from unstructured.partition.auto import partition
        FeatureFlags.UNSTRUCTURED_AVAILABLE = True
        print("‚úÖ Unstructured.io available - using advanced document processing")
    except ImportError as e:
        print(f"‚ö†Ô∏è Unstructured.io not available: {e}")
        print("Install with: pip install unstructured[all-docs]")
    
    # Update AI enabled flag
    FeatureFlags.AI_ENABLED = bool(FeatureFlags.AIOHTTP_AVAILABLE and FeatureFlags.AI_ENABLED)
    
    print(f"Document processing status: PyMuPDF={FeatureFlags.PYMUPDF_AVAILABLE}, pdfplumber={FeatureFlags.PDFPLUMBER_AVAILABLE}, Unstructured={FeatureFlags.UNSTRUCTURED_AVAILABLE}")

def get_nlp():
    """Get NLP instance"""
    return nlp

def get_sentence_model():
    """Get sentence model instance"""
    return sentence_model

def get_embeddings():
    """Get embeddings instance"""
    return embeddings

def get_sentence_model_name():
    """Get sentence model name"""
    return sentence_model_name



=== legal_assistant/core/exceptions.py ===
"""Custom exceptions for the application"""

class LegalAssistantException(Exception):
    """Base exception for all custom exceptions"""
    pass

class DocumentProcessingError(LegalAssistantException):
    """Raised when document processing fails"""
    pass

class ContainerError(LegalAssistantException):
    """Raised when container operations fail"""
    pass

class RetrievalError(LegalAssistantException):
    """Raised when document retrieval fails"""
    pass

class AnalysisError(LegalAssistantException):
    """Raised when analysis operations fail"""
    pass

class AuthenticationError(LegalAssistantException):
    """Raised when authentication fails"""
    pass



=== legal_assistant/core/security.py ===
"""Authentication and authorization"""
from typing import Optional
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from ..models import User
from ..storage.managers import user_sessions

security = HTTPBearer(auto_error=False)

def get_current_user(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> User:
    """Get current user from credentials"""
    if credentials is None:
        default_user_id = "debug_user"
        if default_user_id not in user_sessions:
            from ..services.container_manager import get_container_manager
            container_manager = get_container_manager()
            user_sessions[default_user_id] = User(
                user_id=default_user_id,
                container_id=container_manager.get_container_id(default_user_id),
                subscription_tier="free"
            )
        return user_sessions[default_user_id]
    
    token = credentials.credentials
    user_id = f"user_{token[:8]}"
    
    if user_id not in user_sessions:
        from ..services.container_manager import get_container_manager
        container_manager = get_container_manager()
        user_sessions[user_id] = User(
            user_id=user_id,
            container_id=container_manager.get_container_id(user_id),
            subscription_tier="free"
        )
    
    return user_sessions[user_id]



=== legal_assistant/main.py ===
"""Main FastAPI application entry point"""
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from .config import DEFAULT_CHROMA_PATH, USER_CONTAINERS_PATH, FeatureFlags
from .core import initialize_nlp_models, initialize_feature_flags
from .services import initialize_container_manager
from .api.routers import query, documents, analysis, admin, health

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Create directories
os.makedirs(USER_CONTAINERS_PATH, exist_ok=True)

# Initialize everything
logger.info(f"Using DEFAULT_CHROMA_PATH: {DEFAULT_CHROMA_PATH}")
logger.info(f"Using USER_CONTAINERS_PATH: {USER_CONTAINERS_PATH}")

initialize_feature_flags()
initialize_nlp_models()
initialize_container_manager()

# Create FastAPI app
app = FastAPI(
    title="Unified Legal Assistant API",
    description="Multi-User Legal Assistant with Enhanced RAG, Comprehensive Analysis, and External Database Integration",
    version="10.0.0-SmartRAG-ComprehensiveAnalysis"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(query.router, tags=["queries"])
app.include_router(documents.router, tags=["documents"])
app.include_router(analysis.router, tags=["analysis"])
app.include_router(admin.router, prefix="/admin", tags=["admin"])
app.include_router(health.router, tags=["health"])

# Mount the home page from health router
app.mount("/", health.router)

def create_app():
    """Application factory"""
    return app

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"üöÄ Starting Modular Legal Assistant on port {port}")
    logger.info(f"ChromaDB Path: {DEFAULT_CHROMA_PATH}")
    logger.info(f"User Containers Path: {USER_CONTAINERS_PATH}")
    logger.info(f"AI Status: {'ENABLED' if FeatureFlags.AI_ENABLED else 'DISABLED - Set OPENAI_API_KEY to enable'}")
    logger.info(f"PDF processing: PyMuPDF={FeatureFlags.PYMUPDF_AVAILABLE}, pdfplumber={FeatureFlags.PDFPLUMBER_AVAILABLE}")
    logger.info("Features: Comprehensive analysis, document-specific targeting, container cleanup, enhanced error handling")
    logger.info("Version: 10.0.0-SmartRAG-ComprehensiveAnalysis")
    logger.info("üìÅ MODULAR ARCHITECTURE - Clean separation of concerns!")
    uvicorn.run("legal_assistant.main:app", host="0.0.0.0", port=port, reload=True)



=== legal_assistant/models/__init__.py ===
"""Models package"""
from .api_models import (
    User, Query, QueryResponse, ComprehensiveAnalysisRequest,
    StructuredAnalysisResponse, UserDocumentUpload, DocumentUploadResponse,
    ConversationHistory
)
from .enums import AnalysisType

__all__ = [
    'User', 'Query', 'QueryResponse', 'ComprehensiveAnalysisRequest',
    'StructuredAnalysisResponse', 'UserDocumentUpload', 'DocumentUploadResponse',
    'ConversationHistory', 'AnalysisType'
]



=== legal_assistant/models/api_models.py ===
"""Pydantic models for API requests and responses"""
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from datetime import datetime
from .enums import AnalysisType

class User(BaseModel):
    user_id: str
    email: Optional[str] = None
    container_id: Optional[str] = None
    subscription_tier: str = "free"
    external_db_access: List[str] = []

class Query(BaseModel):
    question: str
    session_id: Optional[str] = None
    response_style: Optional[str] = "balanced"
    user_id: Optional[str] = None
    search_scope: Optional[str] = "all"
    external_databases: Optional[List[str]] = []
    use_enhanced_rag: Optional[bool] = True
    document_id: Optional[str] = None

class QueryResponse(BaseModel):
    response: Optional[str] = None
    error: Optional[str] = None
    context_found: bool = False
    sources: Optional[list] = None
    session_id: str
    confidence_score: float = 0.0
    expand_available: bool = False
    sources_searched: List[str] = []
    retrieval_method: Optional[str] = None

class ComprehensiveAnalysisRequest(BaseModel):
    document_id: Optional[str] = None
    analysis_types: List[AnalysisType] = [AnalysisType.COMPREHENSIVE]
    user_id: str
    session_id: Optional[str] = None
    response_style: str = "detailed"

class StructuredAnalysisResponse(BaseModel):
    document_summary: Optional[str] = None
    key_clauses: Optional[str] = None
    risk_assessment: Optional[str] = None
    timeline_deadlines: Optional[str] = None
    party_obligations: Optional[str] = None
    missing_clauses: Optional[str] = None
    confidence_scores: Dict[str, float] = {}
    sources_by_section: Dict[str, List[Dict]] = {}
    overall_confidence: float = 0.0
    processing_time: float = 0.0
    warnings: List[str] = []
    retrieval_method: str = "comprehensive_analysis"

class UserDocumentUpload(BaseModel):
    user_id: str
    file_id: str
    filename: str
    upload_timestamp: str
    pages_processed: int
    metadata: Dict[str, Any]

class DocumentUploadResponse(BaseModel):
    message: str
    file_id: str
    pages_processed: int
    processing_time: float
    warnings: List[str]
    session_id: str
    user_id: str
    container_id: str

class ConversationHistory(BaseModel):
    session_id: str
    messages: List[Dict[str, Any]]



=== legal_assistant/models/enums.py ===
"""Models package"""
from .api_models import (
    User, Query, QueryResponse, ComprehensiveAnalysisRequest,
    StructuredAnalysisResponse, UserDocumentUpload, DocumentUploadResponse,
    ConversationHistory
)
from .enums import AnalysisType

__all__ = [
    'User', 'Query', 'QueryResponse', 'ComprehensiveAnalysisRequest',
    'StructuredAnalysisResponse', 'UserDocumentUpload', 'DocumentUploadResponse',
    'ConversationHistory', 'AnalysisType'
]



=== legal_assistant/processors/__init__.py ===
"""Processors package"""
from .query_processor import process_query

__all__ = ['process_query']



=== legal_assistant/processors/query_processor.py ===
"""Query processing logic"""
import re
import logging
import traceback
from typing import Optional

from ..models import QueryResponse, ComprehensiveAnalysisRequest, AnalysisType
from ..config import FeatureFlags, OPENROUTER_API_KEY, MIN_RELEVANCE_SCORE
from ..services import (
    ComprehensiveAnalysisProcessor,
    combined_search,
    calculate_confidence_score,
    call_openrouter_api
)
from ..storage.managers import add_to_conversation, get_conversation_context
from ..utils import (
    parse_multiple_questions,
    extract_bill_information,
    extract_universal_information,
    format_context_for_llm
)

logger = logging.getLogger(__name__)

def process_query(question: str, session_id: str, user_id: Optional[str], search_scope: str, 
                 response_style: str = "balanced", use_enhanced_rag: bool = True, 
                 document_id: str = None) -> QueryResponse:
    """Main query processing function"""
    try:
        logger.info(f"Processing query - Question: '{question}', User: {user_id}, Scope: {search_scope}, Enhanced: {use_enhanced_rag}, Document: {document_id}")
        
        if any(phrase in question.lower() for phrase in ["comprehensive analysis", "complete analysis", "full analysis"]):
            logger.info("Detected comprehensive analysis request")
            
            try:
                comp_request = ComprehensiveAnalysisRequest(
                    document_id=document_id,
                    analysis_types=[AnalysisType.COMPREHENSIVE],
                    user_id=user_id or "default_user",
                    session_id=session_id,
                    response_style=response_style
                )
                
                processor = ComprehensiveAnalysisProcessor()
                comp_result = processor.process_comprehensive_analysis(comp_request)
                
                formatted_response = f"""# Comprehensive Legal Document Analysis

## Document Summary
{comp_result.document_summary or 'No summary available'}

## Key Clauses Analysis
{comp_result.key_clauses or 'No clauses analysis available'}

## Risk Assessment
{comp_result.risk_assessment or 'No risk assessment available'}

## Timeline & Deadlines
{comp_result.timeline_deadlines or 'No timeline information available'}

## Party Obligations
{comp_result.party_obligations or 'No obligations analysis available'}

## Missing Clauses Analysis
{comp_result.missing_clauses or 'No missing clauses analysis available'}

---
**Analysis Confidence:** {comp_result.overall_confidence:.1%}
**Processing Time:** {comp_result.processing_time:.2f} seconds

**Sources:** {len(comp_result.sources_by_section.get('summary', []))} document sections analyzed
"""
                
                add_to_conversation(session_id, "user", question)
                add_to_conversation(session_id, "assistant", formatted_response)
                
                return QueryResponse(
                    response=formatted_response,
                    error=None,
                    context_found=True,
                    sources=comp_result.sources_by_section.get('summary', []),
                    session_id=session_id,
                    confidence_score=comp_result.overall_confidence,
                    expand_available=False,
                    sources_searched=["comprehensive_analysis"],
                    retrieval_method=comp_result.retrieval_method
                )
                
            except Exception as e:
                logger.error(f"Comprehensive analysis failed: {e}")
        
        questions = parse_multiple_questions(question) if use_enhanced_rag else [question]
        combined_query = " ".join(questions)
        
        conversation_context = get_conversation_context(session_id)
        
        retrieved_results, sources_searched, retrieval_method = combined_search(
            combined_query, 
            user_id, 
            search_scope, 
            conversation_context,
            use_enhanced=use_enhanced_rag,
            document_id=document_id
        )
        
        if not retrieved_results:
            return QueryResponse(
                response="I couldn't find any relevant information to answer your question in the searched sources.",
                error=None,
                context_found=False,
                sources=[],
                session_id=session_id,
                confidence_score=0.1,
                sources_searched=sources_searched,
                retrieval_method=retrieval_method
            )
        
        # Format context for LLM
        context_text, source_info = format_context_for_llm(retrieved_results)
        
        # Enhanced information extraction
        bill_match = re.search(r"(HB|SB|SSB|ESSB|SHB|ESHB)\s*(\d+)", question, re.IGNORECASE)
        extracted_info = {}

        if bill_match:
            # Bill-specific extraction
            bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
            logger.info(f"Searching for bill: {bill_number}")
            
            # Search specifically for chunks containing this bill
            bill_specific_results = []
            for doc, score in retrieved_results:
                if 'contains_bills' in doc.metadata and bill_number in doc.metadata['contains_bills']:
                    bill_specific_results.append((doc, score))
                    logger.info(f"Found {bill_number} in chunk {doc.metadata.get('chunk_index', 'unknown')} with score {score}")
            
            # If we found bill-specific chunks, prioritize them
            if bill_specific_results:
                logger.info(f"Using {len(bill_specific_results)} bill-specific chunks for {bill_number}")
                # Use the bill-specific chunks with boosted relevance
                boosted_results = [(doc, min(score + 0.3, 1.0)) for doc, score in bill_specific_results]
                retrieved_results = boosted_results + [r for r in retrieved_results if r not in bill_specific_results]
                retrieved_results = retrieved_results[:len(retrieved_results)]
            
            extracted_info = extract_bill_information(context_text, bill_number)
        else:
            # Universal extraction for any document type
            extracted_info = extract_universal_information(context_text, question)

        # Add extracted information to context to make it more visible to AI
        if extracted_info:
            enhancement = "\n\nKEY INFORMATION FOUND:\n"
            for key, value in extracted_info.items():
                if value:  # Only add if there's actual content
                    if isinstance(value, list):
                        enhancement += f"- {key.replace('_', ' ').title()}: {', '.join(value[:5])}\n"
                    else:
                        enhancement += f"- {key.replace('_', ' ').title()}: {value}\n"
            
            if enhancement.strip() != "KEY INFORMATION FOUND:":
                context_text += enhancement
        
        style_instructions = {
            "concise": "Please provide a concise answer (1-2 sentences) based on the context.",
            "balanced": "Please provide a balanced answer (2-3 paragraphs) based on the context.",
            "detailed": "Please provide a detailed answer with explanations based on the context."
        }
        
        instruction = style_instructions.get(response_style, style_instructions["balanced"])
        
        prompt = f"""You are a legal research assistant. Provide thorough, accurate responses based on the provided documents.

STRICT SOURCE REQUIREMENTS:
- Answer ONLY based on the retrieved documents provided in the context
- Do NOT use general legal knowledge, training data, assumptions, or inferences beyond what's explicitly stated
- If information is not in the provided documents, state: "This information is not available in the provided documents"

SOURCES SEARCHED: {', '.join(sources_searched)}
RETRIEVAL METHOD: {retrieval_method}
{f"DOCUMENT FILTER: Specific document {document_id}" if document_id else "DOCUMENT SCOPE: All available documents"}

HALLUCINATION CHECK - Before responding, verify:
1. Is each claim supported by the retrieved documents?
2. Am I adding information not present in the sources?
3. If uncertain, default to "information not available"

INSTRUCTIONS FOR THOROUGH ANALYSIS:
1. **READ CAREFULLY**: Scan the entire context for information that answers the user's question
2. **EXTRACT DIRECTLY**: When information is clearly stated, provide it exactly as written
3. **BE SPECIFIC**: Include names, numbers, dates, and details when present
4. **QUOTE WHEN HELPFUL**: Use direct quotes for key facts or important language
5. **CITE SOURCES**: Reference the document name for each piece of information
6. **BE COMPLETE**: Provide all relevant information found before saying anything is missing
7. **BE HONEST**: Only say information is unavailable when truly absent from the context

LEGAL ANALYSIS MODES:
1. **BASIC LEGAL RESEARCH** - For factual questions about legislation/statutes/regulations
   - Extract statutory/regulatory information, sponsors, dates, provisions
   
2. **COMPREHENSIVE LEGAL ANALYSIS** - For thorough analysis requiring multiple sources
   - Analyze legal implications, compliance obligations, practical impacts
   - Note ambiguities requiring clarification
   
3. **CASE LAW ANALYSIS** - When precedent needed but unavailable, state:
   "This analysis would benefit from relevant case law not available in the current documents."

HANDLING CONFLICTS:
- If documents contain conflicting information, present both views with citations
- Note the conflict explicitly: "Document A states X, while Document B states Y"

WHEN INFORMATION IS MISSING:
"Based on the provided documents, I cannot provide a complete answer. To provide thorough analysis, I would need documents containing: [specific missing elements]"

RESPONSE STYLE: {instruction}

CONVERSATION HISTORY:
{conversation_context}

DOCUMENT CONTEXT (ANALYZE THOROUGHLY):
{context_text}

USER QUESTION:
{questions}

RESPONSE APPROACH:
- **FIRST**: Identify what specific information the user is asking for
- **SECOND**: Search the context thoroughly for that information  
- **THIRD**: Present any information found clearly and completely
- **FOURTH**: Note what information is not available (if any)
- **ALWAYS**: Cite the source document for each fact provided

RESPONSE:"""
        
        if FeatureFlags.AI_ENABLED and OPENROUTER_API_KEY:
            response_text = call_openrouter_api(prompt, OPENROUTER_API_KEY)
        else:
            response_text = f"Based on the retrieved documents:\n\n{context_text}\n\nPlease review this information to answer your question."
        
        relevant_sources = [s for s in source_info if s['relevance'] >= MIN_RELEVANCE_SCORE]
        
        if relevant_sources:
            response_text += "\n\n**SOURCES:**"
            for source in relevant_sources:
                source_type = source['source_type'].replace('_', ' ').title()
                page_info = f", Page {source['page']}" if source['page'] is not None else ""
                response_text += f"\n- [{source_type}] {source['file_name']}{page_info} (Relevance: {source['relevance']:.2f})"
        
        confidence_score = calculate_confidence_score(retrieved_results, len(response_text))
        
        add_to_conversation(session_id, "user", question)
        add_to_conversation(session_id, "assistant", response_text, source_info)
        
        return QueryResponse(
            response=response_text,
            error=None,
            context_found=True,
            sources=source_info,
            session_id=session_id,
            confidence_score=float(confidence_score),
            sources_searched=sources_searched,
            expand_available=len(questions) > 1 if use_enhanced_rag else False,
            retrieval_method=retrieval_method
        )
        
    except Exception as e:
        logger.error(f"Error processing query: {e}")
        traceback.print_exc()
        return QueryResponse(
            response=None,
            error=str(e),
            context_found=False,
            sources=[],
            session_id=session_id,
            confidence_score=0.0,
            sources_searched=[],
            retrieval_method="error"
        )



=== legal_assistant/services/__init__.py ===
"""Services package"""
from .document_processor import SafeDocumentProcessor
from .container_manager import UserContainerManager, get_container_manager, initialize_container_manager
from .rag_service import (
    enhanced_retrieval_v2,
    combined_search,
    load_database,
    remove_duplicate_documents,
    calculate_confidence_score
)
from .analysis_service import ComprehensiveAnalysisProcessor
from .ai_service import call_openrouter_api
from .external_db_service import (
    LegalDatabaseInterface,
    LexisNexisInterface,
    WestlawInterface,
    search_external_databases
)

__all__ = [
    'SafeDocumentProcessor',
    'UserContainerManager',
    'get_container_manager',
    'initialize_container_manager',
    'enhanced_retrieval_v2',
    'combined_search',
    'load_database',
    'remove_duplicate_documents',
    'calculate_confidence_score',
    'ComprehensiveAnalysisProcessor',
    'call_openrouter_api',
    'LegalDatabaseInterface',
    'LexisNexisInterface',
    'WestlawInterface',
    'search_external_databases'
]



=== legal_assistant/services/ai_service.py ===
"""AI/LLM integration service"""
import logging
import requests
from typing import Optional
from ..config import AI_MODELS, OPENROUTER_API_KEY, OPENAI_API_BASE

logger = logging.getLogger(__name__)

def call_openrouter_api(prompt: str, api_key: str = None, api_base: str = None) -> str:
    """Call OpenRouter API with fallback models"""
    if not api_key:
        api_key = OPENROUTER_API_KEY
    if not api_base:
        api_base = OPENAI_API_BASE
    
    if not api_key:
        return "I apologize, but AI features are not configured. Please set OPENAI_API_KEY environment variable."
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:8000",
        "X-Title": "Legal Assistant"
    }
    
    for model in AI_MODELS:
        try:
            payload = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.5,
                "max_tokens": 2000
            }
            
            response = requests.post(api_base + "/chat/completions", headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                logger.info(f"‚úÖ Successfully used model: {model}")
                return result['choices'][0]['message']['content'].strip()
                
        except Exception as e:
            logger.error(f"Error with model {model}: {e}")
            continue
    
    return "I apologize, but I'm experiencing technical difficulties. Please try again."



=== legal_assistant/services/analysis_service.py ===
"""Comprehensive analysis service"""
import time
import logging
from typing import Dict, List, Tuple, Optional

from ..models import ComprehensiveAnalysisRequest, StructuredAnalysisResponse, AnalysisType
from ..config import COMPREHENSIVE_SEARCH_K, OPENROUTER_API_KEY, OPENAI_API_BASE
from .container_manager import get_container_manager
from .ai_service import call_openrouter_api
from ..utils.formatting import format_context_for_llm

logger = logging.getLogger(__name__)

class ComprehensiveAnalysisProcessor:
    def __init__(self):
        self.analysis_prompts = {
            "document_summary": "Analyze this document and provide a comprehensive summary including document type, purpose, main parties, key terms, important dates, and financial obligations.",
            "key_clauses": "Extract and analyze key legal clauses including termination, indemnification, liability, governing law, confidentiality, payment terms, and dispute resolution. For each clause, provide specific text references and implications.",
            "risk_assessment": "Identify and assess legal risks including unilateral rights, broad indemnification, unlimited liability, vague obligations, and unfavorable terms. Rate each risk (High/Medium/Low) and suggest mitigation strategies.",
            "timeline_deadlines": "Extract all time-related information including start/end dates, payment deadlines, notice periods, renewal terms, performance deadlines, and warranty periods. Present chronologically.",
            "party_obligations": "List all obligations for each party including what must be done, deadlines, conditions, performance standards, and consequences of non-compliance. Organize by party.",
            "missing_clauses": "Identify commonly expected clauses that may be missing such as force majeure, limitation of liability, dispute resolution, severability, assignment restrictions, and notice provisions. Explain the importance and risks of each missing clause."
        }
    
    def process_comprehensive_analysis(self, request: ComprehensiveAnalysisRequest) -> StructuredAnalysisResponse:
        start_time = time.time()
        
        try:
            search_results, sources_searched, retrieval_method = self._enhanced_document_specific_search(
                request.user_id, 
                request.document_id, 
                "comprehensive legal document analysis",
                k=COMPREHENSIVE_SEARCH_K
            )
            
            if not search_results:
                return StructuredAnalysisResponse(
                    warnings=["No relevant documents found for analysis"],
                    processing_time=time.time() - start_time,
                    retrieval_method="no_documents_found"
                )
            
            context_text, source_info = format_context_for_llm(search_results, max_length=8000)
            
            response = StructuredAnalysisResponse()
            response.sources_by_section = {}
            response.confidence_scores = {}
            response.retrieval_method = retrieval_method
            
            if AnalysisType.COMPREHENSIVE in request.analysis_types:
                comprehensive_prompt = self._create_comprehensive_prompt(context_text)
                
                try:
                    analysis_result = call_openrouter_api(comprehensive_prompt, OPENROUTER_API_KEY, OPENAI_API_BASE)
                    parsed_sections = self._parse_comprehensive_response(analysis_result)
                    
                    response.document_summary = parsed_sections.get("summary", "")
                    response.key_clauses = parsed_sections.get("clauses", "")
                    response.risk_assessment = parsed_sections.get("risks", "")
                    response.timeline_deadlines = parsed_sections.get("timeline", "")
                    response.party_obligations = parsed_sections.get("obligations", "")
                    response.missing_clauses = parsed_sections.get("missing", "")
                    
                    response.overall_confidence = self._calculate_comprehensive_confidence(parsed_sections, len(search_results))
                    
                    for section in ["summary", "clauses", "risks", "timeline", "obligations", "missing"]:
                        response.sources_by_section[section] = source_info
                        response.confidence_scores[section] = response.overall_confidence
                    
                except Exception as e:
                    logger.error(f"Comprehensive analysis failed: {e}")
                    response.warnings.append(f"Comprehensive analysis failed: {str(e)}")
                    response.overall_confidence = 0.1
            
            else:
                for analysis_type in request.analysis_types:
                    section_result = self._process_individual_analysis(analysis_type, context_text, source_info)
                    
                    if analysis_type == AnalysisType.SUMMARY:
                        response.document_summary = section_result["content"]
                    elif analysis_type == AnalysisType.CLAUSES:
                        response.key_clauses = section_result["content"]
                    elif analysis_type == AnalysisType.RISKS:
                        response.risk_assessment = section_result["content"]
                    elif analysis_type == AnalysisType.TIMELINE:
                        response.timeline_deadlines = section_result["content"]
                    elif analysis_type == AnalysisType.OBLIGATIONS:
                        response.party_obligations = section_result["content"]
                    elif analysis_type == AnalysisType.MISSING_CLAUSES:
                        response.missing_clauses = section_result["content"]
                    
                    response.confidence_scores[analysis_type.value] = section_result["confidence"]
                    response.sources_by_section[analysis_type.value] = source_info
                
                confidences = list(response.confidence_scores.values())
                response.overall_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            response.processing_time = time.time() - start_time
            logger.info(f"Comprehensive analysis completed in {response.processing_time:.2f}s with confidence {response.overall_confidence:.2f}")
            
            return response
            
        except Exception as e:
            logger.error(f"Comprehensive analysis processing failed: {e}")
            return StructuredAnalysisResponse(
                warnings=[f"Analysis processing failed: {str(e)}"],
                processing_time=time.time() - start_time,
                overall_confidence=0.0,
                retrieval_method="error"
            )
    
    def _enhanced_document_specific_search(self, user_id: str, document_id: Optional[str], query: str, k: int = 15) -> Tuple[List, List[str], str]:
        all_results = []
        sources_searched = []
        retrieval_method = "enhanced_document_specific"
        
        try:
            container_manager = get_container_manager()
            if document_id:
                user_results = container_manager.enhanced_search_user_container(
                    user_id, query, "", k=k, document_id=document_id
                )
                sources_searched.append(f"document_{document_id}")
                logger.info(f"Document-specific search for {document_id}: {len(user_results)} results")
            else:
                user_results = container_manager.enhanced_search_user_container(
                    user_id, query, "", k=k
                )
                sources_searched.append("all_user_documents")
                logger.info(f"All documents search: {len(user_results)} results")
            
            for doc, score in user_results:
                doc.metadata['source_type'] = 'user_container'
                doc.metadata['search_scope'] = 'document_specific' if document_id else 'all_user_docs'
                all_results.append((doc, score))
            
            return all_results[:k], sources_searched, retrieval_method
            
        except Exception as e:
            logger.error(f"Error in document-specific search: {e}")
            return [], [], "error"
    
    def _create_comprehensive_prompt(self, context_text: str) -> str:
        return f"""You are a legal document analyst. Analyze the provided legal document and provide a comprehensive analysis with the following structured sections.

STRICT SOURCE REQUIREMENTS:
- Answer ONLY based on the retrieved documents provided in the context
- Do NOT use general legal knowledge, training data, assumptions, or inferences beyond what's explicitly stated
- If information is not in the provided documents, state: "This information is not available in the provided documents"

SOURCES SEARCHED: {', '.join(sources_searched)}
RETRIEVAL METHOD: {retrieval_method}
{f"DOCUMENT FILTER: Specific document {document_id}" if document_id else "DOCUMENT SCOPE: All available documents"}

HALLUCINATION CHECK - Before responding, verify:
1. Is each claim supported by the retrieved documents?
2. Am I adding information not present in the sources?
3. If any fact or phrase cannot be traced to a source document, it must not appear in the response.

# INSTRUCTIONS FOR THOROUGH ANALYSIS (Modified)
1. **READ CAREFULLY**: Scan the entire context for information that answers the user's question
2. **EXTRACT COMPLETELY**: When extracting requirements, include FULL details (e.g., "60 minutes" not just "minimum of"). 
3. **QUOTE VERBATIM**: For statutory standards, use exact quotes: `\"[Exact Text]\" (Source)` 
4. **ENUMERATE EXPLICITLY**: Present listed requirements as numbered points with full quotes 
5. **CITE SOURCES**: Reference the document name for each fact 
6. **BE COMPLETE**: Explicitly note missing standards: "Documents lack full subsection [X]" 
7. **USE DECISIVE PHRASING**: State facts directly ("The statute requires...") - NEVER "documents indicate" 

LEGAL ANALYSIS MODES:
1. **BASIC LEGAL RESEARCH** - For factual questions about legislation/statutes/regulations
   - Extract statutory/regulatory information, sponsors, dates, provisions
   
2. **COMPREHENSIVE LEGAL ANALYSIS** - For thorough analysis requiring multiple sources
   - Analyze legal implications, compliance obligations, practical impacts
   - Note ambiguities requiring clarification
   
3. **CASE LAW ANALYSIS** - When precedent needed but unavailable, state:
   "This analysis would benefit from relevant case law not available in the current documents."

HANDLING CONFLICTS:
- If documents contain conflicting information, present both views with citations
- Note the conflict explicitly: "Document A states X, while Document B states Y"

WHEN INFORMATION IS MISSING:
"Based on the provided documents, I cannot provide a complete answer. To provide thorough analysis, I would need documents containing: [specific missing elements]"

RESPONSE STYLE: {instruction}

CONVERSATION HISTORY:
{conversation_context}

DOCUMENT CONTEXT (ANALYZE THOROUGHLY):
{context_text}

USER QUESTION:
{questions}

RESPONSE APPROACH:
- **FIRST**: Identify what specific information the user is asking for. Do not reference any statute, case law, or principle unless it appears verbatim in the context.
- **SECOND**: Search the context thoroughly for that information  
- **THIRD**: Present any information found clearly and completely. At the end of your response, list all facts provided and their source documents for verification.
- **FOURTH**: Note what information is not available (if any)
- **ALWAYS**: Cite the source document for each fact provided

ADDITIONAL GUIDANCE:
- After fully answering based solely on the provided documents, if relevant key legal principles under Washington state law, any other U.S. state law, or U.S. federal law are not found in the sources, you may add a clearly labeled general legal principles disclaimer.
- This disclaimer must clearly state it is NOT based on the provided documents but represents general background knowledge of applicable Washington state, other state, and federal law.
- Do NOT use this disclaimer to answer the user‚Äôs question directly; it serves only as supplementary context.
- This disclaimer must explicitly state that these principles are not found in the provided documents but are usually relevant legal background.
- Format this disclaimer distinctly at the end of the response under a heading such as "GENERAL LEGAL PRINCIPLES DISCLAIMER."

RESPONSE:"""
    
    def _parse_comprehensive_response(self, response_text: str) -> Dict[str, str]:
        sections = {}
        section_markers = {
            "summary": ["## DOCUMENT SUMMARY", "# DOCUMENT SUMMARY"],
            "clauses": ["## KEY CLAUSES ANALYSIS", "# KEY CLAUSES ANALYSIS", "## KEY CLAUSES"],
            "risks": ["## RISK ASSESSMENT", "# RISK ASSESSMENT", "## RISKS"],
            "timeline": ["## TIMELINE & DEADLINES", "# TIMELINE & DEADLINES", "## TIMELINE"],
            "obligations": ["## PARTY OBLIGATIONS", "# PARTY OBLIGATIONS", "## OBLIGATIONS"],
            "missing": ["## MISSING CLAUSES ANALYSIS", "# MISSING CLAUSES ANALYSIS", "## MISSING CLAUSES"]
        }
        
        lines = response_text.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            line_strip = line.strip()
            
            section_found = None
            for section_key, markers in section_markers.items():
                if any(line_strip.startswith(marker) for marker in markers):
                    section_found = section_key
                    break
            
            if section_found:
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                
                current_section = section_found
                current_content = []
            else:
                if current_section:
                    current_content.append(line)
        
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        for section_key in section_markers.keys():
            if section_key not in sections or not sections[section_key]:
                sections[section_key] = f"No {section_key.replace('_', ' ').title()} information found in the analysis."
        
        return sections
    
    def _process_individual_analysis(self, analysis_type: AnalysisType, context_text: str, source_info: List[Dict]) -> Dict:
        try:
            prompt = self.analysis_prompts.get(analysis_type.value, "Analyze this legal document.")
            full_prompt = f"{prompt}\n\nLEGAL DOCUMENT CONTEXT:\n{context_text}\n\nPlease provide a detailed analysis based ONLY on the provided context."
            
            result = call_openrouter_api(full_prompt, OPENROUTER_API_KEY, OPENAI_API_BASE)
            
            return {
                "content": result,
                "confidence": 0.7,
                "sources": source_info
            }
        except Exception as e:
            logger.error(f"Individual analysis failed for {analysis_type}: {e}")
            return {
                "content": f"Analysis failed for {analysis_type.value}: {str(e)}",
                "confidence": 0.1,
                "sources": []
            }
    
    def _calculate_comprehensive_confidence(self, parsed_sections: Dict[str, str], num_sources: int) -> float:
        try:
            successful_sections = sum(1 for content in parsed_sections.values() 
                                    if content and not content.startswith("No ") and len(content) > 50)
            section_factor = successful_sections / len(parsed_sections)
            
            avg_length = sum(len(content) for content in parsed_sections.values()) / len(parsed_sections)
            length_factor = min(1.0, avg_length / 200)
            
            source_factor = min(1.0, num_sources / 5)
            
            confidence = (section_factor * 0.5 + length_factor * 0.3 + source_factor * 0.2)
            return max(0.1, min(1.0, confidence))
            
        except Exception as e:
            logger.error(f"Error calculating confidence: {e}")
            return 0.5



=== legal_assistant/services/container_manager.py ===
"""User container management service"""
import os
import hashlib
import logging
import re
import traceback
from typing import Optional, List, Tuple, Dict
from datetime import datetime

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

from ..config import USER_CONTAINERS_PATH, FAST_EMBEDDING_MODELS
from ..core.exceptions import ContainerError
from ..core.dependencies import get_embeddings, get_nlp
from ..utils.text_processing import remove_duplicate_documents

logger = logging.getLogger(__name__)

class UserContainerManager:
    """Manages user-specific document containers with powerful embeddings"""
    
    def __init__(self, base_path: str):
        self.base_path = base_path
        self.embeddings = None
        self._initialize_embeddings()
        logger.info(f"UserContainerManager initialized with base path: {base_path}")
    
    def _initialize_embeddings(self):
        """Initialize embeddings with the best available model"""
        # Try to use the global embeddings if available
        global_embeddings = get_embeddings()
        
        if global_embeddings:
            self.embeddings = global_embeddings
            logger.info(f"Using global embeddings model")
            return
        
        # TEMPORARY: Use faster embeddings for large document processing
        for model_name in FAST_EMBEDDING_MODELS:
            try:
                self.embeddings = HuggingFaceEmbeddings(model_name=model_name)
                logger.info(f"‚úÖ UserContainerManager using FAST embeddings: {model_name}")
                return
            except Exception as e:
                logger.warning(f"Failed to load {model_name}: {e}")
                continue
        
        # Last resort fallback
        try:
            self.embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            logger.warning("‚ö†Ô∏è Using fallback embeddings: all-MiniLM-L6-v2")
        except Exception as e:
            logger.error(f"‚ùå Failed to load any embeddings model: {e}")
            self.embeddings = None
    
    def create_user_container(self, user_id: str) -> str:
        """Create a new container for a user"""
        container_id = hashlib.sha256(user_id.encode()).hexdigest()[:16]
        container_path = os.path.join(self.base_path, container_id)
        os.makedirs(container_path, exist_ok=True)
        
        # Ensure embeddings are available
        if not self.embeddings:
            self._initialize_embeddings()
        
        if not self.embeddings:
            raise ContainerError("No embeddings model available for container creation")
        
        user_db = Chroma(
            collection_name=f"user_{container_id}",
            embedding_function=self.embeddings,
            persist_directory=container_path
        )
        
        logger.info(f"Created container for user {user_id}: {container_id}")
        return container_id
    
    def get_container_id(self, user_id: str) -> str:
        """Get container ID for a user"""
        return hashlib.sha256(user_id.encode()).hexdigest()[:16]
    
    def get_user_database(self, user_id: str) -> Optional[Chroma]:
        """Get user's database"""
        container_id = self.get_container_id(user_id)
        container_path = os.path.join(self.base_path, container_id)
        
        if not os.path.exists(container_path):
            logger.warning(f"Container not found for user {user_id}")
            return None
        
        # Ensure embeddings are available
        if not self.embeddings:
            self._initialize_embeddings()
        
        if not self.embeddings:
            logger.error("No embeddings model available for database access")
            return None
        
        return Chroma(
            collection_name=f"user_{container_id}",
            embedding_function=self.embeddings,
            persist_directory=container_path
        )
    
    def get_user_database_safe(self, user_id: str) -> Optional[Chroma]:
        """Get user database with enhanced error handling and recovery"""
        try:
            container_id = self.get_container_id(user_id)
            container_path = os.path.join(self.base_path, container_id)
            
            if not os.path.exists(container_path):
                logger.warning(f"Container not found for user {user_id}, creating new one")
                self.create_user_container(user_id)
            
            # Ensure embeddings are available
            if not self.embeddings:
                self._initialize_embeddings()
            
            if not self.embeddings:
                logger.error("No embeddings model available for safe database access")
                return None
            
            return Chroma(
                collection_name=f"user_{container_id}",
                embedding_function=self.embeddings,
                persist_directory=container_path
            )
            
        except Exception as e:
            logger.error(f"Error getting user database for {user_id}: {e}")
            try:
                logger.info(f"Attempting to recover by creating new container for {user_id}")
                self.create_user_container(user_id)
                container_id = self.get_container_id(user_id)
                container_path = os.path.join(self.base_path, container_id)
                
                if not self.embeddings:
                    self._initialize_embeddings()
                
                if not self.embeddings:
                    logger.error("No embeddings model available for recovery")
                    return None
                
                return Chroma(
                    collection_name=f"user_{container_id}",
                    embedding_function=self.embeddings,
                    persist_directory=container_path
                )
            except Exception as recovery_error:
                logger.error(f"Recovery failed for user {user_id}: {recovery_error}")
                return None
    
    def add_document_to_container(self, user_id: str, document_text: str, metadata: Dict, file_id: str = None) -> bool:
        """Add document to user's container"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                container_id = self.create_user_container(user_id)
                user_db = self.get_user_database_safe(user_id)
            
            # Smart document type detection - lowered threshold
            bill_count = len(re.findall(r'\b(?:HB|SB|SHB|SSB|ESHB|ESSB)\s+\d+', document_text))
            is_legislative = bill_count > 1  # Lowered from 2 to 1 - even 1-2 bills = legislative
            
            if is_legislative:
                # Legislative document: Bill-aware chunking
                logger.info(f"Detected legislative document with {bill_count} bills - using bill-aware chunking")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=2000,
                    chunk_overlap=500,
                    length_function=len,
                    separators=["\n\n", "\nHB ", "\nSB ", "\nSHB ", "\nSSB ", "\nESHB ", "\nESSB ", "\n", " "]
                )
                chunking_method = 'bill_aware_chunking'
            else:
                # Regular document: Standard semantic chunking
                logger.info(f"Detected regular document ({bill_count} bills found) - using standard chunking")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1500,
                    chunk_overlap=300,
                    length_function=len,
                    separators=["\n\n", "\n", ". ", " ", ""]  # Natural breakpoints
                )
                chunking_method = 'semantic_chunking'
            
            chunks = text_splitter.split_text(document_text)
            logger.info(f"Created {len(chunks)} chunks using {chunking_method}")
            
            # Adjust batch size based on chunk size
            batch_size = 25 if is_legislative else 50
            total_batches = (len(chunks) + batch_size - 1) // batch_size
            
            for batch_num in range(total_batches):
                start_idx = batch_num * batch_size
                end_idx = min(start_idx + batch_size, len(chunks))
                batch_chunks = chunks[start_idx:end_idx]
                
                logger.info(f"Processing batch {batch_num + 1}/{total_batches} ({len(batch_chunks)} chunks)")
                
                documents = []
                for i, chunk in enumerate(batch_chunks):
                    doc_metadata = metadata.copy()
                    doc_metadata['chunk_index'] = start_idx + i
                    doc_metadata['total_chunks'] = len(chunks)
                    doc_metadata['user_id'] = user_id
                    doc_metadata['upload_timestamp'] = datetime.utcnow().isoformat()
                    doc_metadata['chunk_size'] = len(chunk)
                    doc_metadata['chunking_method'] = chunking_method
                    doc_metadata['document_type'] = 'legislative' if is_legislative else 'general'
                    
                    # Extract bill numbers for legislative docs only
                    if is_legislative:
                        bill_numbers = re.findall(r'\b(?:HB|SB|SHB|SSB|ESHB|ESSB)\s+\d+', chunk)
                        if bill_numbers:
                            doc_metadata['contains_bills'] = ', '.join(bill_numbers)
                            logger.info(f"Chunk {start_idx + i} contains bills: {bill_numbers}")
                    
                    if file_id:
                        doc_metadata['file_id'] = file_id
                    
                    # Clean metadata for ChromaDB
                    clean_metadata = {}
                    for key, value in doc_metadata.items():
                        if isinstance(value, (str, int, float, bool)):
                            clean_metadata[key] = value
                        elif isinstance(value, list):
                            clean_metadata[key] = str(value)
                        elif value is None:
                            clean_metadata[key] = ""
                        else:
                            clean_metadata[key] = str(value)
                    
                    documents.append(Document(
                        page_content=chunk,
                        metadata=clean_metadata
                    ))
                
                # Add batch to ChromaDB
                try:
                    user_db.add_documents(documents)
                    logger.info(f"‚úÖ Added batch {batch_num + 1} ({len(documents)} chunks)")
                except Exception as batch_error:
                    logger.error(f"‚ùå Batch {batch_num + 1} failed: {batch_error}")
                    return False
            
            logger.info(f"‚úÖ Successfully added ALL {len(chunks)} chunks for document {file_id or 'unknown'}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error in add_document_to_container: {e}")
            logger.error(f"Error type: {type(e).__name__}")
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return False
    
    def search_user_container(self, user_id: str, query: str, k: int = 5, document_id: str = None) -> List[Tuple]:
        """Search with timeout protection"""
        return self.search_user_container_safe(user_id, query, k, document_id)
    
    def search_user_container_safe(self, user_id: str, query: str, k: int = 5, document_id: str = None) -> List[Tuple]:
        """Search with enhanced error handling and timeout protection"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                logger.warning(f"No database available for user {user_id}")
                return []
            
            filter_dict = None
            if document_id:
                filter_dict = {"file_id": document_id}
            
            try:
                results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                return results
            except Exception as search_error:
                logger.warning(f"Search failed for user {user_id}: {search_error}")
                return []
                
        except Exception as e:
            logger.error(f"Error in safe container search for user {user_id}: {e}")
            return []
    
    def enhanced_search_user_container(self, user_id: str, query: str, conversation_context: str, k: int = 12, document_id: str = None) -> List[Tuple]:
        """Enhanced search with timeout protection and bill-specific optimization"""
        try:
            user_db = self.get_user_database_safe(user_id)
            if not user_db:
                return []
            
            filter_dict = None
            if document_id:
                filter_dict = {"file_id": document_id}
            
            try:
                # Check if this is a bill-specific query
                bill_match = re.search(r"\b(HB|SB|SSB|ESSB|SHB|ESHB)\s+(\d+)\b", query, re.IGNORECASE)
                
                if bill_match:
                    bill_number = f"{bill_match.group(1)} {bill_match.group(2)}"
                    logger.info(f"Bill-specific search for: {bill_number}")
                    
                    # First, try to find chunks that contain this specific bill
                    try:
                        all_docs = user_db.get()
                        bill_specific_chunks = []
                        
                        for i, (doc_id, metadata, content) in enumerate(zip(all_docs['ids'], all_docs['metadatas'], all_docs['documents'])):
                            if metadata and 'contains_bills' in metadata:
                                if bill_number in metadata['contains_bills']:
                                    # Create a document object for this chunk
                                    doc_obj = Document(page_content=content, metadata=metadata)
                                    # Use a high relevance score since we found exact bill match
                                    bill_specific_chunks.append((doc_obj, 0.95))  # High relevance for exact matches
                                    logger.info(f"Found {bill_number} in chunk {metadata.get('chunk_index')} with boosted score")
                        
                        if bill_specific_chunks:
                            logger.info(f"Using {len(bill_specific_chunks)} bill-specific chunks with high relevance")
                            # Get additional context chunks with lower threshold
                            regular_results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                            
                            # Combine bill-specific (high score) with regular results
                            all_results = bill_specific_chunks + regular_results
                            return remove_duplicate_documents(all_results)[:k]
                    except Exception as bill_search_error:
                        logger.warning(f"Bill-specific search failed, falling back to regular search: {bill_search_error}")
                        # Fall through to regular search
                
                # Fallback to regular search
                direct_results = user_db.similarity_search_with_score(query, k=k, filter=filter_dict)
                expanded_query = f"{query} {conversation_context}"
                expanded_results = user_db.similarity_search_with_score(expanded_query, k=k, filter=filter_dict)
                
                sub_query_results = []
                nlp = get_nlp()
                if nlp:
                    doc = nlp(query)
                    for ent in doc.ents:
                        if ent.label_ in ["ORG", "PERSON", "LAW", "DATE"]:
                            sub_results = user_db.similarity_search_with_score(f"What is {ent.text}?", k=3, filter=filter_dict)
                            sub_query_results.extend(sub_results)
                
                all_results = direct_results + expanded_results + sub_query_results
                return remove_duplicate_documents(all_results)[:k]
                
            except Exception as search_error:
                logger.warning(f"Enhanced search failed for user {user_id}: {search_error}")
                return []
            
        except Exception as e:
            logger.error(f"Error in enhanced user container search: {e}")
            return []

# Global instance
_container_manager = None

def initialize_container_manager():
    """Initialize the global container manager"""
    global _container_manager
    _container_manager = UserContainerManager(USER_CONTAINERS_PATH)
    return _container_manager

def get_container_manager() -> UserContainerManager:
    """Get the global container manager instance"""
    if _container_manager is None:
        return initialize_container_manager()
    return _container_manager



=== legal_assistant/services/document_processor.py ===
"""Document processing service"""
import os
import io
import logging
import tempfile
from typing import Tuple, List
from ..config import FeatureFlags
from ..core.exceptions import DocumentProcessingError

logger = logging.getLogger(__name__)

class SafeDocumentProcessor:
    """Safe document processor for various file types with enhanced processing capabilities"""
    
    @staticmethod
    def process_document_safe(file) -> Tuple[str, int, List[str]]:
        """
        Process uploaded document safely with enhanced processing
        Returns: (content, pages_processed, warnings)
        """
        warnings = []
        content = ""
        pages_processed = 0
        
        try:
            filename = getattr(file, 'filename', 'unknown')
            file_ext = os.path.splitext(filename)[1].lower()
            file_content = file.file.read()
            
            if file_ext == '.txt':
                content = file_content.decode('utf-8', errors='ignore')
                pages_processed = SafeDocumentProcessor._estimate_pages_from_text(content)
            elif file_ext == '.pdf':
                content, pages_processed = SafeDocumentProcessor._process_pdf_enhanced(file_content, warnings)
            elif file_ext == '.docx':
                content, pages_processed = SafeDocumentProcessor._process_docx_enhanced(file_content, warnings)
            else:
                try:
                    content = file_content.decode('utf-8', errors='ignore')
                    pages_processed = SafeDocumentProcessor._estimate_pages_from_text(content)
                    warnings.append(f"File type {file_ext} processed as plain text")
                except Exception as e:
                    warnings.append(f"Could not process file: {str(e)}")
                    content = "Unable to process this file type"
                    pages_processed = 0
            
            file.file.seek(0)
            
        except Exception as e:
            warnings.append(f"Error processing document: {str(e)}")
            content = "Error processing document"
            pages_processed = 0
        
        return content, pages_processed, warnings
    
    @staticmethod
    def _estimate_pages_from_text(text: str) -> int:
        """Fixed page estimation based on content analysis"""
        if not text:
            return 0
        
        # Enhanced page estimation logic
        word_count = len(text.split())
        char_count = len(text)
        line_count = len(text.split('\n'))
        
        # Average words per page: 250-500 (legal documents tend to be dense)
        pages_by_words = max(1, word_count // 350)
        
        # Average characters per page: 1500-3000 (including spaces)
        pages_by_chars = max(1, char_count // 2000)
        
        # For documents with many line breaks (structured content)
        pages_by_lines = max(1, line_count // 50)
        
        # Use the median of the three estimates for better accuracy
        estimates = [pages_by_words, pages_by_chars, pages_by_lines]
        estimates.sort()
        estimated_pages = estimates[1]  # median
        
        # Ensure reasonable bounds
        return max(1, min(estimated_pages, 1000))
    
    @staticmethod
    def _process_pdf_enhanced(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Enhanced PDF processing with Unstructured.io fallback"""
        # Try Unstructured.io first for best results
        if FeatureFlags.UNSTRUCTURED_AVAILABLE:
            try:
                from unstructured.partition.auto import partition
                
                with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                    temp_file.write(file_content)
                    temp_file_path = temp_file.name
                
                try:
                    # Use Unstructured.io for advanced processing
                    elements = partition(filename=temp_file_path)
                    
                    # Extract text and structure
                    text_content = ""
                    page_count = 0
                    
                    for element in elements:
                        if hasattr(element, 'text') and element.text:
                            text_content += element.text + "\n"
                        
                        # Try to get page information
                        if hasattr(element, 'metadata') and element.metadata:
                            if 'page_number' in element.metadata:
                                page_count = max(page_count, element.metadata['page_number'])
                    
                    # Clean up temp file
                    os.unlink(temp_file_path)
                    
                    if page_count == 0:
                        page_count = SafeDocumentProcessor._estimate_pages_from_text(text_content)
                    
                    return text_content, page_count
                    
                except Exception as e:
                    os.unlink(temp_file_path)
                    warnings.append(f"Unstructured.io processing failed: {str(e)}, falling back to PyMuPDF")
                    
            except Exception as e:
                warnings.append(f"Unstructured.io setup failed: {str(e)}")
        
        # Fallback to existing PDF processing
        return SafeDocumentProcessor._process_pdf(file_content, warnings)
    
    @staticmethod
    def _process_pdf(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process PDF content with enhanced page counting"""
        try:
            if FeatureFlags.PYMUPDF_AVAILABLE:
                try:
                    import fitz
                    doc = fitz.open(stream=file_content, filetype="pdf")
                    text_content = ""
                    pages = len(doc)
                    for page_num in range(pages):
                        page = doc.load_page(page_num)
                        text_content += page.get_text()
                    doc.close()
                    return text_content, pages
                except Exception as e:
                    warnings.append(f"PyMuPDF error: {str(e)}")
            
            if FeatureFlags.PDFPLUMBER_AVAILABLE:
                try:
                    import pdfplumber
                    with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                        text_content = ""
                        pages = len(pdf.pages)
                        for page in pdf.pages:
                            text_content += page.extract_text() or ""
                    return text_content, pages
                except Exception as e:
                    warnings.append(f"pdfplumber error: {str(e)}")
            
            warnings.append("No PDF processing libraries available. Install PyMuPDF or pdfplumber.")
            return "PDF processing not available", 0
            
        except Exception as e:
            warnings.append(f"Error processing PDF: {str(e)}")
            return "Error processing PDF", 0
    
    @staticmethod
    def _process_docx_enhanced(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Enhanced DOCX processing with better page estimation"""
        if FeatureFlags.UNSTRUCTURED_AVAILABLE:
            try:
                from unstructured.partition.auto import partition
                
                with tempfile.NamedTemporaryFile(suffix='.docx', delete=False) as temp_file:
                    temp_file.write(file_content)
                    temp_file_path = temp_file.name
                
                try:
                    # Use Unstructured.io for advanced processing
                    elements = partition(filename=temp_file_path)
                    
                    text_content = ""
                    for element in elements:
                        if hasattr(element, 'text') and element.text:
                            text_content += element.text + "\n"
                    
                    os.unlink(temp_file_path)
                    
                    pages_estimated = SafeDocumentProcessor._estimate_pages_from_text(text_content)
                    return text_content, pages_estimated
                    
                except Exception as e:
                    os.unlink(temp_file_path)
                    warnings.append(f"Unstructured.io DOCX processing failed: {str(e)}")
                    
            except Exception as e:
                warnings.append(f"Unstructured.io DOCX setup failed: {str(e)}")
        
        # Fallback to existing DOCX processing
        return SafeDocumentProcessor._process_docx(file_content, warnings)
    
    @staticmethod
    def _process_docx(file_content: bytes, warnings: List[str]) -> Tuple[str, int]:
        """Process DOCX content with enhanced page estimation"""
        try:
            try:
                from docx import Document
                doc = Document(io.BytesIO(file_content))
                text_content = ""
                for paragraph in doc.paragraphs:
                    text_content += paragraph.text + "\n"
                
                # Enhanced page estimation for DOCX
                pages_estimated = SafeDocumentProcessor._estimate_pages_from_text(text_content)
                return text_content, pages_estimated
                
            except ImportError:
                warnings.append("python-docx not available. Install with: pip install python-docx")
                return "DOCX processing not available", 0
            except Exception as e:
                warnings.append(f"Error processing DOCX: {str(e)}")
                return "Error processing DOCX", 0
                
        except Exception as e:
            warnings.append(f"Error processing DOCX: {str(e)}")
            return "Error processing DOCX", 0



=== legal_assistant/services/external_db_service.py ===
"""External legal database integration service"""
import logging
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
from ..config import LEXISNEXIS_API_KEY, LEXISNEXIS_API_ENDPOINT, WESTLAW_API_KEY, WESTLAW_API_ENDPOINT
from ..models import User

logger = logging.getLogger(__name__)

class LegalDatabaseInterface(ABC):
    """Abstract interface for external legal databases"""
    
    @abstractmethod
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        pass
    
    @abstractmethod
    def get_document(self, document_id: str) -> Dict:
        pass
    
    @abstractmethod
    def authenticate(self, credentials: Dict) -> bool:
        pass

class LexisNexisInterface(LegalDatabaseInterface):
    def __init__(self, api_key: str = None, api_endpoint: str = None):
        self.api_key = api_key or LEXISNEXIS_API_KEY
        self.api_endpoint = api_endpoint or LEXISNEXIS_API_ENDPOINT
        self.authenticated = False
    
    def authenticate(self, credentials: Dict) -> bool:
        logger.info("LexisNexis authentication placeholder")
        return False
    
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        logger.info(f"LexisNexis search placeholder for query: {query}")
        return []
    
    def get_document(self, document_id: str) -> Dict:
        logger.info(f"LexisNexis document retrieval placeholder for ID: {document_id}")
        return {}

class WestlawInterface(LegalDatabaseInterface):
    def __init__(self, api_key: str = None, api_endpoint: str = None):
        self.api_key = api_key or WESTLAW_API_KEY
        self.api_endpoint = api_endpoint or WESTLAW_API_ENDPOINT
        self.authenticated = False
    
    def authenticate(self, credentials: Dict) -> bool:
        logger.info("Westlaw authentication placeholder")
        return False
    
    def search(self, query: str, filters: Optional[Dict] = None) -> List[Dict]:
        logger.info(f"Westlaw search placeholder for query: {query}")
        return []
    
    def get_document(self, document_id: str) -> Dict:
        logger.info(f"Westlaw document retrieval placeholder for ID: {document_id}")
        return {}

# External databases
external_databases = {
    "lexisnexis": LexisNexisInterface(),
    "westlaw": WestlawInterface()
}

def search_external_databases(query: str, databases: List[str], user: User) -> List[Dict]:
    """Search external legal databases"""
    results = []
    
    for db_name in databases:
        if db_name not in user.external_db_access:
            logger.warning(f"User {user.user_id} does not have access to {db_name}")
            continue
        
        if db_name in external_databases:
            db_interface = external_databases[db_name]
            try:
                db_results = db_interface.search(query)
                for result in db_results:
                    result['source_database'] = db_name
                    results.extend(db_results)
            except Exception as e:
                logger.error(f"Error searching {db_name}: {e}")
    
    return results



=== legal_assistant/services/rag_service.py ===
"""RAG (Retrieval-Augmented Generation) operations service"""
import os
import logging
import numpy as np
from typing import List, Tuple, Dict, Optional

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from ..config import DEFAULT_CHROMA_PATH, DEFAULT_SEARCH_K, ENHANCED_SEARCH_K, CONFIDENCE_WEIGHTS
from ..core.dependencies import get_nlp
from ..core.exceptions import RetrievalError
from .container_manager import get_container_manager
from ..utils.text_processing import remove_duplicate_documents

logger = logging.getLogger(__name__)

def load_database():
    """Load the default database"""
    try:
        if not os.path.exists(DEFAULT_CHROMA_PATH):
            logger.warning(f"Default database path does not exist: {DEFAULT_CHROMA_PATH}")
            return None
        
        embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        db = Chroma(
            collection_name="default",
            embedding_function=embedding_function,
            persist_directory=DEFAULT_CHROMA_PATH
        )
        logger.debug("Default database loaded successfully")
        return db
    except Exception as e:
        logger.error(f"Failed to load default database: {e}")
        raise RetrievalError(f"Failed to load default database: {str(e)}")

def enhanced_retrieval_v2(db, query_text: str, conversation_history_context: str, k: int = ENHANCED_SEARCH_K, document_filter: Dict = None) -> Tuple[List, str]:
    """Enhanced retrieval with multiple strategies"""
    logger.info(f"[ENHANCED_RETRIEVAL] Original query: '{query_text}'")
    
    try:
        direct_results = db.similarity_search_with_score(query_text, k=k, filter=document_filter)
        logger.info(f"[ENHANCED_RETRIEVAL] Direct search returned {len(direct_results)} results")
        
        expanded_query = f"{query_text} {conversation_history_context}"
        expanded_results = db.similarity_search_with_score(expanded_query, k=k, filter=document_filter)
        logger.info(f"[ENHANCED_RETRIEVAL] Expanded search returned {len(expanded_results)} results")
        
        sub_queries = []
        nlp = get_nlp()
        if nlp:
            doc = nlp(query_text)
            for ent in doc.ents:
                if ent.label_ in ["ORG", "PERSON", "LAW", "DATE"]:
                    sub_queries.append(f"What is {ent.text}?")
        
        if not sub_queries:
            question_words = ["what", "who", "when", "where", "why", "how"]
            for word in question_words:
                if word in query_text.lower():
                    sub_queries.append(f"{word.capitalize()} {query_text.lower().replace(word, '').strip()}?")
        
        sub_query_results = []
        for sq in sub_queries[:3]:
            sq_results = db.similarity_search_with_score(sq, k=3, filter=document_filter)
            sub_query_results.extend(sq_results)
        
        logger.info(f"[ENHANCED_RETRIEVAL] Sub-query search returned {len(sub_query_results)} results")
        
        all_results = direct_results + expanded_results + sub_query_results
        unique_results = remove_duplicate_documents(all_results)
        top_results = unique_results[:k]
        
        logger.info(f"[ENHANCED_RETRIEVAL] Final results after deduplication: {len(top_results)}")
        return top_results, "enhanced_retrieval_v2"
        
    except Exception as e:
        logger.error(f"[ENHANCED_RETRIEVAL] Error in enhanced retrieval: {e}")
        basic_results = db.similarity_search_with_score(query_text, k=k, filter=document_filter)
        return basic_results, "basic_fallback"

def combined_search(query: str, user_id: Optional[str], search_scope: str, conversation_context: str, 
                   use_enhanced: bool = True, k: int = DEFAULT_SEARCH_K, document_id: str = None) -> Tuple[List, List[str], str]:
    """Combined search across all sources"""
    all_results = []
    sources_searched = []
    retrieval_method = "basic"
    
    if search_scope in ["all", "default_only"]:
        try:
            default_db = load_database()
            if default_db:
                if use_enhanced:
                    default_results, method = enhanced_retrieval_v2(default_db, query, conversation_context, k=k)
                    retrieval_method = method
                else:
                    default_results = default_db.similarity_search_with_score(query, k=k)
                    retrieval_method = "basic_search"
                
                for doc, score in default_results:
                    doc.metadata['source_type'] = 'default_database'
                    all_results.append((doc, score))
                sources_searched.append("default_database")
        except Exception as e:
            logger.error(f"Error searching default database: {e}")
    
    if user_id and search_scope in ["all", "user_only"]:
        try:
            container_manager = get_container_manager()
            if use_enhanced:
                user_results = container_manager.enhanced_search_user_container(user_id, query, conversation_context, k=k, document_id=document_id)
            else:
                user_results = container_manager.search_user_container(user_id, query, k=k, document_id=document_id)
            
            for doc, score in user_results:
                doc.metadata['source_type'] = 'user_container'
                all_results.append((doc, score))
            if user_results:
                sources_searched.append("user_container")
        except Exception as e:
            logger.error(f"Error searching user container: {e}")
    
    if use_enhanced:
        all_results = remove_duplicate_documents(all_results)
    else:
        all_results.sort(key=lambda x: x[1], reverse=True)
    
    return all_results[:k], sources_searched, retrieval_method

def calculate_confidence_score(results_with_scores: List[Tuple], response_length: int) -> float:
    """Calculate confidence score for results"""
    try:
        if not results_with_scores:
            return 0.2
        
        scores = [score for _, score in results_with_scores]
        avg_relevance = np.mean(scores)
        doc_factor = min(1.0, len(results_with_scores) / 5.0)
        
        if len(scores) > 1:
            score_std = np.std(scores)
            consistency_factor = max(0.5, 1.0 - score_std)
        else:
            consistency_factor = 0.7
            
        completeness_factor = min(1.0, response_length / 500.0)
        
        confidence = (
            avg_relevance * CONFIDENCE_WEIGHTS["relevance"] +
            doc_factor * CONFIDENCE_WEIGHTS["document_count"] +
            consistency_factor * CONFIDENCE_WEIGHTS["consistency"] +
            completeness_factor * CONFIDENCE_WEIGHTS["completeness"]
        )
        
        confidence = max(0.0, min(1.0, confidence))
        return confidence
    
    except Exception as e:
        logger.error(f"Error calculating confidence score: {e}")
        return 0.5



=== legal_assistant/storage/__init__.py ===
"""Storage package"""
from .managers import (
    conversations,
    uploaded_files,
    user_sessions,
    add_to_conversation,
    get_conversation_context,
    cleanup_expired_conversations
)

__all__ = [
    'conversations',
    'uploaded_files',
    'user_sessions',
    'add_to_conversation',
    'get_conversation_context',
    'cleanup_expired_conversations'
]



=== legal_assistant/storage/managers.py ===
"""Global state management"""
from typing import Dict, Optional, List, Any
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

# Global state stores
conversations: Dict[str, Dict] = {}
uploaded_files: Dict[str, Dict] = {}
user_sessions: Dict[str, Any] = {}

def add_to_conversation(session_id: str, role: str, content: str, sources: Optional[List] = None):
    """Add message to conversation"""
    if session_id not in conversations:
        conversations[session_id] = {
            'messages': [],
            'created_at': datetime.utcnow(),
            'last_accessed': datetime.utcnow()
        }
    
    message = {
        'role': role,
        'content': content,
        'timestamp': datetime.utcnow().isoformat(),
        'sources': sources or []
    }
    
    conversations[session_id]['messages'].append(message)
    conversations[session_id]['last_accessed'] = datetime.utcnow()

def get_conversation_context(session_id: str, max_length: int = 2000) -> str:
    """Get conversation context for a session"""
    if session_id not in conversations:
        return ""
    
    messages = conversations[session_id]['messages']
    context_parts = []
    recent_messages = messages[-4:]
    
    for msg in recent_messages:
        role = msg['role'].upper()
        content = msg['content']
        if len(content) > 800:
            content = content[:800] + "..."
        context_parts.append(f"{role}: {content}")
    
    if context_parts:
        return "Previous conversation:\n" + "\n".join(context_parts)
    return ""

def cleanup_expired_conversations():
    """Clean up expired conversations"""
    now = datetime.utcnow()
    expired_sessions = [
        session_id for session_id, data in conversations.items()
        if now - data['last_accessed'] > timedelta(hours=1)
    ]
    for session_id in expired_sessions:
        del conversations[session_id]
    if expired_sessions:
        logger.info(f"Cleaned up {len(expired_sessions)} expired conversations")



=== legal_assistant/utils/__init__.py ===
"""Utilities package"""
from .text_processing import (
    parse_multiple_questions,
    semantic_chunking_with_bert,
    basic_text_chunking,
    remove_duplicate_documents,
    extract_bill_information,
    extract_universal_information
)
from .formatting import format_context_for_llm

__all__ = [
    'parse_multiple_questions',
    'semantic_chunking_with_bert',
    'basic_text_chunking',
    'remove_duplicate_documents',
    'extract_bill_information',
    'extract_universal_information',
    'format_context_for_llm'
]



=== legal_assistant/utils/formatting.py ===
"""Formatting utilities"""
import os
from typing import List, Tuple
from ..config import MIN_RELEVANCE_SCORE

def format_context_for_llm(results_with_scores: List[Tuple], max_length: int = 3000) -> Tuple[str, List]:
    """Format search results for LLM context"""
    context_parts = []
    source_info = []
    
    total_length = 0
    for i, (doc, score) in enumerate(results_with_scores):
        if total_length >= max_length:
            break
            
        content = doc.page_content.strip()
        metadata = doc.metadata
        
        source_path = metadata.get('source', 'unknown_source')
        page = metadata.get('page', None)
        source_type = metadata.get('source_type', 'unknown')
        
        display_source = os.path.basename(source_path)
        page_info = f" (Page {page})" if page is not None else ""
        source_prefix = f"[{source_type.upper()}]" if source_type != 'unknown' else ""
        
        if len(content) > 800:
            content = content[:800] + "... [truncated]"
            
        context_part = f"{source_prefix} [{display_source}{page_info}] (Relevance: {score:.2f}): {content}"
        context_parts.append(context_part)
        
        source_info.append({
            'id': i+1,
            'file_name': display_source,
            'page': page,
            'relevance': score,
            'full_path': source_path,
            'source_type': source_type
        })
        
        total_length += len(context_part)
    
    context_text = "\n\n".join(context_parts)
    return context_text, source_info



=== legal_assistant/utils/text_processing.py ===
"""Text processing utilities"""
import re
import logging
import numpy as np
from typing import List, Tuple, Dict, Any
from ..core.dependencies import get_sentence_model, get_sentence_model_name

logger = logging.getLogger(__name__)

def parse_multiple_questions(query_text: str) -> List[str]:
    """Parse multiple questions from a single query"""
    questions = []
    
    if ';' in query_text:
        parts = query_text.split(';')
        for part in parts:
            part = part.strip()
            if part:
                questions.append(part)
    elif '?' in query_text and query_text.count('?') > 1:
        parts = query_text.split('?')
        for part in parts:
            part = part.strip()
            if part:
                questions.append(part + '?')
    else:
        final_question = query_text
        if not final_question.endswith('?') and '?' not in final_question:
            final_question += '?'
        questions = [final_question]
    
    return questions

def semantic_chunking_with_bert(text: str, max_chunk_size: int = 1500, overlap: int = 300) -> List[str]:
    """Advanced semantic chunking with powerful BERT models for legal documents"""
    try:
        sentence_model = get_sentence_model()
        sentence_model_name = get_sentence_model_name()
        
        if sentence_model is None:
            logger.warning("No sentence model available, using basic chunking")
            return basic_text_chunking(text, max_chunk_size, overlap)
        
        logger.info(f"Using semantic chunking with model: {sentence_model_name}")
        
        # For legal documents, split on legal sections and paragraphs
        # Look for common legal document patterns
        legal_patterns = [
            r'\n\s*SECTION\s+\d+',
            r'\n\s*\d+\.\s+',  # Numbered sections
            r'\n\s*\([a-z]\)',  # Subsections (a), (b), etc.
            r'\n\s*WHEREAS',
            r'\n\s*NOW, THEREFORE',
            r'\n\s*Article\s+[IVX\d]+',
        ]
        
        # Split text into meaningful sections first
        sections = []
        current_pos = 0
        
        # Find legal section breaks
        for pattern in legal_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            for match in matches:
                if match.start() > current_pos:
                    section_text = text[current_pos:match.start()].strip()
                    if section_text:
                        sections.append(section_text)
                current_pos = match.start()
        
        # Add remaining text
        if current_pos < len(text):
            remaining_text = text[current_pos:].strip()
            if remaining_text:
                sections.append(remaining_text)
        
        # If no legal patterns found, fall back to paragraph splitting
        if not sections:
            sections = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not sections:
            sections = [text]
        
        # If document is small enough, return as single chunk
        if len(text) <= max_chunk_size:
            return [text]
        
        # Calculate embeddings for sections (batch processing for efficiency)
        try:
            section_embeddings = sentence_model.encode(sections, batch_size=32, show_progress_bar=False)
        except Exception as e:
            logger.warning(f"Embedding calculation failed: {e}, using basic chunking")
            return basic_text_chunking(text, max_chunk_size, overlap)
        
        # Advanced semantic grouping using cosine similarity
        chunks = []
        current_chunk = []
        current_chunk_size = 0
        
        for i, section in enumerate(sections):
            section_size = len(section)
            
            # If adding this section would exceed chunk size
            if current_chunk_size + section_size > max_chunk_size and current_chunk:
                
                # For legal documents, try to find natural breaking points
                if len(current_chunk) > 1:
                    # Calculate semantic similarity to decide on best split point
                    chunk_text = '\n\n'.join(current_chunk)
                    chunks.append(chunk_text)
                    
                    # Intelligent overlap: keep semantically similar content
                    if i > 0:
                        # Use similarity to determine overlap
                        prev_embedding = section_embeddings[i-1:i]
                        curr_embedding = section_embeddings[i:i+1]
                        
                        try:
                            similarity = np.dot(prev_embedding[0], curr_embedding[0])
                            if similarity > 0.7:  # High similarity - include more overlap
                                overlap_sections = current_chunk[-2:] if len(current_chunk) > 1 else current_chunk[-1:]
                            else:
                                overlap_sections = current_chunk[-1:] if current_chunk else []
                            
                            current_chunk = overlap_sections + [section]
                            current_chunk_size = sum(len(s) for s in current_chunk)
                        except:
                            # Fallback to simple overlap
                            current_chunk = [current_chunk[-1], section] if current_chunk else [section]
                            current_chunk_size = sum(len(s) for s in current_chunk)
                    else:
                        current_chunk = [section]
                        current_chunk_size = section_size
                else:
                    # Single large section - need to split it
                    if section_size > max_chunk_size:
                        # Split large section into smaller parts
                        large_section_chunks = basic_text_chunking(section, max_chunk_size, overlap)
                        chunks.extend(large_section_chunks[:-1])  # Add all but last
                        current_chunk = [large_section_chunks[-1]]  # Keep last for next iteration
                        current_chunk_size = len(large_section_chunks[-1])
                    else:
                        chunks.append(section)
                        current_chunk = []
                        current_chunk_size = 0
            else:
                current_chunk.append(section)
                current_chunk_size += section_size
        
        # Add remaining chunk
        if current_chunk:
            chunk_text = '\n\n'.join(current_chunk)
            chunks.append(chunk_text)
        
        # Ensure we have at least one chunk
        if not chunks:
            chunks = [text[:max_chunk_size]]
        
        logger.info(f"Semantic chunking created {len(chunks)} chunks from {len(sections)} sections")
        return chunks
        
    except Exception as e:
        logger.error(f"Advanced semantic chunking failed: {e}, falling back to basic chunking")
        return basic_text_chunking(text, max_chunk_size, overlap)

def basic_text_chunking(text: str, max_chunk_size: int = 1500, overlap: int = 300) -> List[str]:
    """Basic text chunking fallback"""
    if len(text) <= max_chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + max_chunk_size
        
        if end >= len(text):
            chunks.append(text[start:])
            break
        
        # Try to break at a sentence boundary
        chunk = text[start:end]
        last_period = chunk.rfind('.')
        last_newline = chunk.rfind('\n')
        
        # Find the best breaking point
        break_point = max(last_period, last_newline)
        if break_point > start + max_chunk_size // 2:  # Only if break point is reasonable
            end = start + break_point + 1
        
        chunks.append(text[start:end])
        start = end - overlap  # Add overlap
    
    return chunks

def remove_duplicate_documents(results_with_scores: List[Tuple]) -> List[Tuple]:
    """Remove duplicate documents from search results"""
    if not results_with_scores:
        return []
    
    unique_results = []
    seen_content = set()
    
    for doc, score in results_with_scores:
        content_hash = hash(doc.page_content[:100])
        if content_hash not in seen_content:
            seen_content.add(content_hash)
            unique_results.append((doc, score))
    
    unique_results.sort(key=lambda x: x[1], reverse=True)
    return unique_results

def extract_bill_information(context_text: str, bill_number: str) -> Dict[str, str]:
    """Pre-extract bill information using regex patterns"""
    extracted_info = {}
    
    # Enhanced pattern to find bill information with more context
    bill_patterns = [
        rf"{bill_number}[^\n]*(?:\n(?:[^\n]*(?:sponsors?|final\s+status|enables|authorizes|establishes)[^\n]*\n?)*)",
        rf"{bill_number}.*?(?=\n\s*[A-Z]{{2,}}|\n\s*[A-Z]{{1,3}}\s+\d+|\Z)",
        rf"{bill_number}[^\n]*\n(?:[^\n]+\n?){{0,5}}"
    ]
    
    for pattern in bill_patterns:
        bill_match = re.search(pattern, context_text, re.DOTALL | re.IGNORECASE)
        if bill_match:
            bill_text = bill_match.group(0)
            logger.info(f"Found bill text for {bill_number}: {bill_text[:200]}...")
            
            # Extract sponsors with multiple patterns
            sponsor_patterns = [
                rf"Sponsors?\s*:\s*([^\n]+)",
                rf"Sponsor\s*:\s*([^\n]+)",
                rf"(?:Rep\.|Sen\.)\s+([^,\n]+(?:,\s*[^,\n]+)*)"
            ]
            
            for sponsor_pattern in sponsor_patterns:
                sponsor_match = re.search(sponsor_pattern, bill_text, re.IGNORECASE)
                if sponsor_match:
                    extracted_info["sponsors"] = sponsor_match.group(1).strip()
                    break
            
            # Extract final status with multiple patterns
            status_patterns = [
                rf"Final Status\s*:\s*([^\n]+)",
                rf"Status\s*:\s*([^\n]+)",
                rf"(?:C\s+\d+\s+L\s+\d+)"
            ]
            
            for status_pattern in status_patterns:
                status_match = re.search(status_pattern, bill_text, re.IGNORECASE)
                if status_match:
                    extracted_info["final_status"] = status_match.group(1).strip()
                    break
            
            # Extract description - everything after bill number until next bill or section
            desc_patterns = [
                rf"{bill_number}[^\n]*\n([^\n]+(?:\n[^\n]+)*?)(?=\n\s*[A-Z]{{2,}}|\n\s*[A-Z]{{1,3}}\s+\d+|\Z)",
                rf"{bill_number}[^\n]*\n([^\n]+)"
            ]
            
            for desc_pattern in desc_patterns:
                desc_match = re.search(desc_pattern, bill_text, re.IGNORECASE)
                if desc_match:
                    description = desc_match.group(1).strip()
                    # Clean up description
                    description = re.sub(r'\s+', ' ', description)
                    extracted_info["description"] = description
                    break
            
            logger.info(f"Extracted info for {bill_number}: {extracted_info}")
            return extracted_info
    
    logger.warning(f"No bill information found for {bill_number}")
    return extracted_info

def extract_universal_information(context_text: str, question: str) -> Dict[str, Any]:
    """Universal information extraction that works for any document type"""
    extracted_info = {
        "key_entities": [],
        "numbers_and_dates": [],
        "relationships": []
    }
    
    try:
        # Extract names (people, organizations, bills, cases, etc.)
        name_patterns = [
            r"\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*",  # Names
            r"(?:HB|SB|SSB|ESSB|SHB|ESHB)\s*\d+",  # Bill numbers
        ]
        
        for pattern in name_patterns:
            matches = re.findall(pattern, context_text)
            extracted_info["key_entities"].extend(matches[:10])  # Limit to prevent overflow
        
        # Extract numbers, dates, amounts
        number_patterns = [
            r"\$[\d,]+(?:\.\d{2})?",  # Dollar amounts
            r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}",  # Dates
            r"\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4}",  # Written dates
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, context_text, re.IGNORECASE)
            extracted_info["numbers_and_dates"].extend(matches[:10])
        
        # Extract relationships
        relationship_patterns = [
            r"(?:sponsors?|authored?\s+by):\s*([^.\n]+)",
            r"(?:final\s+status|status):\s*([^.\n]+)",
        ]
        
        for pattern in relationship_patterns:
            matches = re.findall(pattern, context_text, re.IGNORECASE)
            extracted_info["relationships"].extend(matches[:5])
    
    except Exception as e:
        logger.warning(f"Error in universal extraction: {e}")
    
    return extracted_info



=== requirements.txt ===
# FastAPI and server
fastapi==0.104.1
uvicorn[standard]==0.24.0

# LangChain core and components
langchain
langchain-community
langchain-chroma==0.1.4
langchain-huggingface==0.0.3

# Vector database
chromadb==0.4.17

# Embeddings and ML (CPU-only, minimal)
sentence-transformers==2.6.1
huggingface-hub==0.23.1
# Skip torch - it will be installed as dependency of sentence-transformers

# PDF processing
PyMuPDF==1.23.8
pypdf==3.17.4
pdfplumber==0.10.3
PyPDF2==3.0.1

# Document processing
python-docx==1.1.0

# HTTP and API
httpx==0.25.2
requests==2.31.0
aiohttp==3.9.1

# Environment and utilities
python-dotenv==1.0.0
pydantic==2.5.0

# Additional dependencies
typing-extensions==4.8.0
python-multipart==0.0.6

# NLP dependencies
spacy==3.7.2
numpy==1.26.4

# Skip transformers and other heavy ML libraries for now



=== .env.example ===
# OpenRouter API Configuration
OPENAI_API_KEY=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
OPENAI_API_BASE=https://openrouter.ai/api/v1

# External Legal Database APIs (Optional)
LEXISNEXIS_API_KEY=
LEXISNEXIS_API_ENDPOINT=
WESTLAW_API_KEY=
WESTLAW_API_ENDPOINT=

# Server Configuration
PORT=8000



=== .gitignore ===
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/

# ChromaDB
chromadb-database/
user-containers/

# Environment
.env

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db



